{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZPxydcF51QU"
   },
   "source": [
    "# Some regularizations and optimization algorithms on [Abalone](http://archive.ics.uci.edu/ml/datasets/Abalone) dataset\n",
    "By [Zahra Taheri](https://github.com/zahta), November 25, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgL-7DcU51QV"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R49_ppPf51QW",
    "outputId": "c1fbd332-5ee2-437b-d709-f4d14d2df698"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb81198ab28>"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set the seeds to ensure reproducibility\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dN5ZRJE151Qa"
   },
   "source": [
    "## Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PVEdHn5t51Qa",
    "outputId": "377ad0ec-36df-4499-ca56-991b7c54cdbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4177, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('abalone.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "c0dNX5QB51Qf",
    "outputId": "56b9130e-aca4-4079-96d3-88f0b5bdf0ce"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sex  Length  Diameter  ...  Viscera weight  Shell weight  Rings\n",
       "0   M   0.455     0.365  ...          0.1010         0.150     15\n",
       "1   M   0.350     0.265  ...          0.0485         0.070      7\n",
       "2   F   0.530     0.420  ...          0.1415         0.210      9\n",
       "3   M   0.440     0.365  ...          0.1140         0.155     10\n",
       "4   I   0.330     0.255  ...          0.0395         0.055      7\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2UzU7SJy51Qj",
    "outputId": "ed38aa57-0ea5-4f08-9151-48dcb32273d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4177 entries, 0 to 4176\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Sex             4177 non-null   object \n",
      " 1   Length          4177 non-null   float64\n",
      " 2   Diameter        4177 non-null   float64\n",
      " 3   Height          4177 non-null   float64\n",
      " 4   Whole weight    4177 non-null   float64\n",
      " 5   Shucked weight  4177 non-null   float64\n",
      " 6   Viscera weight  4177 non-null   float64\n",
      " 7   Shell weight    4177 non-null   float64\n",
      " 8   Rings           4177 non-null   int64  \n",
      "dtypes: float64(7), int64(1), object(1)\n",
      "memory usage: 293.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tv0cmysC51Qn"
   },
   "source": [
    "## Descriptive statistics and data visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "O8j8FWS-51Qo",
    "outputId": "d3260476-f58b-4c8d-af46-b2c6b8f6532e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.523992</td>\n",
       "      <td>0.407881</td>\n",
       "      <td>0.139516</td>\n",
       "      <td>0.828742</td>\n",
       "      <td>0.359367</td>\n",
       "      <td>0.180594</td>\n",
       "      <td>0.238831</td>\n",
       "      <td>9.933684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.120093</td>\n",
       "      <td>0.099240</td>\n",
       "      <td>0.041827</td>\n",
       "      <td>0.490389</td>\n",
       "      <td>0.221963</td>\n",
       "      <td>0.109614</td>\n",
       "      <td>0.139203</td>\n",
       "      <td>3.224169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.115000</td>\n",
       "      <td>0.441500</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.093500</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.545000</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.799500</td>\n",
       "      <td>0.336000</td>\n",
       "      <td>0.171000</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.615000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>1.153000</td>\n",
       "      <td>0.502000</td>\n",
       "      <td>0.253000</td>\n",
       "      <td>0.329000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>1.130000</td>\n",
       "      <td>2.825500</td>\n",
       "      <td>1.488000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>1.005000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Length     Diameter  ...  Shell weight        Rings\n",
       "count  4177.000000  4177.000000  ...   4177.000000  4177.000000\n",
       "mean      0.523992     0.407881  ...      0.238831     9.933684\n",
       "std       0.120093     0.099240  ...      0.139203     3.224169\n",
       "min       0.075000     0.055000  ...      0.001500     1.000000\n",
       "25%       0.450000     0.350000  ...      0.130000     8.000000\n",
       "50%       0.545000     0.425000  ...      0.234000     9.000000\n",
       "75%       0.615000     0.480000  ...      0.329000    11.000000\n",
       "max       0.815000     0.650000  ...      1.005000    29.000000\n",
       "\n",
       "[8 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Describe the dataset\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fkG4XDuK51Qs",
    "outputId": "456f27d8-756b-4b76-bff1-8e2569955937"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sex    3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the no. of unique items in each categorical column\n",
    "\n",
    "data.select_dtypes('object').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KsR6fPAy51Qw",
    "outputId": "4b1c0c14-3845-42c1-c625-c42b527f2a3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex               0.0\n",
      "Length            0.0\n",
      "Diameter          0.0\n",
      "Height            0.0\n",
      "Whole weight      0.0\n",
      "Shucked weight    0.0\n",
      "Viscera weight    0.0\n",
      "Shell weight      0.0\n",
      "Rings             0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Percentage of missing data in each column\n",
    "\n",
    "number_of_columns = data.shape[0]\n",
    "percentage_of_missing_data = data.isnull().sum()/number_of_columns\n",
    "print(percentage_of_missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "z_nrssDG51Q0",
    "outputId": "cc7bdd2d-6830-4040-ffe0-cc0a6490400d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>11.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sex  Length  Diameter  ...  Viscera weight  Shell weight   age\n",
       "0   M   0.455     0.365  ...          0.1010         0.150  16.5\n",
       "1   M   0.350     0.265  ...          0.0485         0.070   8.5\n",
       "2   F   0.530     0.420  ...          0.1415         0.210  10.5\n",
       "3   M   0.440     0.365  ...          0.1140         0.155  11.5\n",
       "4   I   0.330     0.255  ...          0.0395         0.055   8.5\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['age'] = data['Rings']+1.5\n",
    "data.drop('Rings', axis = 1, inplace = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "9c122298-471f-4f02-aa14-5d4218385f56",
    "_execution_state": "idle",
    "_uuid": "cc96273ceb750522bc1f48e76599bbe12f384e9d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "HWlVIp4u51Q2",
    "outputId": "8b9bad16-f9a8-4092-e93c-b8d43573e436"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb7b95f0a90>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Correlations\n",
    "\n",
    "sns.heatmap(data.corr(), annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 879
    },
    "id": "S6VFgpfX51Q5",
    "outputId": "e54717d7-ca84-40b8-94c2-ee57857c70c3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.hist(bins=100, figsize=(20,15), color='purple')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "ub9D7dkQqgop",
    "outputId": "306b134d-baee-4c6e-df38-21b8e7e17f7e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sex\n",
       "0   M\n",
       "1   M\n",
       "2   F\n",
       "3   M\n",
       "4   I\n",
       "5   I\n",
       "6   F\n",
       "7   F\n",
       "8   M\n",
       "9   F"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preprocess the categorical input feature, `Sex`:\n",
    "data_cat = data[[\"Sex\"]]\n",
    "data_cat.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "l2nvjJU3qgot"
   },
   "outputs": [],
   "source": [
    "data[\"M\"] = np.nan\n",
    "data[\"F\"] = np.nan\n",
    "data[\"I\"] = np.nan\n",
    "columnName='Sex'\n",
    "for i in range (len(data[columnName])):\n",
    "    if data[columnName][i]=='M':\n",
    "        data['M'][i]=1\n",
    "        data['F'][i]=0\n",
    "        data['I'][i]=0\n",
    "    elif data[columnName][i]=='F':\n",
    "        data['M'][i]=0\n",
    "        data['F'][i]=1\n",
    "        data['I'][i]=0\n",
    "    elif data[columnName][i]=='I' :\n",
    "        data['M'][i]=0\n",
    "        data['F'][i]=0\n",
    "        data['I'][i]=1\n",
    "data=data.drop(['Sex'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "TkCPRnCJhfHd",
    "outputId": "dc29d89e-4bb4-4083-d46e-8ce9ee4a2a94"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>age</th>\n",
       "      <th>M</th>\n",
       "      <th>F</th>\n",
       "      <th>I</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>16.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>11.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Length  Diameter  Height  Whole weight  ...   age    M    F    I\n",
       "0   0.455     0.365   0.095        0.5140  ...  16.5  1.0  0.0  0.0\n",
       "1   0.350     0.265   0.090        0.2255  ...   8.5  1.0  0.0  0.0\n",
       "2   0.530     0.420   0.135        0.6770  ...  10.5  0.0  1.0  0.0\n",
       "3   0.440     0.365   0.125        0.5160  ...  11.5  1.0  0.0  0.0\n",
       "4   0.330     0.255   0.080        0.2050  ...   8.5  0.0  0.0  1.0\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tOHgSyj851Rb",
    "outputId": "e47a835b-39c1-44df-a7a6-9e627d04cd84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4177, 10)\n",
      "(4177,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "X=data.drop(['age'],axis=1)\n",
    "y=data['age']\n",
    "X = preprocessing.normalize(X)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KIsMjfyt51Rq",
    "outputId": "0f2473d9-08a1-44d4-e97c-65f2f071bc37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2923, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3iPrnK_-51Ry"
   },
   "outputs": [],
   "source": [
    "train_data = X_train\n",
    "test_data = X_test\n",
    "train_labels = y_train.values.reshape(-1,1)\n",
    "test_labels = y_test.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3s5fAfkYxLYp"
   },
   "source": [
    "## Preparation of the dataset for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "zjBZmMz7NEo5"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "wJ1L0HBAjGxk"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "tensor_x = torch.tensor(train_data, dtype=torch.float).to(device) # transform to torch tensor\n",
    "tensor_x2 = torch.tensor(test_data, dtype=torch.float).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "eZCUXWsRjGxt"
   },
   "outputs": [],
   "source": [
    "tensor_y = torch.tensor(train_labels, dtype=torch.float).to(device)\n",
    "tensor_y2 = torch.tensor(test_labels, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "viaVSWpsGzDM"
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(tensor_x, tensor_y) # create your dataset\n",
    "test_dataset = TensorDataset(tensor_x2,tensor_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Q3zsgW2jGx8"
   },
   "source": [
    "## Building Feedforward Neural Networks with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "aR6T38SlO9oL"
   },
   "outputs": [],
   "source": [
    "def compute_loss(net, data_loader):\n",
    "    curr_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for cnt, (features, targets) in enumerate(data_loader):\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = net.forward(features)\n",
    "            loss = criterion(outputs, targets)\n",
    "            curr_loss += loss\n",
    "        return float(curr_loss)/cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3p2Us_9CNEpO"
   },
   "source": [
    "## Regularizations (Ensemble Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIqYJTB7O9oL"
   },
   "source": [
    "- [Ensembles: the only (almost) free Lunch in Machine Learning](https://towardsdatascience.com/ensembles-the-almost-free-lunch-in-machine-learning-91af7ebe5090)\n",
    "    - Build optimal ensembles of neural networks with PyTorch and NumPy\n",
    "    \n",
    "    \n",
    "- [Bagging and Dropout Learning](https://jrodthoughts.medium.com/bagging-and-dropout-learning-ae484023b0da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrPPRc_EO9oL"
   },
   "source": [
    "### Dropout: Ensemble Method Interpretation\n",
    "**Reference:** [Regularization by Sebastian Raschka](https://sebastianraschka.com/pdf/lecture-notes/stat479ss19/L10_regularization_slides.pdf)\n",
    "\n",
    "- In DL, we typically don't do regular ensembling (majority vote over a large number of networks, bagging, etc.) because it is very expensive to fit neural nets.\n",
    "- However, we know that the squared error for a prediction by a randomly selected model is larger than the squared error using an ensemble prediction (here, average over class probabilities).\n",
    "- Now, in dropout, we have a different model for each minibatch. \n",
    "- Via the minibatch iterations, we essentially sample over $M=2^h$ models, where h is the number of hidden units.\n",
    "- Restriction is that we have weight sharing over these models, which can be seen as a form of regularization.\n",
    "- During \"inference\" we can then average over all these models (but this is very expensive). \n",
    "- However, using the last model after training and scaling the predictions by a factor 1/(1-p) approximates the geometric mean and is much cheaper (actually, it's exactly the geometric mean if we have a linear model).\n",
    "\n",
    "### Dropout: More Practical Tips\n",
    "\n",
    "- Don't use Dropout if your model does not overfit .\n",
    "- However, in that case above, it is then recommended to increase the capacity to make it overfit, and then use dropout to be able to use a larger capacity model (but make it not overfit).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_xGYFrsjG0R"
   },
   "source": [
    "### Model A: 3 Hidden Layers Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eq-gTJ_SjG0v"
   },
   "source": [
    "### Create model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yEh7DV00HVU9",
    "outputId": "792fa269-d291-41d1-f1c5-3b43daadab89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "n_iters = 16000\n",
    "num_epochs = n_iters / (len(train_data) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "print(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "CoH9zZ4sHX4T"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # create your dataloader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "pHP-6Gbrt223"
   },
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim):\n",
    "        super(FNN, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.linear4 = nn.Linear(hidden_dim3, output_dim)\n",
    "        #self.relu4 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.linear3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.linear4(out)\n",
    "        #out = self.relu4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "hW1TdbRVt225"
   },
   "outputs": [],
   "source": [
    "input_dim = tensor_x.shape[1]\n",
    "output_dim = 1\n",
    "hidden_dim1 = 100\n",
    "hidden_dim2 = 2000\n",
    "hidden_dim3 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "3xR4dZyut227"
   },
   "outputs": [],
   "source": [
    "model = FNN(input_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yZTOUPrkP5JX",
    "outputId": "330b996b-0223-49af-83e3-a11ea2c9f0e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 100]           1,100\n",
      "              ReLU-2               [-1, 1, 100]               0\n",
      "            Linear-3              [-1, 1, 2000]         202,000\n",
      "           Dropout-4              [-1, 1, 2000]               0\n",
      "              ReLU-5              [-1, 1, 2000]               0\n",
      "            Linear-6               [-1, 1, 100]         200,100\n",
      "              ReLU-7               [-1, 1, 100]               0\n",
      "           Dropout-8               [-1, 1, 100]               0\n",
      "            Linear-9                 [-1, 1, 1]             101\n",
      "================================================================\n",
      "Total params: 403,301\n",
      "Trainable params: 403,301\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 1.54\n",
      "Estimated Total Size (MB): 1.59\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1,tensor_x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "irnPkz-DP5Ji"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 0.00001\n",
    "LAMBDA = 0.08\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=LAMBDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVRGbjHok2SR"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yIiIq8Kek2SS",
    "outputId": "780a6639-986b-4c9f-9556-7978d8f45755"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/273 | Batch 000/059 | Loss: 7034.7866\n",
      "Epoch: 001/273 Train Loss: 958.7271\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 002/273 | Batch 000/059 | Loss: 755.4784\n",
      "Epoch: 002/273 Train Loss: 466.5528\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 003/273 | Batch 000/059 | Loss: 490.6847\n",
      "Epoch: 003/273 Train Loss: 421.8385\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 004/273 | Batch 000/059 | Loss: 382.5484\n",
      "Epoch: 004/273 Train Loss: 387.9421\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 005/273 | Batch 000/059 | Loss: 416.9127\n",
      "Epoch: 005/273 Train Loss: 376.2651\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 006/273 | Batch 000/059 | Loss: 463.5710\n",
      "Epoch: 006/273 Train Loss: 368.6197\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 007/273 | Batch 000/059 | Loss: 487.5338\n",
      "Epoch: 007/273 Train Loss: 360.4701\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 008/273 | Batch 000/059 | Loss: 587.7026\n",
      "Epoch: 008/273 Train Loss: 366.6722\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 009/273 | Batch 000/059 | Loss: 467.0023\n",
      "Epoch: 009/273 Train Loss: 356.7021\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 010/273 | Batch 000/059 | Loss: 273.7931\n",
      "Epoch: 010/273 Train Loss: 353.0950\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 011/273 | Batch 000/059 | Loss: 480.9841\n",
      "Epoch: 011/273 Train Loss: 351.1959\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 012/273 | Batch 000/059 | Loss: 347.8577\n",
      "Epoch: 012/273 Train Loss: 357.4401\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 013/273 | Batch 000/059 | Loss: 744.6729\n",
      "Epoch: 013/273 Train Loss: 348.9582\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 014/273 | Batch 000/059 | Loss: 398.2275\n",
      "Epoch: 014/273 Train Loss: 346.9373\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 015/273 | Batch 000/059 | Loss: 578.8876\n",
      "Epoch: 015/273 Train Loss: 347.5475\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 016/273 | Batch 000/059 | Loss: 225.8729\n",
      "Epoch: 016/273 Train Loss: 347.3450\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 017/273 | Batch 000/059 | Loss: 390.0198\n",
      "Epoch: 017/273 Train Loss: 344.4931\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 018/273 | Batch 000/059 | Loss: 356.9957\n",
      "Epoch: 018/273 Train Loss: 344.4509\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 019/273 | Batch 000/059 | Loss: 369.1617\n",
      "Epoch: 019/273 Train Loss: 346.0433\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 020/273 | Batch 000/059 | Loss: 509.2168\n",
      "Epoch: 020/273 Train Loss: 341.3534\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 021/273 | Batch 000/059 | Loss: 427.7515\n",
      "Epoch: 021/273 Train Loss: 340.7932\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 022/273 | Batch 000/059 | Loss: 315.8156\n",
      "Epoch: 022/273 Train Loss: 343.2709\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 023/273 | Batch 000/059 | Loss: 277.5758\n",
      "Epoch: 023/273 Train Loss: 339.3530\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 024/273 | Batch 000/059 | Loss: 456.9866\n",
      "Epoch: 024/273 Train Loss: 338.8453\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 025/273 | Batch 000/059 | Loss: 350.7787\n",
      "Epoch: 025/273 Train Loss: 339.1831\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 026/273 | Batch 000/059 | Loss: 303.6981\n",
      "Epoch: 026/273 Train Loss: 338.5444\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 027/273 | Batch 000/059 | Loss: 235.1920\n",
      "Epoch: 027/273 Train Loss: 335.7083\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 028/273 | Batch 000/059 | Loss: 271.2441\n",
      "Epoch: 028/273 Train Loss: 335.2935\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 029/273 | Batch 000/059 | Loss: 343.8351\n",
      "Epoch: 029/273 Train Loss: 334.2523\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 030/273 | Batch 000/059 | Loss: 283.5188\n",
      "Epoch: 030/273 Train Loss: 333.5488\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 031/273 | Batch 000/059 | Loss: 339.8154\n",
      "Epoch: 031/273 Train Loss: 333.6536\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 032/273 | Batch 000/059 | Loss: 463.4139\n",
      "Epoch: 032/273 Train Loss: 332.9040\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 033/273 | Batch 000/059 | Loss: 549.7554\n",
      "Epoch: 033/273 Train Loss: 332.3620\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 034/273 | Batch 000/059 | Loss: 368.3878\n",
      "Epoch: 034/273 Train Loss: 331.6185\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 035/273 | Batch 000/059 | Loss: 374.1784\n",
      "Epoch: 035/273 Train Loss: 332.7313\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 036/273 | Batch 000/059 | Loss: 732.9568\n",
      "Epoch: 036/273 Train Loss: 331.0314\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 037/273 | Batch 000/059 | Loss: 420.7764\n",
      "Epoch: 037/273 Train Loss: 332.2177\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 038/273 | Batch 000/059 | Loss: 317.7124\n",
      "Epoch: 038/273 Train Loss: 328.6012\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 039/273 | Batch 000/059 | Loss: 267.0216\n",
      "Epoch: 039/273 Train Loss: 329.4799\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 040/273 | Batch 000/059 | Loss: 210.8596\n",
      "Epoch: 040/273 Train Loss: 327.5070\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 041/273 | Batch 000/059 | Loss: 331.8616\n",
      "Epoch: 041/273 Train Loss: 327.6524\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 042/273 | Batch 000/059 | Loss: 417.3626\n",
      "Epoch: 042/273 Train Loss: 328.3611\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 043/273 | Batch 000/059 | Loss: 374.7534\n",
      "Epoch: 043/273 Train Loss: 326.4402\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 044/273 | Batch 000/059 | Loss: 489.5679\n",
      "Epoch: 044/273 Train Loss: 329.0009\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 045/273 | Batch 000/059 | Loss: 456.6680\n",
      "Epoch: 045/273 Train Loss: 326.5840\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 046/273 | Batch 000/059 | Loss: 512.9760\n",
      "Epoch: 046/273 Train Loss: 323.6464\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 047/273 | Batch 000/059 | Loss: 422.3454\n",
      "Epoch: 047/273 Train Loss: 323.8459\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 048/273 | Batch 000/059 | Loss: 281.5064\n",
      "Epoch: 048/273 Train Loss: 326.7061\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 049/273 | Batch 000/059 | Loss: 390.4804\n",
      "Epoch: 049/273 Train Loss: 322.8916\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 050/273 | Batch 000/059 | Loss: 268.1430\n",
      "Epoch: 050/273 Train Loss: 323.0427\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 051/273 | Batch 000/059 | Loss: 224.0401\n",
      "Epoch: 051/273 Train Loss: 320.4454\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 052/273 | Batch 000/059 | Loss: 348.9537\n",
      "Epoch: 052/273 Train Loss: 321.5010\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 053/273 | Batch 000/059 | Loss: 240.5020\n",
      "Epoch: 053/273 Train Loss: 326.8302\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 054/273 | Batch 000/059 | Loss: 358.8042\n",
      "Epoch: 054/273 Train Loss: 318.4406\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 055/273 | Batch 000/059 | Loss: 406.5372\n",
      "Epoch: 055/273 Train Loss: 319.7553\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 056/273 | Batch 000/059 | Loss: 477.2517\n",
      "Epoch: 056/273 Train Loss: 317.9179\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 057/273 | Batch 000/059 | Loss: 314.3431\n",
      "Epoch: 057/273 Train Loss: 316.9409\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 058/273 | Batch 000/059 | Loss: 309.5373\n",
      "Epoch: 058/273 Train Loss: 316.6933\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 059/273 | Batch 000/059 | Loss: 440.3954\n",
      "Epoch: 059/273 Train Loss: 315.4816\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 060/273 | Batch 000/059 | Loss: 293.3861\n",
      "Epoch: 060/273 Train Loss: 314.4125\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 061/273 | Batch 000/059 | Loss: 187.7597\n",
      "Epoch: 061/273 Train Loss: 313.5950\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 062/273 | Batch 000/059 | Loss: 193.5080\n",
      "Epoch: 062/273 Train Loss: 316.7641\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 063/273 | Batch 000/059 | Loss: 436.7706\n",
      "Epoch: 063/273 Train Loss: 311.9583\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 064/273 | Batch 000/059 | Loss: 542.2399\n",
      "Epoch: 064/273 Train Loss: 312.6580\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 065/273 | Batch 000/059 | Loss: 326.8052\n",
      "Epoch: 065/273 Train Loss: 310.6543\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 066/273 | Batch 000/059 | Loss: 485.8775\n",
      "Epoch: 066/273 Train Loss: 311.1679\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 067/273 | Batch 000/059 | Loss: 155.3586\n",
      "Epoch: 067/273 Train Loss: 311.2138\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 068/273 | Batch 000/059 | Loss: 299.2664\n",
      "Epoch: 068/273 Train Loss: 308.0822\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 069/273 | Batch 000/059 | Loss: 271.2341\n",
      "Epoch: 069/273 Train Loss: 307.2117\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 070/273 | Batch 000/059 | Loss: 222.3567\n",
      "Epoch: 070/273 Train Loss: 308.9786\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 071/273 | Batch 000/059 | Loss: 286.4481\n",
      "Epoch: 071/273 Train Loss: 310.3109\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 072/273 | Batch 000/059 | Loss: 522.9934\n",
      "Epoch: 072/273 Train Loss: 306.0113\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 073/273 | Batch 000/059 | Loss: 306.2717\n",
      "Epoch: 073/273 Train Loss: 303.9200\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 074/273 | Batch 000/059 | Loss: 333.0732\n",
      "Epoch: 074/273 Train Loss: 304.0117\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 075/273 | Batch 000/059 | Loss: 306.0688\n",
      "Epoch: 075/273 Train Loss: 302.0618\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 076/273 | Batch 000/059 | Loss: 182.6344\n",
      "Epoch: 076/273 Train Loss: 302.0924\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 077/273 | Batch 000/059 | Loss: 454.5327\n",
      "Epoch: 077/273 Train Loss: 301.3990\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 078/273 | Batch 000/059 | Loss: 430.1878\n",
      "Epoch: 078/273 Train Loss: 301.3404\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 079/273 | Batch 000/059 | Loss: 330.5820\n",
      "Epoch: 079/273 Train Loss: 299.6413\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 080/273 | Batch 000/059 | Loss: 401.9473\n",
      "Epoch: 080/273 Train Loss: 297.8981\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 081/273 | Batch 000/059 | Loss: 342.5818\n",
      "Epoch: 081/273 Train Loss: 296.5520\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 082/273 | Batch 000/059 | Loss: 211.2473\n",
      "Epoch: 082/273 Train Loss: 295.5991\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 083/273 | Batch 000/059 | Loss: 461.3612\n",
      "Epoch: 083/273 Train Loss: 297.7169\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 084/273 | Batch 000/059 | Loss: 360.6877\n",
      "Epoch: 084/273 Train Loss: 297.5546\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 085/273 | Batch 000/059 | Loss: 270.5818\n",
      "Epoch: 085/273 Train Loss: 292.8783\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 086/273 | Batch 000/059 | Loss: 340.8132\n",
      "Epoch: 086/273 Train Loss: 292.3592\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 087/273 | Batch 000/059 | Loss: 339.7818\n",
      "Epoch: 087/273 Train Loss: 297.3579\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 088/273 | Batch 000/059 | Loss: 311.9579\n",
      "Epoch: 088/273 Train Loss: 289.2822\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 089/273 | Batch 000/059 | Loss: 384.4835\n",
      "Epoch: 089/273 Train Loss: 288.0097\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 090/273 | Batch 000/059 | Loss: 342.1793\n",
      "Epoch: 090/273 Train Loss: 288.5458\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 091/273 | Batch 000/059 | Loss: 434.5416\n",
      "Epoch: 091/273 Train Loss: 285.6555\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 092/273 | Batch 000/059 | Loss: 322.0562\n",
      "Epoch: 092/273 Train Loss: 284.2152\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 093/273 | Batch 000/059 | Loss: 499.1951\n",
      "Epoch: 093/273 Train Loss: 283.8194\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 094/273 | Batch 000/059 | Loss: 324.2421\n",
      "Epoch: 094/273 Train Loss: 282.0841\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 095/273 | Batch 000/059 | Loss: 273.6216\n",
      "Epoch: 095/273 Train Loss: 282.8984\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 096/273 | Batch 000/059 | Loss: 233.7041\n",
      "Epoch: 096/273 Train Loss: 279.3985\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 097/273 | Batch 000/059 | Loss: 322.5499\n",
      "Epoch: 097/273 Train Loss: 278.4117\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 098/273 | Batch 000/059 | Loss: 232.5201\n",
      "Epoch: 098/273 Train Loss: 276.7808\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 099/273 | Batch 000/059 | Loss: 253.4517\n",
      "Epoch: 099/273 Train Loss: 276.2846\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 100/273 | Batch 000/059 | Loss: 321.1731\n",
      "Epoch: 100/273 Train Loss: 277.4470\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 101/273 | Batch 000/059 | Loss: 275.5862\n",
      "Epoch: 101/273 Train Loss: 273.0062\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 102/273 | Batch 000/059 | Loss: 475.6258\n",
      "Epoch: 102/273 Train Loss: 271.7637\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 103/273 | Batch 000/059 | Loss: 271.7036\n",
      "Epoch: 103/273 Train Loss: 271.4108\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 104/273 | Batch 000/059 | Loss: 243.9143\n",
      "Epoch: 104/273 Train Loss: 269.2265\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 105/273 | Batch 000/059 | Loss: 285.4893\n",
      "Epoch: 105/273 Train Loss: 268.6022\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 106/273 | Batch 000/059 | Loss: 491.7965\n",
      "Epoch: 106/273 Train Loss: 268.7322\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 107/273 | Batch 000/059 | Loss: 437.8004\n",
      "Epoch: 107/273 Train Loss: 264.8343\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 108/273 | Batch 000/059 | Loss: 249.3150\n",
      "Epoch: 108/273 Train Loss: 266.5443\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 109/273 | Batch 000/059 | Loss: 246.6682\n",
      "Epoch: 109/273 Train Loss: 263.6825\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 110/273 | Batch 000/059 | Loss: 350.3837\n",
      "Epoch: 110/273 Train Loss: 262.4233\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 111/273 | Batch 000/059 | Loss: 185.5693\n",
      "Epoch: 111/273 Train Loss: 261.8638\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 112/273 | Batch 000/059 | Loss: 334.0375\n",
      "Epoch: 112/273 Train Loss: 260.3675\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 113/273 | Batch 000/059 | Loss: 345.9984\n",
      "Epoch: 113/273 Train Loss: 262.9321\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 114/273 | Batch 000/059 | Loss: 314.0334\n",
      "Epoch: 114/273 Train Loss: 256.4047\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 115/273 | Batch 000/059 | Loss: 171.3390\n",
      "Epoch: 115/273 Train Loss: 254.1787\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 116/273 | Batch 000/059 | Loss: 108.3815\n",
      "Epoch: 116/273 Train Loss: 254.2670\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 117/273 | Batch 000/059 | Loss: 367.3087\n",
      "Epoch: 117/273 Train Loss: 251.6564\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 118/273 | Batch 000/059 | Loss: 225.3507\n",
      "Epoch: 118/273 Train Loss: 250.4350\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 119/273 | Batch 000/059 | Loss: 165.9447\n",
      "Epoch: 119/273 Train Loss: 251.3388\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 120/273 | Batch 000/059 | Loss: 172.7834\n",
      "Epoch: 120/273 Train Loss: 254.4993\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 121/273 | Batch 000/059 | Loss: 387.9773\n",
      "Epoch: 121/273 Train Loss: 252.0247\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 122/273 | Batch 000/059 | Loss: 254.4693\n",
      "Epoch: 122/273 Train Loss: 246.0245\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 123/273 | Batch 000/059 | Loss: 195.8166\n",
      "Epoch: 123/273 Train Loss: 244.5526\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 124/273 | Batch 000/059 | Loss: 380.3047\n",
      "Epoch: 124/273 Train Loss: 243.9443\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 125/273 | Batch 000/059 | Loss: 245.1736\n",
      "Epoch: 125/273 Train Loss: 243.3911\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 126/273 | Batch 000/059 | Loss: 243.4395\n",
      "Epoch: 126/273 Train Loss: 241.3352\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 127/273 | Batch 000/059 | Loss: 159.7757\n",
      "Epoch: 127/273 Train Loss: 240.4716\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 128/273 | Batch 000/059 | Loss: 298.1667\n",
      "Epoch: 128/273 Train Loss: 251.4041\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 129/273 | Batch 000/059 | Loss: 243.0033\n",
      "Epoch: 129/273 Train Loss: 238.5179\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 130/273 | Batch 000/059 | Loss: 264.5801\n",
      "Epoch: 130/273 Train Loss: 240.3785\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 131/273 | Batch 000/059 | Loss: 220.0922\n",
      "Epoch: 131/273 Train Loss: 236.7409\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 132/273 | Batch 000/059 | Loss: 287.4008\n",
      "Epoch: 132/273 Train Loss: 237.2961\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 133/273 | Batch 000/059 | Loss: 394.9232\n",
      "Epoch: 133/273 Train Loss: 242.7435\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 134/273 | Batch 000/059 | Loss: 312.5577\n",
      "Epoch: 134/273 Train Loss: 234.8739\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 135/273 | Batch 000/059 | Loss: 348.7948\n",
      "Epoch: 135/273 Train Loss: 235.1601\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 136/273 | Batch 000/059 | Loss: 488.7205\n",
      "Epoch: 136/273 Train Loss: 233.2447\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 137/273 | Batch 000/059 | Loss: 202.0991\n",
      "Epoch: 137/273 Train Loss: 237.6377\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 138/273 | Batch 000/059 | Loss: 288.2457\n",
      "Epoch: 138/273 Train Loss: 237.7412\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 139/273 | Batch 000/059 | Loss: 195.5133\n",
      "Epoch: 139/273 Train Loss: 243.0706\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 140/273 | Batch 000/059 | Loss: 242.5893\n",
      "Epoch: 140/273 Train Loss: 231.2511\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 141/273 | Batch 000/059 | Loss: 140.0484\n",
      "Epoch: 141/273 Train Loss: 234.6358\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 142/273 | Batch 000/059 | Loss: 319.5516\n",
      "Epoch: 142/273 Train Loss: 230.3861\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 143/273 | Batch 000/059 | Loss: 254.3679\n",
      "Epoch: 143/273 Train Loss: 231.9827\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 144/273 | Batch 000/059 | Loss: 211.9978\n",
      "Epoch: 144/273 Train Loss: 228.8014\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 145/273 | Batch 000/059 | Loss: 263.4670\n",
      "Epoch: 145/273 Train Loss: 229.3525\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 146/273 | Batch 000/059 | Loss: 271.6077\n",
      "Epoch: 146/273 Train Loss: 232.5749\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 147/273 | Batch 000/059 | Loss: 113.1286\n",
      "Epoch: 147/273 Train Loss: 228.7623\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 148/273 | Batch 000/059 | Loss: 223.0123\n",
      "Epoch: 148/273 Train Loss: 227.4235\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 149/273 | Batch 000/059 | Loss: 232.9819\n",
      "Epoch: 149/273 Train Loss: 227.2299\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 150/273 | Batch 000/059 | Loss: 264.7615\n",
      "Epoch: 150/273 Train Loss: 227.6248\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 151/273 | Batch 000/059 | Loss: 341.5324\n",
      "Epoch: 151/273 Train Loss: 226.4185\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 152/273 | Batch 000/059 | Loss: 217.8491\n",
      "Epoch: 152/273 Train Loss: 226.9342\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 153/273 | Batch 000/059 | Loss: 172.1517\n",
      "Epoch: 153/273 Train Loss: 226.0416\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 154/273 | Batch 000/059 | Loss: 181.3022\n",
      "Epoch: 154/273 Train Loss: 226.7543\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 155/273 | Batch 000/059 | Loss: 280.3002\n",
      "Epoch: 155/273 Train Loss: 229.7663\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 156/273 | Batch 000/059 | Loss: 153.8642\n",
      "Epoch: 156/273 Train Loss: 224.9045\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 157/273 | Batch 000/059 | Loss: 242.7467\n",
      "Epoch: 157/273 Train Loss: 226.2723\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 158/273 | Batch 000/059 | Loss: 197.6290\n",
      "Epoch: 158/273 Train Loss: 227.6388\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 159/273 | Batch 000/059 | Loss: 159.2892\n",
      "Epoch: 159/273 Train Loss: 224.3057\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 160/273 | Batch 000/059 | Loss: 223.5318\n",
      "Epoch: 160/273 Train Loss: 227.4450\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 161/273 | Batch 000/059 | Loss: 225.8597\n",
      "Epoch: 161/273 Train Loss: 224.2603\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 162/273 | Batch 000/059 | Loss: 161.0520\n",
      "Epoch: 162/273 Train Loss: 223.5386\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 163/273 | Batch 000/059 | Loss: 331.1199\n",
      "Epoch: 163/273 Train Loss: 224.8753\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 164/273 | Batch 000/059 | Loss: 394.2832\n",
      "Epoch: 164/273 Train Loss: 223.2760\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 165/273 | Batch 000/059 | Loss: 165.5900\n",
      "Epoch: 165/273 Train Loss: 222.9163\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 166/273 | Batch 000/059 | Loss: 278.3528\n",
      "Epoch: 166/273 Train Loss: 222.7381\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 167/273 | Batch 000/059 | Loss: 197.3186\n",
      "Epoch: 167/273 Train Loss: 228.5838\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 168/273 | Batch 000/059 | Loss: 209.5769\n",
      "Epoch: 168/273 Train Loss: 225.4831\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 169/273 | Batch 000/059 | Loss: 252.6799\n",
      "Epoch: 169/273 Train Loss: 222.5169\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 170/273 | Batch 000/059 | Loss: 233.2719\n",
      "Epoch: 170/273 Train Loss: 224.8586\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 171/273 | Batch 000/059 | Loss: 165.6386\n",
      "Epoch: 171/273 Train Loss: 223.0155\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 172/273 | Batch 000/059 | Loss: 276.2253\n",
      "Epoch: 172/273 Train Loss: 227.5430\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 173/273 | Batch 000/059 | Loss: 223.6826\n",
      "Epoch: 173/273 Train Loss: 222.0265\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 174/273 | Batch 000/059 | Loss: 270.0497\n",
      "Epoch: 174/273 Train Loss: 226.4943\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 175/273 | Batch 000/059 | Loss: 166.8102\n",
      "Epoch: 175/273 Train Loss: 223.8695\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 176/273 | Batch 000/059 | Loss: 199.2428\n",
      "Epoch: 176/273 Train Loss: 225.1875\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 177/273 | Batch 000/059 | Loss: 284.1739\n",
      "Epoch: 177/273 Train Loss: 221.7049\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 178/273 | Batch 000/059 | Loss: 362.3015\n",
      "Epoch: 178/273 Train Loss: 228.4853\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 179/273 | Batch 000/059 | Loss: 306.7835\n",
      "Epoch: 179/273 Train Loss: 222.3289\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 180/273 | Batch 000/059 | Loss: 250.2794\n",
      "Epoch: 180/273 Train Loss: 223.7287\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 181/273 | Batch 000/059 | Loss: 212.3916\n",
      "Epoch: 181/273 Train Loss: 227.7881\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 182/273 | Batch 000/059 | Loss: 271.8088\n",
      "Epoch: 182/273 Train Loss: 220.8480\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 183/273 | Batch 000/059 | Loss: 426.7372\n",
      "Epoch: 183/273 Train Loss: 234.4112\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 184/273 | Batch 000/059 | Loss: 239.6219\n",
      "Epoch: 184/273 Train Loss: 221.4425\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 185/273 | Batch 000/059 | Loss: 267.5703\n",
      "Epoch: 185/273 Train Loss: 221.0029\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 186/273 | Batch 000/059 | Loss: 247.7230\n",
      "Epoch: 186/273 Train Loss: 221.4934\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 187/273 | Batch 000/059 | Loss: 266.8040\n",
      "Epoch: 187/273 Train Loss: 220.7589\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 188/273 | Batch 000/059 | Loss: 173.3178\n",
      "Epoch: 188/273 Train Loss: 220.6047\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 189/273 | Batch 000/059 | Loss: 377.8898\n",
      "Epoch: 189/273 Train Loss: 220.4717\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 190/273 | Batch 000/059 | Loss: 180.6816\n",
      "Epoch: 190/273 Train Loss: 236.1698\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 191/273 | Batch 000/059 | Loss: 227.8448\n",
      "Epoch: 191/273 Train Loss: 223.3776\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 192/273 | Batch 000/059 | Loss: 116.3905\n",
      "Epoch: 192/273 Train Loss: 230.7036\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 193/273 | Batch 000/059 | Loss: 229.4934\n",
      "Epoch: 193/273 Train Loss: 222.3994\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 194/273 | Batch 000/059 | Loss: 216.8557\n",
      "Epoch: 194/273 Train Loss: 221.3779\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 195/273 | Batch 000/059 | Loss: 294.8737\n",
      "Epoch: 195/273 Train Loss: 223.4628\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 196/273 | Batch 000/059 | Loss: 215.4585\n",
      "Epoch: 196/273 Train Loss: 221.9299\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 197/273 | Batch 000/059 | Loss: 279.9434\n",
      "Epoch: 197/273 Train Loss: 221.7760\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 198/273 | Batch 000/059 | Loss: 265.1471\n",
      "Epoch: 198/273 Train Loss: 220.5542\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 199/273 | Batch 000/059 | Loss: 345.7020\n",
      "Epoch: 199/273 Train Loss: 221.7767\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 200/273 | Batch 000/059 | Loss: 219.3724\n",
      "Epoch: 200/273 Train Loss: 220.9566\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 201/273 | Batch 000/059 | Loss: 272.8441\n",
      "Epoch: 201/273 Train Loss: 220.6537\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 202/273 | Batch 000/059 | Loss: 366.6212\n",
      "Epoch: 202/273 Train Loss: 219.4479\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 203/273 | Batch 000/059 | Loss: 278.4408\n",
      "Epoch: 203/273 Train Loss: 221.1034\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 204/273 | Batch 000/059 | Loss: 237.6998\n",
      "Epoch: 204/273 Train Loss: 220.7910\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 205/273 | Batch 000/059 | Loss: 323.6945\n",
      "Epoch: 205/273 Train Loss: 223.5158\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 206/273 | Batch 000/059 | Loss: 320.4538\n",
      "Epoch: 206/273 Train Loss: 220.5389\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 207/273 | Batch 000/059 | Loss: 257.2078\n",
      "Epoch: 207/273 Train Loss: 222.1633\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 208/273 | Batch 000/059 | Loss: 215.2887\n",
      "Epoch: 208/273 Train Loss: 221.9705\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 209/273 | Batch 000/059 | Loss: 271.9573\n",
      "Epoch: 209/273 Train Loss: 223.8521\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 210/273 | Batch 000/059 | Loss: 90.7657\n",
      "Epoch: 210/273 Train Loss: 218.9073\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 211/273 | Batch 000/059 | Loss: 204.9545\n",
      "Epoch: 211/273 Train Loss: 218.8984\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 212/273 | Batch 000/059 | Loss: 207.1547\n",
      "Epoch: 212/273 Train Loss: 221.7037\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 213/273 | Batch 000/059 | Loss: 195.2424\n",
      "Epoch: 213/273 Train Loss: 220.8157\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 214/273 | Batch 000/059 | Loss: 380.3419\n",
      "Epoch: 214/273 Train Loss: 218.7238\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 215/273 | Batch 000/059 | Loss: 245.7737\n",
      "Epoch: 215/273 Train Loss: 220.5729\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 216/273 | Batch 000/059 | Loss: 265.0254\n",
      "Epoch: 216/273 Train Loss: 218.7469\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 217/273 | Batch 000/059 | Loss: 131.1620\n",
      "Epoch: 217/273 Train Loss: 219.7936\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 218/273 | Batch 000/059 | Loss: 217.9442\n",
      "Epoch: 218/273 Train Loss: 218.9845\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 219/273 | Batch 000/059 | Loss: 232.6161\n",
      "Epoch: 219/273 Train Loss: 219.3608\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 220/273 | Batch 000/059 | Loss: 190.4919\n",
      "Epoch: 220/273 Train Loss: 218.8033\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 221/273 | Batch 000/059 | Loss: 160.7785\n",
      "Epoch: 221/273 Train Loss: 219.3865\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 222/273 | Batch 000/059 | Loss: 288.8756\n",
      "Epoch: 222/273 Train Loss: 221.8835\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 223/273 | Batch 000/059 | Loss: 208.2219\n",
      "Epoch: 223/273 Train Loss: 222.7065\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 224/273 | Batch 000/059 | Loss: 200.6077\n",
      "Epoch: 224/273 Train Loss: 218.1300\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 225/273 | Batch 000/059 | Loss: 432.7260\n",
      "Epoch: 225/273 Train Loss: 218.2172\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 226/273 | Batch 000/059 | Loss: 395.2570\n",
      "Epoch: 226/273 Train Loss: 219.4771\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 227/273 | Batch 000/059 | Loss: 319.7149\n",
      "Epoch: 227/273 Train Loss: 220.6830\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 228/273 | Batch 000/059 | Loss: 365.1574\n",
      "Epoch: 228/273 Train Loss: 218.0302\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 229/273 | Batch 000/059 | Loss: 346.9152\n",
      "Epoch: 229/273 Train Loss: 218.7589\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 230/273 | Batch 000/059 | Loss: 99.9078\n",
      "Epoch: 230/273 Train Loss: 218.5108\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 231/273 | Batch 000/059 | Loss: 242.8849\n",
      "Epoch: 231/273 Train Loss: 218.6672\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 232/273 | Batch 000/059 | Loss: 288.4591\n",
      "Epoch: 232/273 Train Loss: 220.6074\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 233/273 | Batch 000/059 | Loss: 285.1277\n",
      "Epoch: 233/273 Train Loss: 219.1494\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 234/273 | Batch 000/059 | Loss: 216.5799\n",
      "Epoch: 234/273 Train Loss: 220.9520\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 235/273 | Batch 000/059 | Loss: 200.1839\n",
      "Epoch: 235/273 Train Loss: 221.3479\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 236/273 | Batch 000/059 | Loss: 379.9444\n",
      "Epoch: 236/273 Train Loss: 219.7733\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 237/273 | Batch 000/059 | Loss: 283.1606\n",
      "Epoch: 237/273 Train Loss: 221.6094\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 238/273 | Batch 000/059 | Loss: 309.2093\n",
      "Epoch: 238/273 Train Loss: 218.1794\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 239/273 | Batch 000/059 | Loss: 306.9763\n",
      "Epoch: 239/273 Train Loss: 223.6931\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 240/273 | Batch 000/059 | Loss: 217.1571\n",
      "Epoch: 240/273 Train Loss: 218.5498\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 241/273 | Batch 000/059 | Loss: 148.0016\n",
      "Epoch: 241/273 Train Loss: 222.4922\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 242/273 | Batch 000/059 | Loss: 191.9226\n",
      "Epoch: 242/273 Train Loss: 218.1603\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 243/273 | Batch 000/059 | Loss: 154.6492\n",
      "Epoch: 243/273 Train Loss: 220.2614\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 244/273 | Batch 000/059 | Loss: 145.0584\n",
      "Epoch: 244/273 Train Loss: 218.1277\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 245/273 | Batch 000/059 | Loss: 205.7952\n",
      "Epoch: 245/273 Train Loss: 218.7533\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 246/273 | Batch 000/059 | Loss: 180.4459\n",
      "Epoch: 246/273 Train Loss: 218.9154\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 247/273 | Batch 000/059 | Loss: 316.9504\n",
      "Epoch: 247/273 Train Loss: 217.5894\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 248/273 | Batch 000/059 | Loss: 205.5139\n",
      "Epoch: 248/273 Train Loss: 219.0992\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 249/273 | Batch 000/059 | Loss: 279.4456\n",
      "Epoch: 249/273 Train Loss: 217.5734\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 250/273 | Batch 000/059 | Loss: 192.0509\n",
      "Epoch: 250/273 Train Loss: 218.1254\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 251/273 | Batch 000/059 | Loss: 393.6732\n",
      "Epoch: 251/273 Train Loss: 220.4620\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 252/273 | Batch 000/059 | Loss: 191.0709\n",
      "Epoch: 252/273 Train Loss: 217.0396\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 253/273 | Batch 000/059 | Loss: 186.1527\n",
      "Epoch: 253/273 Train Loss: 221.3155\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 254/273 | Batch 000/059 | Loss: 286.8251\n",
      "Epoch: 254/273 Train Loss: 218.6679\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 255/273 | Batch 000/059 | Loss: 416.7575\n",
      "Epoch: 255/273 Train Loss: 221.2132\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 256/273 | Batch 000/059 | Loss: 256.6750\n",
      "Epoch: 256/273 Train Loss: 224.5751\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 257/273 | Batch 000/059 | Loss: 202.1868\n",
      "Epoch: 257/273 Train Loss: 216.7386\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 258/273 | Batch 000/059 | Loss: 265.9658\n",
      "Epoch: 258/273 Train Loss: 216.5368\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 259/273 | Batch 000/059 | Loss: 183.5432\n",
      "Epoch: 259/273 Train Loss: 216.3533\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 260/273 | Batch 000/059 | Loss: 176.0989\n",
      "Epoch: 260/273 Train Loss: 217.2089\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 261/273 | Batch 000/059 | Loss: 277.0405\n",
      "Epoch: 261/273 Train Loss: 216.7319\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 262/273 | Batch 000/059 | Loss: 228.4139\n",
      "Epoch: 262/273 Train Loss: 223.0881\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 263/273 | Batch 000/059 | Loss: 297.2345\n",
      "Epoch: 263/273 Train Loss: 219.3813\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 264/273 | Batch 000/059 | Loss: 289.8321\n",
      "Epoch: 264/273 Train Loss: 218.4096\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 265/273 | Batch 000/059 | Loss: 213.6229\n",
      "Epoch: 265/273 Train Loss: 216.2170\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 266/273 | Batch 000/059 | Loss: 236.4590\n",
      "Epoch: 266/273 Train Loss: 217.0683\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 267/273 | Batch 000/059 | Loss: 337.9185\n",
      "Epoch: 267/273 Train Loss: 215.9647\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 268/273 | Batch 000/059 | Loss: 164.4969\n",
      "Epoch: 268/273 Train Loss: 216.4940\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 269/273 | Batch 000/059 | Loss: 275.6552\n",
      "Epoch: 269/273 Train Loss: 216.4936\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 270/273 | Batch 000/059 | Loss: 223.8634\n",
      "Epoch: 270/273 Train Loss: 220.2322\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 271/273 | Batch 000/059 | Loss: 270.1128\n",
      "Epoch: 271/273 Train Loss: 219.1526\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 272/273 | Batch 000/059 | Loss: 183.6352\n",
      "Epoch: 272/273 Train Loss: 220.4152\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 273/273 | Batch 000/059 | Loss: 406.1989\n",
      "Epoch: 273/273 Train Loss: 217.5656\n",
      "Time elapsed: 0.80 min\n",
      "Total Training Time: 0.80 min\n",
      "Training Loss: 217.57\n",
      "Test Loss: 229.48\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "minibatch_cost = []\n",
    "epoch_cost = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(device)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        outputs = model(features)\n",
    "        \n",
    "        cost = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        minibatch_cost.append(cost)\n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 1000:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "       \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        cost = compute_loss(model, train_loader)\n",
    "        epoch_cost.append(cost)\n",
    "        print('Epoch: %03d/%03d Train Loss: %.4f' % (\n",
    "                epoch+1, num_epochs, cost))\n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "        \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "    print('Training Loss: %.2f' % compute_loss(model, train_loader))\n",
    "    print('Test Loss: %.2f' % compute_loss(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "ZId0EZvqNEp-",
    "outputId": "d4ce07a3-18cc-419d-9005-32bc134c0b31"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the loss curve\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(len(epoch_cost)), epoch_cost)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross entropy loss\")\n",
    "plt.title(\"Loss of training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZK2nI5fg9t1K"
   },
   "source": [
    "### Model B: 1 Hidden Layers Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsZcoY4f9t1P"
   },
   "source": [
    "### Create model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4VGDKZm9t1R",
    "outputId": "8fa48c34-6daf-4f71-d21b-29f66f148724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "n_iters = 30000\n",
    "num_epochs = n_iters / (len(train_data) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "print(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "id": "LwS_3q6j9t1c"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # create your dataloader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "k4LO4zDT9t1e"
   },
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, output_dim):\n",
    "        super(FNN, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear4 = nn.Linear(hidden_dim1, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.linear4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "vXjJVVlZ9t1h"
   },
   "outputs": [],
   "source": [
    "input_dim = tensor_x.shape[1]\n",
    "output_dim = 1\n",
    "hidden_dim1 = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "Sqlh3Yuc9t1j"
   },
   "outputs": [],
   "source": [
    "model = FNN(input_dim, hidden_dim1, output_dim)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jWTxycOP9t1n",
    "outputId": "50331b4a-f3b9-4293-8682-4f2cfa2c9594"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 256]           2,816\n",
      "           Dropout-2               [-1, 1, 256]               0\n",
      "              ReLU-3               [-1, 1, 256]               0\n",
      "            Linear-4                 [-1, 1, 1]             257\n",
      "================================================================\n",
      "Total params: 3,073\n",
      "Trainable params: 3,073\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1,tensor_x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "id": "u2iXIwZs9t1p"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 0.0001\n",
    "LAMBDA = 0.05\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=LAMBDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPprd-M8V7hz"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b_B6axqV7hz",
    "outputId": "58bcbadb-0cd2-4c76-8cbf-bc19f42ceafd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/513 | Batch 000/059 | Loss: 7143.6631\n",
      "Epoch: 001/513 Train Loss: 366.7753\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 002/513 | Batch 000/059 | Loss: 325.7243\n",
      "Epoch: 002/513 Train Loss: 356.2976\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 003/513 | Batch 000/059 | Loss: 384.9210\n",
      "Epoch: 003/513 Train Loss: 351.7988\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 004/513 | Batch 000/059 | Loss: 574.1559\n",
      "Epoch: 004/513 Train Loss: 340.2809\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 005/513 | Batch 000/059 | Loss: 336.5417\n",
      "Epoch: 005/513 Train Loss: 343.7890\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 006/513 | Batch 000/059 | Loss: 207.1764\n",
      "Epoch: 006/513 Train Loss: 335.2269\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 007/513 | Batch 000/059 | Loss: 287.7484\n",
      "Epoch: 007/513 Train Loss: 333.5784\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 008/513 | Batch 000/059 | Loss: 530.5637\n",
      "Epoch: 008/513 Train Loss: 334.0661\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 009/513 | Batch 000/059 | Loss: 324.0823\n",
      "Epoch: 009/513 Train Loss: 329.7777\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 010/513 | Batch 000/059 | Loss: 389.6513\n",
      "Epoch: 010/513 Train Loss: 329.6606\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 011/513 | Batch 000/059 | Loss: 643.0399\n",
      "Epoch: 011/513 Train Loss: 326.8694\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 012/513 | Batch 000/059 | Loss: 448.0968\n",
      "Epoch: 012/513 Train Loss: 325.2425\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 013/513 | Batch 000/059 | Loss: 332.1288\n",
      "Epoch: 013/513 Train Loss: 323.6847\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 014/513 | Batch 000/059 | Loss: 377.3568\n",
      "Epoch: 014/513 Train Loss: 325.4522\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 015/513 | Batch 000/059 | Loss: 247.9302\n",
      "Epoch: 015/513 Train Loss: 320.8959\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 016/513 | Batch 000/059 | Loss: 545.8027\n",
      "Epoch: 016/513 Train Loss: 322.8526\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 017/513 | Batch 000/059 | Loss: 289.9216\n",
      "Epoch: 017/513 Train Loss: 318.2317\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 018/513 | Batch 000/059 | Loss: 289.4982\n",
      "Epoch: 018/513 Train Loss: 317.2891\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 019/513 | Batch 000/059 | Loss: 356.4602\n",
      "Epoch: 019/513 Train Loss: 316.6308\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 020/513 | Batch 000/059 | Loss: 462.6888\n",
      "Epoch: 020/513 Train Loss: 316.0284\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 021/513 | Batch 000/059 | Loss: 316.6064\n",
      "Epoch: 021/513 Train Loss: 315.1617\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 022/513 | Batch 000/059 | Loss: 324.0258\n",
      "Epoch: 022/513 Train Loss: 313.5783\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 023/513 | Batch 000/059 | Loss: 378.8570\n",
      "Epoch: 023/513 Train Loss: 311.6686\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 024/513 | Batch 000/059 | Loss: 346.3395\n",
      "Epoch: 024/513 Train Loss: 309.3289\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 025/513 | Batch 000/059 | Loss: 285.4239\n",
      "Epoch: 025/513 Train Loss: 307.6174\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 026/513 | Batch 000/059 | Loss: 305.6551\n",
      "Epoch: 026/513 Train Loss: 306.2895\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 027/513 | Batch 000/059 | Loss: 162.6733\n",
      "Epoch: 027/513 Train Loss: 305.2529\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 028/513 | Batch 000/059 | Loss: 461.6667\n",
      "Epoch: 028/513 Train Loss: 303.5921\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 029/513 | Batch 000/059 | Loss: 288.5693\n",
      "Epoch: 029/513 Train Loss: 302.1703\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 030/513 | Batch 000/059 | Loss: 543.3010\n",
      "Epoch: 030/513 Train Loss: 300.7849\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 031/513 | Batch 000/059 | Loss: 286.8067\n",
      "Epoch: 031/513 Train Loss: 299.3755\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 032/513 | Batch 000/059 | Loss: 177.1622\n",
      "Epoch: 032/513 Train Loss: 298.6177\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 033/513 | Batch 000/059 | Loss: 478.1535\n",
      "Epoch: 033/513 Train Loss: 297.2834\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 034/513 | Batch 000/059 | Loss: 141.2521\n",
      "Epoch: 034/513 Train Loss: 306.1321\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 035/513 | Batch 000/059 | Loss: 308.6077\n",
      "Epoch: 035/513 Train Loss: 297.6947\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 036/513 | Batch 000/059 | Loss: 216.0728\n",
      "Epoch: 036/513 Train Loss: 292.7986\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 037/513 | Batch 000/059 | Loss: 250.7081\n",
      "Epoch: 037/513 Train Loss: 290.0835\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 038/513 | Batch 000/059 | Loss: 152.8297\n",
      "Epoch: 038/513 Train Loss: 289.8298\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 039/513 | Batch 000/059 | Loss: 302.3452\n",
      "Epoch: 039/513 Train Loss: 286.8548\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 040/513 | Batch 000/059 | Loss: 353.5099\n",
      "Epoch: 040/513 Train Loss: 288.4062\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 041/513 | Batch 000/059 | Loss: 179.3078\n",
      "Epoch: 041/513 Train Loss: 284.5754\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 042/513 | Batch 000/059 | Loss: 212.0332\n",
      "Epoch: 042/513 Train Loss: 285.3088\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 043/513 | Batch 000/059 | Loss: 278.7072\n",
      "Epoch: 043/513 Train Loss: 283.3387\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 044/513 | Batch 000/059 | Loss: 328.8641\n",
      "Epoch: 044/513 Train Loss: 281.0599\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 045/513 | Batch 000/059 | Loss: 158.3673\n",
      "Epoch: 045/513 Train Loss: 277.2407\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 046/513 | Batch 000/059 | Loss: 285.2742\n",
      "Epoch: 046/513 Train Loss: 276.9933\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 047/513 | Batch 000/059 | Loss: 454.6296\n",
      "Epoch: 047/513 Train Loss: 274.0502\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 048/513 | Batch 000/059 | Loss: 237.4422\n",
      "Epoch: 048/513 Train Loss: 273.9793\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 049/513 | Batch 000/059 | Loss: 586.5269\n",
      "Epoch: 049/513 Train Loss: 273.9467\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 050/513 | Batch 000/059 | Loss: 679.7770\n",
      "Epoch: 050/513 Train Loss: 271.1111\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 051/513 | Batch 000/059 | Loss: 425.3984\n",
      "Epoch: 051/513 Train Loss: 272.4549\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 052/513 | Batch 000/059 | Loss: 233.6562\n",
      "Epoch: 052/513 Train Loss: 267.3217\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 053/513 | Batch 000/059 | Loss: 356.1878\n",
      "Epoch: 053/513 Train Loss: 265.2854\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 054/513 | Batch 000/059 | Loss: 385.7278\n",
      "Epoch: 054/513 Train Loss: 273.4098\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 055/513 | Batch 000/059 | Loss: 177.5118\n",
      "Epoch: 055/513 Train Loss: 263.5564\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 056/513 | Batch 000/059 | Loss: 205.6036\n",
      "Epoch: 056/513 Train Loss: 264.4157\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 057/513 | Batch 000/059 | Loss: 260.2001\n",
      "Epoch: 057/513 Train Loss: 258.9129\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 058/513 | Batch 000/059 | Loss: 421.5347\n",
      "Epoch: 058/513 Train Loss: 257.9368\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 059/513 | Batch 000/059 | Loss: 226.3846\n",
      "Epoch: 059/513 Train Loss: 260.2494\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 060/513 | Batch 000/059 | Loss: 339.6297\n",
      "Epoch: 060/513 Train Loss: 254.7272\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 061/513 | Batch 000/059 | Loss: 288.1573\n",
      "Epoch: 061/513 Train Loss: 253.3225\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 062/513 | Batch 000/059 | Loss: 230.2967\n",
      "Epoch: 062/513 Train Loss: 253.7435\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 063/513 | Batch 000/059 | Loss: 168.5846\n",
      "Epoch: 063/513 Train Loss: 251.0228\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 064/513 | Batch 000/059 | Loss: 256.3709\n",
      "Epoch: 064/513 Train Loss: 250.9779\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 065/513 | Batch 000/059 | Loss: 497.1326\n",
      "Epoch: 065/513 Train Loss: 250.8846\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 066/513 | Batch 000/059 | Loss: 240.2885\n",
      "Epoch: 066/513 Train Loss: 250.6908\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 067/513 | Batch 000/059 | Loss: 196.1264\n",
      "Epoch: 067/513 Train Loss: 247.1673\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 068/513 | Batch 000/059 | Loss: 210.3144\n",
      "Epoch: 068/513 Train Loss: 246.0262\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 069/513 | Batch 000/059 | Loss: 302.6089\n",
      "Epoch: 069/513 Train Loss: 244.2013\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 070/513 | Batch 000/059 | Loss: 285.6602\n",
      "Epoch: 070/513 Train Loss: 243.1887\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 071/513 | Batch 000/059 | Loss: 144.6092\n",
      "Epoch: 071/513 Train Loss: 242.8234\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 072/513 | Batch 000/059 | Loss: 327.6215\n",
      "Epoch: 072/513 Train Loss: 244.4673\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 073/513 | Batch 000/059 | Loss: 402.9798\n",
      "Epoch: 073/513 Train Loss: 241.9597\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 074/513 | Batch 000/059 | Loss: 298.6511\n",
      "Epoch: 074/513 Train Loss: 241.7076\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 075/513 | Batch 000/059 | Loss: 160.6004\n",
      "Epoch: 075/513 Train Loss: 239.9982\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 076/513 | Batch 000/059 | Loss: 443.5187\n",
      "Epoch: 076/513 Train Loss: 239.5795\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 077/513 | Batch 000/059 | Loss: 187.9213\n",
      "Epoch: 077/513 Train Loss: 238.2095\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 078/513 | Batch 000/059 | Loss: 281.9677\n",
      "Epoch: 078/513 Train Loss: 236.7071\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 079/513 | Batch 000/059 | Loss: 186.9802\n",
      "Epoch: 079/513 Train Loss: 238.7479\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 080/513 | Batch 000/059 | Loss: 196.1750\n",
      "Epoch: 080/513 Train Loss: 235.7308\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 081/513 | Batch 000/059 | Loss: 243.0683\n",
      "Epoch: 081/513 Train Loss: 236.4305\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 082/513 | Batch 000/059 | Loss: 347.0236\n",
      "Epoch: 082/513 Train Loss: 234.1607\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 083/513 | Batch 000/059 | Loss: 418.9426\n",
      "Epoch: 083/513 Train Loss: 241.5092\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 084/513 | Batch 000/059 | Loss: 201.1212\n",
      "Epoch: 084/513 Train Loss: 233.7112\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 085/513 | Batch 000/059 | Loss: 147.1891\n",
      "Epoch: 085/513 Train Loss: 232.7989\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 086/513 | Batch 000/059 | Loss: 311.9201\n",
      "Epoch: 086/513 Train Loss: 232.1966\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 087/513 | Batch 000/059 | Loss: 421.5191\n",
      "Epoch: 087/513 Train Loss: 231.7799\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 088/513 | Batch 000/059 | Loss: 254.2346\n",
      "Epoch: 088/513 Train Loss: 231.6681\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 089/513 | Batch 000/059 | Loss: 192.0553\n",
      "Epoch: 089/513 Train Loss: 234.5164\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 090/513 | Batch 000/059 | Loss: 136.1163\n",
      "Epoch: 090/513 Train Loss: 230.5080\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 091/513 | Batch 000/059 | Loss: 244.5220\n",
      "Epoch: 091/513 Train Loss: 236.3674\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 092/513 | Batch 000/059 | Loss: 268.7646\n",
      "Epoch: 092/513 Train Loss: 229.9665\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 093/513 | Batch 000/059 | Loss: 243.9240\n",
      "Epoch: 093/513 Train Loss: 241.2207\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 094/513 | Batch 000/059 | Loss: 410.7069\n",
      "Epoch: 094/513 Train Loss: 229.2811\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 095/513 | Batch 000/059 | Loss: 227.5981\n",
      "Epoch: 095/513 Train Loss: 232.7723\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 096/513 | Batch 000/059 | Loss: 216.1123\n",
      "Epoch: 096/513 Train Loss: 229.6538\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 097/513 | Batch 000/059 | Loss: 290.7635\n",
      "Epoch: 097/513 Train Loss: 228.9057\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 098/513 | Batch 000/059 | Loss: 133.6400\n",
      "Epoch: 098/513 Train Loss: 227.3360\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 099/513 | Batch 000/059 | Loss: 149.2650\n",
      "Epoch: 099/513 Train Loss: 227.5755\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 100/513 | Batch 000/059 | Loss: 159.8821\n",
      "Epoch: 100/513 Train Loss: 227.4048\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 101/513 | Batch 000/059 | Loss: 267.7511\n",
      "Epoch: 101/513 Train Loss: 234.4329\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 102/513 | Batch 000/059 | Loss: 193.9721\n",
      "Epoch: 102/513 Train Loss: 236.2588\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 103/513 | Batch 000/059 | Loss: 150.4756\n",
      "Epoch: 103/513 Train Loss: 228.2676\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 104/513 | Batch 000/059 | Loss: 191.5813\n",
      "Epoch: 104/513 Train Loss: 227.3687\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 105/513 | Batch 000/059 | Loss: 395.3798\n",
      "Epoch: 105/513 Train Loss: 235.1524\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 106/513 | Batch 000/059 | Loss: 327.9234\n",
      "Epoch: 106/513 Train Loss: 225.4711\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 107/513 | Batch 000/059 | Loss: 337.7634\n",
      "Epoch: 107/513 Train Loss: 224.7626\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 108/513 | Batch 000/059 | Loss: 220.6777\n",
      "Epoch: 108/513 Train Loss: 225.2885\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 109/513 | Batch 000/059 | Loss: 179.8622\n",
      "Epoch: 109/513 Train Loss: 224.8353\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 110/513 | Batch 000/059 | Loss: 221.8948\n",
      "Epoch: 110/513 Train Loss: 224.1263\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 111/513 | Batch 000/059 | Loss: 152.0213\n",
      "Epoch: 111/513 Train Loss: 225.1957\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 112/513 | Batch 000/059 | Loss: 213.5397\n",
      "Epoch: 112/513 Train Loss: 233.1435\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 113/513 | Batch 000/059 | Loss: 217.1769\n",
      "Epoch: 113/513 Train Loss: 230.3827\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 114/513 | Batch 000/059 | Loss: 176.9084\n",
      "Epoch: 114/513 Train Loss: 223.2829\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 115/513 | Batch 000/059 | Loss: 276.6904\n",
      "Epoch: 115/513 Train Loss: 222.9255\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 116/513 | Batch 000/059 | Loss: 334.4109\n",
      "Epoch: 116/513 Train Loss: 227.7613\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 117/513 | Batch 000/059 | Loss: 181.6250\n",
      "Epoch: 117/513 Train Loss: 225.5531\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 118/513 | Batch 000/059 | Loss: 261.3878\n",
      "Epoch: 118/513 Train Loss: 224.8303\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 119/513 | Batch 000/059 | Loss: 248.6500\n",
      "Epoch: 119/513 Train Loss: 222.7782\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 120/513 | Batch 000/059 | Loss: 310.5624\n",
      "Epoch: 120/513 Train Loss: 222.3121\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 121/513 | Batch 000/059 | Loss: 195.4113\n",
      "Epoch: 121/513 Train Loss: 226.9966\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 122/513 | Batch 000/059 | Loss: 200.3815\n",
      "Epoch: 122/513 Train Loss: 222.4220\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 123/513 | Batch 000/059 | Loss: 190.1542\n",
      "Epoch: 123/513 Train Loss: 224.1280\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 124/513 | Batch 000/059 | Loss: 167.6893\n",
      "Epoch: 124/513 Train Loss: 227.7111\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 125/513 | Batch 000/059 | Loss: 257.8810\n",
      "Epoch: 125/513 Train Loss: 231.8780\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 126/513 | Batch 000/059 | Loss: 234.6720\n",
      "Epoch: 126/513 Train Loss: 220.8502\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 127/513 | Batch 000/059 | Loss: 272.1519\n",
      "Epoch: 127/513 Train Loss: 221.3147\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 128/513 | Batch 000/059 | Loss: 254.1108\n",
      "Epoch: 128/513 Train Loss: 226.1780\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 129/513 | Batch 000/059 | Loss: 254.1908\n",
      "Epoch: 129/513 Train Loss: 221.3355\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 130/513 | Batch 000/059 | Loss: 194.2060\n",
      "Epoch: 130/513 Train Loss: 221.2611\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 131/513 | Batch 000/059 | Loss: 136.4028\n",
      "Epoch: 131/513 Train Loss: 225.6618\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 132/513 | Batch 000/059 | Loss: 177.5096\n",
      "Epoch: 132/513 Train Loss: 220.0686\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 133/513 | Batch 000/059 | Loss: 279.6216\n",
      "Epoch: 133/513 Train Loss: 229.0483\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 134/513 | Batch 000/059 | Loss: 228.5300\n",
      "Epoch: 134/513 Train Loss: 220.6445\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 135/513 | Batch 000/059 | Loss: 216.8970\n",
      "Epoch: 135/513 Train Loss: 219.6668\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 136/513 | Batch 000/059 | Loss: 163.3197\n",
      "Epoch: 136/513 Train Loss: 224.4107\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 137/513 | Batch 000/059 | Loss: 324.6570\n",
      "Epoch: 137/513 Train Loss: 219.4150\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 138/513 | Batch 000/059 | Loss: 206.3654\n",
      "Epoch: 138/513 Train Loss: 223.5519\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 139/513 | Batch 000/059 | Loss: 139.2029\n",
      "Epoch: 139/513 Train Loss: 219.1181\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 140/513 | Batch 000/059 | Loss: 190.1624\n",
      "Epoch: 140/513 Train Loss: 219.3926\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 141/513 | Batch 000/059 | Loss: 108.9204\n",
      "Epoch: 141/513 Train Loss: 228.4051\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 142/513 | Batch 000/059 | Loss: 174.9885\n",
      "Epoch: 142/513 Train Loss: 218.8890\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 143/513 | Batch 000/059 | Loss: 273.4338\n",
      "Epoch: 143/513 Train Loss: 219.7632\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 144/513 | Batch 000/059 | Loss: 217.7082\n",
      "Epoch: 144/513 Train Loss: 219.3180\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 145/513 | Batch 000/059 | Loss: 176.2672\n",
      "Epoch: 145/513 Train Loss: 224.9693\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 146/513 | Batch 000/059 | Loss: 252.7067\n",
      "Epoch: 146/513 Train Loss: 221.6772\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 147/513 | Batch 000/059 | Loss: 302.1219\n",
      "Epoch: 147/513 Train Loss: 219.2972\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 148/513 | Batch 000/059 | Loss: 215.2115\n",
      "Epoch: 148/513 Train Loss: 218.4033\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 149/513 | Batch 000/059 | Loss: 203.6425\n",
      "Epoch: 149/513 Train Loss: 226.6503\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 150/513 | Batch 000/059 | Loss: 309.4340\n",
      "Epoch: 150/513 Train Loss: 218.8755\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 151/513 | Batch 000/059 | Loss: 230.8344\n",
      "Epoch: 151/513 Train Loss: 218.7999\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 152/513 | Batch 000/059 | Loss: 298.0124\n",
      "Epoch: 152/513 Train Loss: 218.4906\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 153/513 | Batch 000/059 | Loss: 161.3185\n",
      "Epoch: 153/513 Train Loss: 224.2998\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 154/513 | Batch 000/059 | Loss: 175.0307\n",
      "Epoch: 154/513 Train Loss: 217.5945\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 155/513 | Batch 000/059 | Loss: 303.8196\n",
      "Epoch: 155/513 Train Loss: 220.9040\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 156/513 | Batch 000/059 | Loss: 148.3236\n",
      "Epoch: 156/513 Train Loss: 219.6057\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 157/513 | Batch 000/059 | Loss: 137.1173\n",
      "Epoch: 157/513 Train Loss: 217.8949\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 158/513 | Batch 000/059 | Loss: 200.7493\n",
      "Epoch: 158/513 Train Loss: 221.2260\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 159/513 | Batch 000/059 | Loss: 147.5848\n",
      "Epoch: 159/513 Train Loss: 218.7043\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 160/513 | Batch 000/059 | Loss: 283.3838\n",
      "Epoch: 160/513 Train Loss: 221.0557\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 161/513 | Batch 000/059 | Loss: 163.1496\n",
      "Epoch: 161/513 Train Loss: 219.2571\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 162/513 | Batch 000/059 | Loss: 248.5372\n",
      "Epoch: 162/513 Train Loss: 217.6767\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 163/513 | Batch 000/059 | Loss: 135.4256\n",
      "Epoch: 163/513 Train Loss: 217.5757\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 164/513 | Batch 000/059 | Loss: 256.0009\n",
      "Epoch: 164/513 Train Loss: 216.9460\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 165/513 | Batch 000/059 | Loss: 109.4266\n",
      "Epoch: 165/513 Train Loss: 216.9062\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 166/513 | Batch 000/059 | Loss: 240.7578\n",
      "Epoch: 166/513 Train Loss: 217.3172\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 167/513 | Batch 000/059 | Loss: 171.1824\n",
      "Epoch: 167/513 Train Loss: 216.8639\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 168/513 | Batch 000/059 | Loss: 213.8201\n",
      "Epoch: 168/513 Train Loss: 218.3774\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 169/513 | Batch 000/059 | Loss: 219.3415\n",
      "Epoch: 169/513 Train Loss: 224.0548\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 170/513 | Batch 000/059 | Loss: 226.9627\n",
      "Epoch: 170/513 Train Loss: 220.5366\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 171/513 | Batch 000/059 | Loss: 85.5170\n",
      "Epoch: 171/513 Train Loss: 220.2837\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 172/513 | Batch 000/059 | Loss: 205.5332\n",
      "Epoch: 172/513 Train Loss: 217.8033\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 173/513 | Batch 000/059 | Loss: 234.4649\n",
      "Epoch: 173/513 Train Loss: 221.6615\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 174/513 | Batch 000/059 | Loss: 259.9803\n",
      "Epoch: 174/513 Train Loss: 216.2905\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 175/513 | Batch 000/059 | Loss: 115.8756\n",
      "Epoch: 175/513 Train Loss: 216.4669\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 176/513 | Batch 000/059 | Loss: 200.1499\n",
      "Epoch: 176/513 Train Loss: 216.5695\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 177/513 | Batch 000/059 | Loss: 230.4651\n",
      "Epoch: 177/513 Train Loss: 220.0167\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 178/513 | Batch 000/059 | Loss: 324.3473\n",
      "Epoch: 178/513 Train Loss: 218.4440\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 179/513 | Batch 000/059 | Loss: 334.7366\n",
      "Epoch: 179/513 Train Loss: 216.3471\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 180/513 | Batch 000/059 | Loss: 155.3849\n",
      "Epoch: 180/513 Train Loss: 216.7464\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 181/513 | Batch 000/059 | Loss: 221.2249\n",
      "Epoch: 181/513 Train Loss: 217.3218\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 182/513 | Batch 000/059 | Loss: 259.8223\n",
      "Epoch: 182/513 Train Loss: 216.9943\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 183/513 | Batch 000/059 | Loss: 182.7008\n",
      "Epoch: 183/513 Train Loss: 219.3243\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 184/513 | Batch 000/059 | Loss: 217.3464\n",
      "Epoch: 184/513 Train Loss: 220.0408\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 185/513 | Batch 000/059 | Loss: 154.6742\n",
      "Epoch: 185/513 Train Loss: 216.2632\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 186/513 | Batch 000/059 | Loss: 275.2415\n",
      "Epoch: 186/513 Train Loss: 217.3496\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 187/513 | Batch 000/059 | Loss: 128.6323\n",
      "Epoch: 187/513 Train Loss: 216.1471\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 188/513 | Batch 000/059 | Loss: 237.3167\n",
      "Epoch: 188/513 Train Loss: 219.6783\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 189/513 | Batch 000/059 | Loss: 206.8361\n",
      "Epoch: 189/513 Train Loss: 217.4481\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 190/513 | Batch 000/059 | Loss: 370.1607\n",
      "Epoch: 190/513 Train Loss: 216.4216\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 191/513 | Batch 000/059 | Loss: 154.9023\n",
      "Epoch: 191/513 Train Loss: 215.4727\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 192/513 | Batch 000/059 | Loss: 313.0683\n",
      "Epoch: 192/513 Train Loss: 217.1342\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 193/513 | Batch 000/059 | Loss: 251.8080\n",
      "Epoch: 193/513 Train Loss: 215.3354\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 194/513 | Batch 000/059 | Loss: 116.4916\n",
      "Epoch: 194/513 Train Loss: 220.2594\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 195/513 | Batch 000/059 | Loss: 220.2366\n",
      "Epoch: 195/513 Train Loss: 221.2856\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 196/513 | Batch 000/059 | Loss: 155.5122\n",
      "Epoch: 196/513 Train Loss: 215.6736\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 197/513 | Batch 000/059 | Loss: 254.3846\n",
      "Epoch: 197/513 Train Loss: 214.9922\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 198/513 | Batch 000/059 | Loss: 258.2647\n",
      "Epoch: 198/513 Train Loss: 217.4949\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 199/513 | Batch 000/059 | Loss: 284.1778\n",
      "Epoch: 199/513 Train Loss: 221.3943\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 200/513 | Batch 000/059 | Loss: 288.6316\n",
      "Epoch: 200/513 Train Loss: 216.4393\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 201/513 | Batch 000/059 | Loss: 258.1660\n",
      "Epoch: 201/513 Train Loss: 219.4700\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 202/513 | Batch 000/059 | Loss: 189.8373\n",
      "Epoch: 202/513 Train Loss: 220.7899\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 203/513 | Batch 000/059 | Loss: 301.8354\n",
      "Epoch: 203/513 Train Loss: 216.4221\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 204/513 | Batch 000/059 | Loss: 160.0926\n",
      "Epoch: 204/513 Train Loss: 214.9930\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 205/513 | Batch 000/059 | Loss: 226.6575\n",
      "Epoch: 205/513 Train Loss: 215.1335\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 206/513 | Batch 000/059 | Loss: 155.1589\n",
      "Epoch: 206/513 Train Loss: 215.0473\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 207/513 | Batch 000/059 | Loss: 329.0146\n",
      "Epoch: 207/513 Train Loss: 215.1652\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 208/513 | Batch 000/059 | Loss: 165.1964\n",
      "Epoch: 208/513 Train Loss: 215.2063\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 209/513 | Batch 000/059 | Loss: 154.0485\n",
      "Epoch: 209/513 Train Loss: 219.4210\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 210/513 | Batch 000/059 | Loss: 169.6279\n",
      "Epoch: 210/513 Train Loss: 219.2877\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 211/513 | Batch 000/059 | Loss: 137.8450\n",
      "Epoch: 211/513 Train Loss: 215.1379\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 212/513 | Batch 000/059 | Loss: 157.5244\n",
      "Epoch: 212/513 Train Loss: 215.5362\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 213/513 | Batch 000/059 | Loss: 188.8480\n",
      "Epoch: 213/513 Train Loss: 218.4027\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 214/513 | Batch 000/059 | Loss: 199.2099\n",
      "Epoch: 214/513 Train Loss: 214.4311\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 215/513 | Batch 000/059 | Loss: 141.1877\n",
      "Epoch: 215/513 Train Loss: 214.9017\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 216/513 | Batch 000/059 | Loss: 173.7860\n",
      "Epoch: 216/513 Train Loss: 214.8289\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 217/513 | Batch 000/059 | Loss: 314.0547\n",
      "Epoch: 217/513 Train Loss: 215.0402\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 218/513 | Batch 000/059 | Loss: 314.4867\n",
      "Epoch: 218/513 Train Loss: 215.5150\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 219/513 | Batch 000/059 | Loss: 235.1700\n",
      "Epoch: 219/513 Train Loss: 216.2724\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 220/513 | Batch 000/059 | Loss: 198.6101\n",
      "Epoch: 220/513 Train Loss: 215.4589\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 221/513 | Batch 000/059 | Loss: 352.7122\n",
      "Epoch: 221/513 Train Loss: 216.4694\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 222/513 | Batch 000/059 | Loss: 172.6898\n",
      "Epoch: 222/513 Train Loss: 214.4009\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 223/513 | Batch 000/059 | Loss: 195.5540\n",
      "Epoch: 223/513 Train Loss: 222.0059\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 224/513 | Batch 000/059 | Loss: 256.2354\n",
      "Epoch: 224/513 Train Loss: 218.7779\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 225/513 | Batch 000/059 | Loss: 301.5132\n",
      "Epoch: 225/513 Train Loss: 214.3340\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 226/513 | Batch 000/059 | Loss: 272.3460\n",
      "Epoch: 226/513 Train Loss: 215.5573\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 227/513 | Batch 000/059 | Loss: 150.1782\n",
      "Epoch: 227/513 Train Loss: 214.9035\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 228/513 | Batch 000/059 | Loss: 165.0107\n",
      "Epoch: 228/513 Train Loss: 215.2370\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 229/513 | Batch 000/059 | Loss: 166.1407\n",
      "Epoch: 229/513 Train Loss: 214.0495\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 230/513 | Batch 000/059 | Loss: 277.9752\n",
      "Epoch: 230/513 Train Loss: 214.3300\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 231/513 | Batch 000/059 | Loss: 297.1154\n",
      "Epoch: 231/513 Train Loss: 214.1651\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 232/513 | Batch 000/059 | Loss: 180.0251\n",
      "Epoch: 232/513 Train Loss: 214.0960\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 233/513 | Batch 000/059 | Loss: 213.8957\n",
      "Epoch: 233/513 Train Loss: 215.0525\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 234/513 | Batch 000/059 | Loss: 194.5701\n",
      "Epoch: 234/513 Train Loss: 215.6092\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 235/513 | Batch 000/059 | Loss: 162.3990\n",
      "Epoch: 235/513 Train Loss: 215.3887\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 236/513 | Batch 000/059 | Loss: 190.5775\n",
      "Epoch: 236/513 Train Loss: 213.9463\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 237/513 | Batch 000/059 | Loss: 162.9352\n",
      "Epoch: 237/513 Train Loss: 214.5230\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 238/513 | Batch 000/059 | Loss: 188.0157\n",
      "Epoch: 238/513 Train Loss: 214.1901\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 239/513 | Batch 000/059 | Loss: 129.6152\n",
      "Epoch: 239/513 Train Loss: 213.7956\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 240/513 | Batch 000/059 | Loss: 244.5912\n",
      "Epoch: 240/513 Train Loss: 218.8054\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 241/513 | Batch 000/059 | Loss: 240.8455\n",
      "Epoch: 241/513 Train Loss: 221.3745\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 242/513 | Batch 000/059 | Loss: 189.9548\n",
      "Epoch: 242/513 Train Loss: 214.9883\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 243/513 | Batch 000/059 | Loss: 246.6600\n",
      "Epoch: 243/513 Train Loss: 216.0711\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 244/513 | Batch 000/059 | Loss: 229.7806\n",
      "Epoch: 244/513 Train Loss: 214.9740\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 245/513 | Batch 000/059 | Loss: 293.6965\n",
      "Epoch: 245/513 Train Loss: 219.9972\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 246/513 | Batch 000/059 | Loss: 227.5443\n",
      "Epoch: 246/513 Train Loss: 214.4733\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 247/513 | Batch 000/059 | Loss: 218.9676\n",
      "Epoch: 247/513 Train Loss: 214.4173\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 248/513 | Batch 000/059 | Loss: 267.9093\n",
      "Epoch: 248/513 Train Loss: 214.1780\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 249/513 | Batch 000/059 | Loss: 214.7749\n",
      "Epoch: 249/513 Train Loss: 214.1512\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 250/513 | Batch 000/059 | Loss: 88.9284\n",
      "Epoch: 250/513 Train Loss: 215.3432\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 251/513 | Batch 000/059 | Loss: 329.7154\n",
      "Epoch: 251/513 Train Loss: 214.0403\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 252/513 | Batch 000/059 | Loss: 209.1488\n",
      "Epoch: 252/513 Train Loss: 215.3124\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 253/513 | Batch 000/059 | Loss: 239.0473\n",
      "Epoch: 253/513 Train Loss: 214.2009\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 254/513 | Batch 000/059 | Loss: 232.8471\n",
      "Epoch: 254/513 Train Loss: 214.4911\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 255/513 | Batch 000/059 | Loss: 359.9908\n",
      "Epoch: 255/513 Train Loss: 214.4305\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 256/513 | Batch 000/059 | Loss: 330.4129\n",
      "Epoch: 256/513 Train Loss: 216.3187\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 257/513 | Batch 000/059 | Loss: 136.5412\n",
      "Epoch: 257/513 Train Loss: 215.6136\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 258/513 | Batch 000/059 | Loss: 186.2058\n",
      "Epoch: 258/513 Train Loss: 214.4532\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 259/513 | Batch 000/059 | Loss: 372.3445\n",
      "Epoch: 259/513 Train Loss: 218.1536\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 260/513 | Batch 000/059 | Loss: 251.2200\n",
      "Epoch: 260/513 Train Loss: 215.4466\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 261/513 | Batch 000/059 | Loss: 266.8493\n",
      "Epoch: 261/513 Train Loss: 213.5744\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 262/513 | Batch 000/059 | Loss: 174.1047\n",
      "Epoch: 262/513 Train Loss: 217.7167\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 263/513 | Batch 000/059 | Loss: 301.3940\n",
      "Epoch: 263/513 Train Loss: 214.7899\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 264/513 | Batch 000/059 | Loss: 173.3819\n",
      "Epoch: 264/513 Train Loss: 213.8747\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 265/513 | Batch 000/059 | Loss: 235.7256\n",
      "Epoch: 265/513 Train Loss: 214.5897\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 266/513 | Batch 000/059 | Loss: 266.4000\n",
      "Epoch: 266/513 Train Loss: 213.9815\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 267/513 | Batch 000/059 | Loss: 370.5426\n",
      "Epoch: 267/513 Train Loss: 213.5180\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 268/513 | Batch 000/059 | Loss: 314.0966\n",
      "Epoch: 268/513 Train Loss: 213.9235\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 269/513 | Batch 000/059 | Loss: 332.3102\n",
      "Epoch: 269/513 Train Loss: 213.8750\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 270/513 | Batch 000/059 | Loss: 157.0029\n",
      "Epoch: 270/513 Train Loss: 224.2251\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 271/513 | Batch 000/059 | Loss: 221.2832\n",
      "Epoch: 271/513 Train Loss: 216.0836\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 272/513 | Batch 000/059 | Loss: 418.4380\n",
      "Epoch: 272/513 Train Loss: 217.5669\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 273/513 | Batch 000/059 | Loss: 251.4455\n",
      "Epoch: 273/513 Train Loss: 215.6265\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 274/513 | Batch 000/059 | Loss: 276.5448\n",
      "Epoch: 274/513 Train Loss: 213.6650\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 275/513 | Batch 000/059 | Loss: 168.9345\n",
      "Epoch: 275/513 Train Loss: 213.9773\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 276/513 | Batch 000/059 | Loss: 165.0314\n",
      "Epoch: 276/513 Train Loss: 214.5642\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 277/513 | Batch 000/059 | Loss: 241.6605\n",
      "Epoch: 277/513 Train Loss: 212.9851\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 278/513 | Batch 000/059 | Loss: 146.4004\n",
      "Epoch: 278/513 Train Loss: 213.1263\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 279/513 | Batch 000/059 | Loss: 175.5976\n",
      "Epoch: 279/513 Train Loss: 213.6941\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 280/513 | Batch 000/059 | Loss: 208.3899\n",
      "Epoch: 280/513 Train Loss: 223.2997\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 281/513 | Batch 000/059 | Loss: 216.6467\n",
      "Epoch: 281/513 Train Loss: 214.5300\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 282/513 | Batch 000/059 | Loss: 261.9627\n",
      "Epoch: 282/513 Train Loss: 218.6018\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 283/513 | Batch 000/059 | Loss: 315.6190\n",
      "Epoch: 283/513 Train Loss: 216.4238\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 284/513 | Batch 000/059 | Loss: 159.4342\n",
      "Epoch: 284/513 Train Loss: 224.9909\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 285/513 | Batch 000/059 | Loss: 254.4474\n",
      "Epoch: 285/513 Train Loss: 218.3822\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 286/513 | Batch 000/059 | Loss: 211.9062\n",
      "Epoch: 286/513 Train Loss: 212.8214\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 287/513 | Batch 000/059 | Loss: 367.8852\n",
      "Epoch: 287/513 Train Loss: 213.4200\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 288/513 | Batch 000/059 | Loss: 219.6700\n",
      "Epoch: 288/513 Train Loss: 218.4621\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 289/513 | Batch 000/059 | Loss: 216.9332\n",
      "Epoch: 289/513 Train Loss: 213.5060\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 290/513 | Batch 000/059 | Loss: 366.1275\n",
      "Epoch: 290/513 Train Loss: 214.6194\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 291/513 | Batch 000/059 | Loss: 281.6381\n",
      "Epoch: 291/513 Train Loss: 213.4107\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 292/513 | Batch 000/059 | Loss: 124.3258\n",
      "Epoch: 292/513 Train Loss: 213.6893\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 293/513 | Batch 000/059 | Loss: 194.0689\n",
      "Epoch: 293/513 Train Loss: 213.3896\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 294/513 | Batch 000/059 | Loss: 250.7370\n",
      "Epoch: 294/513 Train Loss: 212.8186\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 295/513 | Batch 000/059 | Loss: 274.0121\n",
      "Epoch: 295/513 Train Loss: 219.1680\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 296/513 | Batch 000/059 | Loss: 220.4490\n",
      "Epoch: 296/513 Train Loss: 213.2454\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 297/513 | Batch 000/059 | Loss: 170.8614\n",
      "Epoch: 297/513 Train Loss: 214.1660\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 298/513 | Batch 000/059 | Loss: 251.9760\n",
      "Epoch: 298/513 Train Loss: 213.4935\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 299/513 | Batch 000/059 | Loss: 279.2063\n",
      "Epoch: 299/513 Train Loss: 219.3115\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 300/513 | Batch 000/059 | Loss: 210.8618\n",
      "Epoch: 300/513 Train Loss: 212.5332\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 301/513 | Batch 000/059 | Loss: 258.5054\n",
      "Epoch: 301/513 Train Loss: 215.5287\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 302/513 | Batch 000/059 | Loss: 277.3190\n",
      "Epoch: 302/513 Train Loss: 213.1183\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 303/513 | Batch 000/059 | Loss: 309.6599\n",
      "Epoch: 303/513 Train Loss: 212.7633\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 304/513 | Batch 000/059 | Loss: 144.3654\n",
      "Epoch: 304/513 Train Loss: 213.0842\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 305/513 | Batch 000/059 | Loss: 213.8122\n",
      "Epoch: 305/513 Train Loss: 220.3860\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 306/513 | Batch 000/059 | Loss: 173.0515\n",
      "Epoch: 306/513 Train Loss: 213.1721\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 307/513 | Batch 000/059 | Loss: 168.1926\n",
      "Epoch: 307/513 Train Loss: 213.8644\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 308/513 | Batch 000/059 | Loss: 205.8482\n",
      "Epoch: 308/513 Train Loss: 226.3029\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 309/513 | Batch 000/059 | Loss: 140.1069\n",
      "Epoch: 309/513 Train Loss: 212.8069\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 310/513 | Batch 000/059 | Loss: 129.3302\n",
      "Epoch: 310/513 Train Loss: 218.2028\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 311/513 | Batch 000/059 | Loss: 170.5514\n",
      "Epoch: 311/513 Train Loss: 212.7474\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 312/513 | Batch 000/059 | Loss: 253.9107\n",
      "Epoch: 312/513 Train Loss: 213.9552\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 313/513 | Batch 000/059 | Loss: 268.3806\n",
      "Epoch: 313/513 Train Loss: 212.9935\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 314/513 | Batch 000/059 | Loss: 266.9536\n",
      "Epoch: 314/513 Train Loss: 213.0446\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 315/513 | Batch 000/059 | Loss: 136.4309\n",
      "Epoch: 315/513 Train Loss: 214.6076\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 316/513 | Batch 000/059 | Loss: 198.9965\n",
      "Epoch: 316/513 Train Loss: 214.4311\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 317/513 | Batch 000/059 | Loss: 298.2179\n",
      "Epoch: 317/513 Train Loss: 214.7027\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 318/513 | Batch 000/059 | Loss: 142.8038\n",
      "Epoch: 318/513 Train Loss: 212.4199\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 319/513 | Batch 000/059 | Loss: 181.9734\n",
      "Epoch: 319/513 Train Loss: 212.8227\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 320/513 | Batch 000/059 | Loss: 159.4982\n",
      "Epoch: 320/513 Train Loss: 218.6441\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 321/513 | Batch 000/059 | Loss: 269.4438\n",
      "Epoch: 321/513 Train Loss: 228.3437\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 322/513 | Batch 000/059 | Loss: 242.4252\n",
      "Epoch: 322/513 Train Loss: 214.9411\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 323/513 | Batch 000/059 | Loss: 179.8208\n",
      "Epoch: 323/513 Train Loss: 212.6828\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 324/513 | Batch 000/059 | Loss: 287.6920\n",
      "Epoch: 324/513 Train Loss: 219.8335\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 325/513 | Batch 000/059 | Loss: 233.2469\n",
      "Epoch: 325/513 Train Loss: 212.7051\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 326/513 | Batch 000/059 | Loss: 224.6357\n",
      "Epoch: 326/513 Train Loss: 213.7542\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 327/513 | Batch 000/059 | Loss: 243.0281\n",
      "Epoch: 327/513 Train Loss: 212.4734\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 328/513 | Batch 000/059 | Loss: 313.1942\n",
      "Epoch: 328/513 Train Loss: 212.6339\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 329/513 | Batch 000/059 | Loss: 230.0826\n",
      "Epoch: 329/513 Train Loss: 214.6774\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 330/513 | Batch 000/059 | Loss: 156.9614\n",
      "Epoch: 330/513 Train Loss: 212.5948\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 331/513 | Batch 000/059 | Loss: 189.9811\n",
      "Epoch: 331/513 Train Loss: 215.6775\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 332/513 | Batch 000/059 | Loss: 128.4613\n",
      "Epoch: 332/513 Train Loss: 212.3757\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 333/513 | Batch 000/059 | Loss: 329.6245\n",
      "Epoch: 333/513 Train Loss: 212.4599\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 334/513 | Batch 000/059 | Loss: 130.7901\n",
      "Epoch: 334/513 Train Loss: 213.0838\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 335/513 | Batch 000/059 | Loss: 250.0767\n",
      "Epoch: 335/513 Train Loss: 213.8970\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 336/513 | Batch 000/059 | Loss: 314.3632\n",
      "Epoch: 336/513 Train Loss: 212.4430\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 337/513 | Batch 000/059 | Loss: 306.4501\n",
      "Epoch: 337/513 Train Loss: 212.5414\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 338/513 | Batch 000/059 | Loss: 181.2254\n",
      "Epoch: 338/513 Train Loss: 216.4638\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 339/513 | Batch 000/059 | Loss: 297.1612\n",
      "Epoch: 339/513 Train Loss: 217.6583\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 340/513 | Batch 000/059 | Loss: 331.1773\n",
      "Epoch: 340/513 Train Loss: 213.5192\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 341/513 | Batch 000/059 | Loss: 243.3634\n",
      "Epoch: 341/513 Train Loss: 222.7076\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 342/513 | Batch 000/059 | Loss: 218.6310\n",
      "Epoch: 342/513 Train Loss: 212.6134\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 343/513 | Batch 000/059 | Loss: 237.7273\n",
      "Epoch: 343/513 Train Loss: 213.0377\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 344/513 | Batch 000/059 | Loss: 286.4714\n",
      "Epoch: 344/513 Train Loss: 215.0886\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 345/513 | Batch 000/059 | Loss: 303.7418\n",
      "Epoch: 345/513 Train Loss: 214.1916\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 346/513 | Batch 000/059 | Loss: 232.9920\n",
      "Epoch: 346/513 Train Loss: 214.4601\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 347/513 | Batch 000/059 | Loss: 283.0920\n",
      "Epoch: 347/513 Train Loss: 211.8554\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 348/513 | Batch 000/059 | Loss: 376.0998\n",
      "Epoch: 348/513 Train Loss: 212.3811\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 349/513 | Batch 000/059 | Loss: 205.4664\n",
      "Epoch: 349/513 Train Loss: 215.0573\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 350/513 | Batch 000/059 | Loss: 283.0201\n",
      "Epoch: 350/513 Train Loss: 212.0768\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 351/513 | Batch 000/059 | Loss: 220.0170\n",
      "Epoch: 351/513 Train Loss: 212.0953\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 352/513 | Batch 000/059 | Loss: 216.8903\n",
      "Epoch: 352/513 Train Loss: 212.7833\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 353/513 | Batch 000/059 | Loss: 262.4366\n",
      "Epoch: 353/513 Train Loss: 212.3883\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 354/513 | Batch 000/059 | Loss: 241.9773\n",
      "Epoch: 354/513 Train Loss: 215.7638\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 355/513 | Batch 000/059 | Loss: 309.4744\n",
      "Epoch: 355/513 Train Loss: 212.3812\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 356/513 | Batch 000/059 | Loss: 193.0590\n",
      "Epoch: 356/513 Train Loss: 216.9610\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 357/513 | Batch 000/059 | Loss: 197.9418\n",
      "Epoch: 357/513 Train Loss: 213.6708\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 358/513 | Batch 000/059 | Loss: 173.6021\n",
      "Epoch: 358/513 Train Loss: 212.9319\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 359/513 | Batch 000/059 | Loss: 227.5114\n",
      "Epoch: 359/513 Train Loss: 211.7773\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 360/513 | Batch 000/059 | Loss: 249.0394\n",
      "Epoch: 360/513 Train Loss: 211.9136\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 361/513 | Batch 000/059 | Loss: 235.4829\n",
      "Epoch: 361/513 Train Loss: 211.8627\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 362/513 | Batch 000/059 | Loss: 142.4819\n",
      "Epoch: 362/513 Train Loss: 219.8195\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 363/513 | Batch 000/059 | Loss: 206.4224\n",
      "Epoch: 363/513 Train Loss: 213.0584\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 364/513 | Batch 000/059 | Loss: 298.1042\n",
      "Epoch: 364/513 Train Loss: 216.3707\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 365/513 | Batch 000/059 | Loss: 342.8954\n",
      "Epoch: 365/513 Train Loss: 218.3402\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 366/513 | Batch 000/059 | Loss: 316.6130\n",
      "Epoch: 366/513 Train Loss: 212.0390\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 367/513 | Batch 000/059 | Loss: 187.9190\n",
      "Epoch: 367/513 Train Loss: 212.5855\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 368/513 | Batch 000/059 | Loss: 255.5588\n",
      "Epoch: 368/513 Train Loss: 211.6320\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 369/513 | Batch 000/059 | Loss: 183.4283\n",
      "Epoch: 369/513 Train Loss: 213.1939\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 370/513 | Batch 000/059 | Loss: 233.9976\n",
      "Epoch: 370/513 Train Loss: 215.0293\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 371/513 | Batch 000/059 | Loss: 215.9049\n",
      "Epoch: 371/513 Train Loss: 213.8827\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 372/513 | Batch 000/059 | Loss: 222.0980\n",
      "Epoch: 372/513 Train Loss: 212.8101\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 373/513 | Batch 000/059 | Loss: 213.1434\n",
      "Epoch: 373/513 Train Loss: 219.5711\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 374/513 | Batch 000/059 | Loss: 161.6563\n",
      "Epoch: 374/513 Train Loss: 212.6495\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 375/513 | Batch 000/059 | Loss: 157.3121\n",
      "Epoch: 375/513 Train Loss: 211.8653\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 376/513 | Batch 000/059 | Loss: 287.9953\n",
      "Epoch: 376/513 Train Loss: 211.7734\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 377/513 | Batch 000/059 | Loss: 145.9401\n",
      "Epoch: 377/513 Train Loss: 212.7294\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 378/513 | Batch 000/059 | Loss: 232.8339\n",
      "Epoch: 378/513 Train Loss: 215.1445\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 379/513 | Batch 000/059 | Loss: 187.0374\n",
      "Epoch: 379/513 Train Loss: 211.7911\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 380/513 | Batch 000/059 | Loss: 389.2328\n",
      "Epoch: 380/513 Train Loss: 213.6376\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 381/513 | Batch 000/059 | Loss: 252.4339\n",
      "Epoch: 381/513 Train Loss: 215.7251\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 382/513 | Batch 000/059 | Loss: 158.5460\n",
      "Epoch: 382/513 Train Loss: 213.3336\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 383/513 | Batch 000/059 | Loss: 247.2332\n",
      "Epoch: 383/513 Train Loss: 212.4875\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 384/513 | Batch 000/059 | Loss: 161.6538\n",
      "Epoch: 384/513 Train Loss: 211.4410\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 385/513 | Batch 000/059 | Loss: 261.5380\n",
      "Epoch: 385/513 Train Loss: 211.5559\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 386/513 | Batch 000/059 | Loss: 228.8951\n",
      "Epoch: 386/513 Train Loss: 211.7247\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 387/513 | Batch 000/059 | Loss: 239.3141\n",
      "Epoch: 387/513 Train Loss: 215.1520\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 388/513 | Batch 000/059 | Loss: 180.2622\n",
      "Epoch: 388/513 Train Loss: 212.0428\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 389/513 | Batch 000/059 | Loss: 255.6093\n",
      "Epoch: 389/513 Train Loss: 212.7499\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 390/513 | Batch 000/059 | Loss: 240.1801\n",
      "Epoch: 390/513 Train Loss: 213.4843\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 391/513 | Batch 000/059 | Loss: 234.0640\n",
      "Epoch: 391/513 Train Loss: 211.8687\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 392/513 | Batch 000/059 | Loss: 92.3649\n",
      "Epoch: 392/513 Train Loss: 214.4184\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 393/513 | Batch 000/059 | Loss: 254.0603\n",
      "Epoch: 393/513 Train Loss: 215.6325\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 394/513 | Batch 000/059 | Loss: 316.6565\n",
      "Epoch: 394/513 Train Loss: 212.0635\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 395/513 | Batch 000/059 | Loss: 128.6738\n",
      "Epoch: 395/513 Train Loss: 211.2756\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 396/513 | Batch 000/059 | Loss: 229.7847\n",
      "Epoch: 396/513 Train Loss: 211.6256\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 397/513 | Batch 000/059 | Loss: 239.6068\n",
      "Epoch: 397/513 Train Loss: 213.6826\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 398/513 | Batch 000/059 | Loss: 289.8640\n",
      "Epoch: 398/513 Train Loss: 212.0714\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 399/513 | Batch 000/059 | Loss: 238.5092\n",
      "Epoch: 399/513 Train Loss: 217.2239\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 400/513 | Batch 000/059 | Loss: 317.2948\n",
      "Epoch: 400/513 Train Loss: 212.9136\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 401/513 | Batch 000/059 | Loss: 261.0500\n",
      "Epoch: 401/513 Train Loss: 211.7569\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 402/513 | Batch 000/059 | Loss: 225.4273\n",
      "Epoch: 402/513 Train Loss: 211.7415\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 403/513 | Batch 000/059 | Loss: 135.4563\n",
      "Epoch: 403/513 Train Loss: 214.7545\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 404/513 | Batch 000/059 | Loss: 223.1917\n",
      "Epoch: 404/513 Train Loss: 217.5249\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 405/513 | Batch 000/059 | Loss: 165.4010\n",
      "Epoch: 405/513 Train Loss: 211.6922\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 406/513 | Batch 000/059 | Loss: 220.1688\n",
      "Epoch: 406/513 Train Loss: 217.3976\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 407/513 | Batch 000/059 | Loss: 183.3412\n",
      "Epoch: 407/513 Train Loss: 212.1913\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 408/513 | Batch 000/059 | Loss: 155.5496\n",
      "Epoch: 408/513 Train Loss: 214.6474\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 409/513 | Batch 000/059 | Loss: 362.9307\n",
      "Epoch: 409/513 Train Loss: 211.5904\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 410/513 | Batch 000/059 | Loss: 126.0647\n",
      "Epoch: 410/513 Train Loss: 211.8356\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 411/513 | Batch 000/059 | Loss: 194.5634\n",
      "Epoch: 411/513 Train Loss: 211.5506\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 412/513 | Batch 000/059 | Loss: 211.6070\n",
      "Epoch: 412/513 Train Loss: 213.3040\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 413/513 | Batch 000/059 | Loss: 171.6569\n",
      "Epoch: 413/513 Train Loss: 213.3390\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 414/513 | Batch 000/059 | Loss: 195.6320\n",
      "Epoch: 414/513 Train Loss: 214.7977\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 415/513 | Batch 000/059 | Loss: 236.4729\n",
      "Epoch: 415/513 Train Loss: 211.3905\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 416/513 | Batch 000/059 | Loss: 157.6709\n",
      "Epoch: 416/513 Train Loss: 216.8679\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 417/513 | Batch 000/059 | Loss: 197.6364\n",
      "Epoch: 417/513 Train Loss: 212.2765\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 418/513 | Batch 000/059 | Loss: 223.3212\n",
      "Epoch: 418/513 Train Loss: 211.4248\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 419/513 | Batch 000/059 | Loss: 265.5172\n",
      "Epoch: 419/513 Train Loss: 220.4104\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 420/513 | Batch 000/059 | Loss: 255.2295\n",
      "Epoch: 420/513 Train Loss: 215.0380\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 421/513 | Batch 000/059 | Loss: 205.5963\n",
      "Epoch: 421/513 Train Loss: 211.7944\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 422/513 | Batch 000/059 | Loss: 191.3094\n",
      "Epoch: 422/513 Train Loss: 212.3194\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 423/513 | Batch 000/059 | Loss: 181.4481\n",
      "Epoch: 423/513 Train Loss: 227.6672\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 424/513 | Batch 000/059 | Loss: 356.0177\n",
      "Epoch: 424/513 Train Loss: 212.1609\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 425/513 | Batch 000/059 | Loss: 228.2654\n",
      "Epoch: 425/513 Train Loss: 218.8234\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 426/513 | Batch 000/059 | Loss: 166.9418\n",
      "Epoch: 426/513 Train Loss: 211.1281\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 427/513 | Batch 000/059 | Loss: 228.7169\n",
      "Epoch: 427/513 Train Loss: 215.8689\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 428/513 | Batch 000/059 | Loss: 125.2478\n",
      "Epoch: 428/513 Train Loss: 211.5536\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 429/513 | Batch 000/059 | Loss: 204.5690\n",
      "Epoch: 429/513 Train Loss: 211.3178\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 430/513 | Batch 000/059 | Loss: 230.4962\n",
      "Epoch: 430/513 Train Loss: 211.2327\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 431/513 | Batch 000/059 | Loss: 208.5590\n",
      "Epoch: 431/513 Train Loss: 212.3288\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 432/513 | Batch 000/059 | Loss: 120.0880\n",
      "Epoch: 432/513 Train Loss: 211.8470\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 433/513 | Batch 000/059 | Loss: 485.5627\n",
      "Epoch: 433/513 Train Loss: 216.9596\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 434/513 | Batch 000/059 | Loss: 203.1211\n",
      "Epoch: 434/513 Train Loss: 211.8484\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 435/513 | Batch 000/059 | Loss: 178.5166\n",
      "Epoch: 435/513 Train Loss: 218.0838\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 436/513 | Batch 000/059 | Loss: 256.9402\n",
      "Epoch: 436/513 Train Loss: 212.1574\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 437/513 | Batch 000/059 | Loss: 178.8199\n",
      "Epoch: 437/513 Train Loss: 212.2444\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 438/513 | Batch 000/059 | Loss: 270.6441\n",
      "Epoch: 438/513 Train Loss: 211.9193\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 439/513 | Batch 000/059 | Loss: 251.5225\n",
      "Epoch: 439/513 Train Loss: 217.2219\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 440/513 | Batch 000/059 | Loss: 212.2232\n",
      "Epoch: 440/513 Train Loss: 210.9167\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 441/513 | Batch 000/059 | Loss: 187.0083\n",
      "Epoch: 441/513 Train Loss: 211.2634\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 442/513 | Batch 000/059 | Loss: 282.8396\n",
      "Epoch: 442/513 Train Loss: 212.3836\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 443/513 | Batch 000/059 | Loss: 160.6532\n",
      "Epoch: 443/513 Train Loss: 214.5151\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 444/513 | Batch 000/059 | Loss: 248.7584\n",
      "Epoch: 444/513 Train Loss: 211.5180\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 445/513 | Batch 000/059 | Loss: 206.4506\n",
      "Epoch: 445/513 Train Loss: 210.8782\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 446/513 | Batch 000/059 | Loss: 164.7092\n",
      "Epoch: 446/513 Train Loss: 210.9560\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 447/513 | Batch 000/059 | Loss: 226.8533\n",
      "Epoch: 447/513 Train Loss: 217.8657\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 448/513 | Batch 000/059 | Loss: 102.4755\n",
      "Epoch: 448/513 Train Loss: 213.7578\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 449/513 | Batch 000/059 | Loss: 233.1693\n",
      "Epoch: 449/513 Train Loss: 212.6444\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 450/513 | Batch 000/059 | Loss: 174.8610\n",
      "Epoch: 450/513 Train Loss: 212.6528\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 451/513 | Batch 000/059 | Loss: 354.2308\n",
      "Epoch: 451/513 Train Loss: 213.5638\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 452/513 | Batch 000/059 | Loss: 237.3210\n",
      "Epoch: 452/513 Train Loss: 212.7727\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 453/513 | Batch 000/059 | Loss: 226.1561\n",
      "Epoch: 453/513 Train Loss: 211.0724\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 454/513 | Batch 000/059 | Loss: 123.3874\n",
      "Epoch: 454/513 Train Loss: 211.2606\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 455/513 | Batch 000/059 | Loss: 197.3210\n",
      "Epoch: 455/513 Train Loss: 212.5950\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 456/513 | Batch 000/059 | Loss: 106.1851\n",
      "Epoch: 456/513 Train Loss: 211.7793\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 457/513 | Batch 000/059 | Loss: 209.3467\n",
      "Epoch: 457/513 Train Loss: 212.0342\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 458/513 | Batch 000/059 | Loss: 153.4685\n",
      "Epoch: 458/513 Train Loss: 211.4241\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 459/513 | Batch 000/059 | Loss: 328.1696\n",
      "Epoch: 459/513 Train Loss: 215.2846\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 460/513 | Batch 000/059 | Loss: 182.8163\n",
      "Epoch: 460/513 Train Loss: 211.9581\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 461/513 | Batch 000/059 | Loss: 224.9168\n",
      "Epoch: 461/513 Train Loss: 215.3920\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 462/513 | Batch 000/059 | Loss: 145.2922\n",
      "Epoch: 462/513 Train Loss: 212.7959\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 463/513 | Batch 000/059 | Loss: 226.5055\n",
      "Epoch: 463/513 Train Loss: 216.2416\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 464/513 | Batch 000/059 | Loss: 353.9666\n",
      "Epoch: 464/513 Train Loss: 210.7836\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 465/513 | Batch 000/059 | Loss: 243.7726\n",
      "Epoch: 465/513 Train Loss: 215.5154\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 466/513 | Batch 000/059 | Loss: 214.4143\n",
      "Epoch: 466/513 Train Loss: 210.8880\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 467/513 | Batch 000/059 | Loss: 254.1821\n",
      "Epoch: 467/513 Train Loss: 211.0341\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 468/513 | Batch 000/059 | Loss: 255.7605\n",
      "Epoch: 468/513 Train Loss: 215.0111\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 469/513 | Batch 000/059 | Loss: 212.9255\n",
      "Epoch: 469/513 Train Loss: 210.9702\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 470/513 | Batch 000/059 | Loss: 167.7470\n",
      "Epoch: 470/513 Train Loss: 210.6336\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 471/513 | Batch 000/059 | Loss: 152.9488\n",
      "Epoch: 471/513 Train Loss: 211.1085\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 472/513 | Batch 000/059 | Loss: 169.5506\n",
      "Epoch: 472/513 Train Loss: 214.1047\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 473/513 | Batch 000/059 | Loss: 362.2323\n",
      "Epoch: 473/513 Train Loss: 212.5908\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 474/513 | Batch 000/059 | Loss: 289.9759\n",
      "Epoch: 474/513 Train Loss: 215.8116\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 475/513 | Batch 000/059 | Loss: 365.5016\n",
      "Epoch: 475/513 Train Loss: 212.9836\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 476/513 | Batch 000/059 | Loss: 322.3796\n",
      "Epoch: 476/513 Train Loss: 213.1964\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 477/513 | Batch 000/059 | Loss: 166.7208\n",
      "Epoch: 477/513 Train Loss: 212.8305\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 478/513 | Batch 000/059 | Loss: 298.8144\n",
      "Epoch: 478/513 Train Loss: 219.6971\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 479/513 | Batch 000/059 | Loss: 252.9499\n",
      "Epoch: 479/513 Train Loss: 212.7994\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 480/513 | Batch 000/059 | Loss: 142.5176\n",
      "Epoch: 480/513 Train Loss: 211.2959\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 481/513 | Batch 000/059 | Loss: 190.7964\n",
      "Epoch: 481/513 Train Loss: 211.7894\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 482/513 | Batch 000/059 | Loss: 212.3567\n",
      "Epoch: 482/513 Train Loss: 211.0151\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 483/513 | Batch 000/059 | Loss: 148.2393\n",
      "Epoch: 483/513 Train Loss: 210.7801\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 484/513 | Batch 000/059 | Loss: 161.7570\n",
      "Epoch: 484/513 Train Loss: 216.4098\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 485/513 | Batch 000/059 | Loss: 263.4673\n",
      "Epoch: 485/513 Train Loss: 211.6550\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 486/513 | Batch 000/059 | Loss: 165.0361\n",
      "Epoch: 486/513 Train Loss: 211.6317\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 487/513 | Batch 000/059 | Loss: 307.9772\n",
      "Epoch: 487/513 Train Loss: 211.3435\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 488/513 | Batch 000/059 | Loss: 434.6293\n",
      "Epoch: 488/513 Train Loss: 210.6431\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 489/513 | Batch 000/059 | Loss: 249.0178\n",
      "Epoch: 489/513 Train Loss: 210.9329\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 490/513 | Batch 000/059 | Loss: 77.4554\n",
      "Epoch: 490/513 Train Loss: 215.6481\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 491/513 | Batch 000/059 | Loss: 175.7991\n",
      "Epoch: 491/513 Train Loss: 210.9947\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 492/513 | Batch 000/059 | Loss: 163.0743\n",
      "Epoch: 492/513 Train Loss: 212.6709\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 493/513 | Batch 000/059 | Loss: 265.5028\n",
      "Epoch: 493/513 Train Loss: 212.7122\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 494/513 | Batch 000/059 | Loss: 238.7214\n",
      "Epoch: 494/513 Train Loss: 211.3966\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 495/513 | Batch 000/059 | Loss: 180.6863\n",
      "Epoch: 495/513 Train Loss: 212.8933\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 496/513 | Batch 000/059 | Loss: 187.1003\n",
      "Epoch: 496/513 Train Loss: 212.1271\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 497/513 | Batch 000/059 | Loss: 167.2562\n",
      "Epoch: 497/513 Train Loss: 214.0649\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 498/513 | Batch 000/059 | Loss: 296.8395\n",
      "Epoch: 498/513 Train Loss: 211.8686\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 499/513 | Batch 000/059 | Loss: 257.6594\n",
      "Epoch: 499/513 Train Loss: 212.3522\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 500/513 | Batch 000/059 | Loss: 459.3725\n",
      "Epoch: 500/513 Train Loss: 213.3301\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 501/513 | Batch 000/059 | Loss: 171.8410\n",
      "Epoch: 501/513 Train Loss: 210.9734\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 502/513 | Batch 000/059 | Loss: 204.7914\n",
      "Epoch: 502/513 Train Loss: 211.7686\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 503/513 | Batch 000/059 | Loss: 95.5872\n",
      "Epoch: 503/513 Train Loss: 212.0488\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 504/513 | Batch 000/059 | Loss: 213.0912\n",
      "Epoch: 504/513 Train Loss: 219.9411\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 505/513 | Batch 000/059 | Loss: 132.1102\n",
      "Epoch: 505/513 Train Loss: 211.8505\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 506/513 | Batch 000/059 | Loss: 214.3457\n",
      "Epoch: 506/513 Train Loss: 214.5249\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 507/513 | Batch 000/059 | Loss: 391.0702\n",
      "Epoch: 507/513 Train Loss: 210.6812\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 508/513 | Batch 000/059 | Loss: 175.4978\n",
      "Epoch: 508/513 Train Loss: 212.8680\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 509/513 | Batch 000/059 | Loss: 202.3159\n",
      "Epoch: 509/513 Train Loss: 211.4862\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 510/513 | Batch 000/059 | Loss: 215.5397\n",
      "Epoch: 510/513 Train Loss: 210.8554\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 511/513 | Batch 000/059 | Loss: 248.4622\n",
      "Epoch: 511/513 Train Loss: 211.0090\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 512/513 | Batch 000/059 | Loss: 246.9585\n",
      "Epoch: 512/513 Train Loss: 211.8454\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 513/513 | Batch 000/059 | Loss: 162.7429\n",
      "Epoch: 513/513 Train Loss: 212.9617\n",
      "Time elapsed: 1.10 min\n",
      "Total Training Time: 1.10 min\n",
      "Training Loss: 212.96\n",
      "Test Loss: 224.75\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "minibatch_cost = []\n",
    "epoch_cost = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(device)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        outputs = model(features)\n",
    "        \n",
    "        cost = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        minibatch_cost.append(cost)\n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 1000:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "       \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        cost = compute_loss(model, train_loader)\n",
    "        epoch_cost.append(cost)\n",
    "        print('Epoch: %03d/%03d Train Loss: %.4f' % (\n",
    "                epoch+1, num_epochs, cost))\n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "        \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "    print('Training Loss: %.2f' % compute_loss(model, train_loader))\n",
    "    print('Test Loss: %.2f' % compute_loss(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "6-k1FlbuV7h0",
    "outputId": "2d7af77d-5abb-4cce-fbaa-84722540dfac"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the loss curve\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(len(epoch_cost)), epoch_cost)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross entropy loss\")\n",
    "plt.title(\"Loss of training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zI7P2o5cNErL"
   },
   "source": [
    "## **Optimization:** Different optimizers and batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPJd1fysO9oN"
   },
   "source": [
    "## 1. SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNsdULmDnb4F"
   },
   "source": [
    "### Model B: 1 Hidden Layer Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raRmvYZjnb4F"
   },
   "source": [
    "### Create model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PVdqs6L5nb4F",
    "outputId": "9d492524-778f-49e9-a9e2-e072fb874bde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "n_iters = 30000\n",
    "num_epochs = n_iters / (len(train_data) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "print(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "ILAdj2aAnb4J"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # create your dataloader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "id": "2feJbjMWnb4J"
   },
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, output_dim):\n",
    "        super(FNN, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear4 = nn.Linear(hidden_dim1, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.linear4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "id": "aZ-NoloHnb4J"
   },
   "outputs": [],
   "source": [
    "input_dim = tensor_x.shape[1]\n",
    "output_dim = 1\n",
    "hidden_dim1 = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "id": "Zre6aMzqnb4J"
   },
   "outputs": [],
   "source": [
    "model = FNN(input_dim, hidden_dim1, output_dim)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NKdE1jd7nb4J",
    "outputId": "73c70a5e-cb26-449a-e90d-0e7b789c371f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 256]           2,816\n",
      "           Dropout-2               [-1, 1, 256]               0\n",
      "              ReLU-3               [-1, 1, 256]               0\n",
      "            Linear-4                 [-1, 1, 1]             257\n",
      "================================================================\n",
      "Total params: 3,073\n",
      "Trainable params: 3,073\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1,tensor_x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "id": "yYX1i-O1nb4K"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 0.0001\n",
    "LAMBDA = 0.05\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=LAMBDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ar6xatH9nb4K"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hb-qOyNKnb4K",
    "outputId": "f7c4f157-c2cd-45a8-baca-58d70a2a80f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/513 | Batch 000/059 | Loss: 6557.9111\n",
      "Epoch: 001/513 Train Loss: 366.9059\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 002/513 | Batch 000/059 | Loss: 294.8279\n",
      "Epoch: 002/513 Train Loss: 349.7394\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 003/513 | Batch 000/059 | Loss: 357.8585\n",
      "Epoch: 003/513 Train Loss: 345.0372\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 004/513 | Batch 000/059 | Loss: 341.0561\n",
      "Epoch: 004/513 Train Loss: 341.2540\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 005/513 | Batch 000/059 | Loss: 219.9858\n",
      "Epoch: 005/513 Train Loss: 338.8375\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 006/513 | Batch 000/059 | Loss: 317.0258\n",
      "Epoch: 006/513 Train Loss: 334.8321\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 007/513 | Batch 000/059 | Loss: 416.7763\n",
      "Epoch: 007/513 Train Loss: 333.6553\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 008/513 | Batch 000/059 | Loss: 318.3058\n",
      "Epoch: 008/513 Train Loss: 330.8125\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 009/513 | Batch 000/059 | Loss: 354.1913\n",
      "Epoch: 009/513 Train Loss: 329.5482\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 010/513 | Batch 000/059 | Loss: 832.2006\n",
      "Epoch: 010/513 Train Loss: 327.6560\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 011/513 | Batch 000/059 | Loss: 304.8559\n",
      "Epoch: 011/513 Train Loss: 326.1344\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 012/513 | Batch 000/059 | Loss: 381.7395\n",
      "Epoch: 012/513 Train Loss: 325.2131\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 013/513 | Batch 000/059 | Loss: 485.7360\n",
      "Epoch: 013/513 Train Loss: 323.6055\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 014/513 | Batch 000/059 | Loss: 401.2355\n",
      "Epoch: 014/513 Train Loss: 321.9437\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 015/513 | Batch 000/059 | Loss: 471.5095\n",
      "Epoch: 015/513 Train Loss: 325.8102\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 016/513 | Batch 000/059 | Loss: 343.0768\n",
      "Epoch: 016/513 Train Loss: 319.1283\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 017/513 | Batch 000/059 | Loss: 311.2038\n",
      "Epoch: 017/513 Train Loss: 317.9417\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 018/513 | Batch 000/059 | Loss: 257.0804\n",
      "Epoch: 018/513 Train Loss: 316.2724\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 019/513 | Batch 000/059 | Loss: 461.6198\n",
      "Epoch: 019/513 Train Loss: 317.4614\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 020/513 | Batch 000/059 | Loss: 419.0281\n",
      "Epoch: 020/513 Train Loss: 314.1699\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 021/513 | Batch 000/059 | Loss: 393.4994\n",
      "Epoch: 021/513 Train Loss: 312.2882\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 022/513 | Batch 000/059 | Loss: 245.7349\n",
      "Epoch: 022/513 Train Loss: 311.9831\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 023/513 | Batch 000/059 | Loss: 199.7003\n",
      "Epoch: 023/513 Train Loss: 310.3339\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 024/513 | Batch 000/059 | Loss: 423.5585\n",
      "Epoch: 024/513 Train Loss: 310.1799\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 025/513 | Batch 000/059 | Loss: 315.5624\n",
      "Epoch: 025/513 Train Loss: 306.9509\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 026/513 | Batch 000/059 | Loss: 506.6229\n",
      "Epoch: 026/513 Train Loss: 308.0246\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 027/513 | Batch 000/059 | Loss: 203.8600\n",
      "Epoch: 027/513 Train Loss: 304.2798\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 028/513 | Batch 000/059 | Loss: 254.2951\n",
      "Epoch: 028/513 Train Loss: 303.1243\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 029/513 | Batch 000/059 | Loss: 412.1952\n",
      "Epoch: 029/513 Train Loss: 301.9204\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 030/513 | Batch 000/059 | Loss: 215.6069\n",
      "Epoch: 030/513 Train Loss: 300.6602\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 031/513 | Batch 000/059 | Loss: 355.7536\n",
      "Epoch: 031/513 Train Loss: 298.4124\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 032/513 | Batch 000/059 | Loss: 383.5942\n",
      "Epoch: 032/513 Train Loss: 297.0712\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 033/513 | Batch 000/059 | Loss: 216.9265\n",
      "Epoch: 033/513 Train Loss: 295.3065\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 034/513 | Batch 000/059 | Loss: 193.8605\n",
      "Epoch: 034/513 Train Loss: 294.1183\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 035/513 | Batch 000/059 | Loss: 298.2020\n",
      "Epoch: 035/513 Train Loss: 292.5447\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 036/513 | Batch 000/059 | Loss: 189.3860\n",
      "Epoch: 036/513 Train Loss: 291.2011\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 037/513 | Batch 000/059 | Loss: 253.8903\n",
      "Epoch: 037/513 Train Loss: 292.6045\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 038/513 | Batch 000/059 | Loss: 332.1913\n",
      "Epoch: 038/513 Train Loss: 288.3239\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 039/513 | Batch 000/059 | Loss: 241.8079\n",
      "Epoch: 039/513 Train Loss: 286.1078\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 040/513 | Batch 000/059 | Loss: 459.3434\n",
      "Epoch: 040/513 Train Loss: 284.6296\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 041/513 | Batch 000/059 | Loss: 293.3286\n",
      "Epoch: 041/513 Train Loss: 284.0347\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 042/513 | Batch 000/059 | Loss: 428.5128\n",
      "Epoch: 042/513 Train Loss: 282.8900\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 043/513 | Batch 000/059 | Loss: 286.5040\n",
      "Epoch: 043/513 Train Loss: 280.7280\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 044/513 | Batch 000/059 | Loss: 417.3781\n",
      "Epoch: 044/513 Train Loss: 279.5391\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 045/513 | Batch 000/059 | Loss: 414.8316\n",
      "Epoch: 045/513 Train Loss: 278.2341\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 046/513 | Batch 000/059 | Loss: 220.7468\n",
      "Epoch: 046/513 Train Loss: 275.0257\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 047/513 | Batch 000/059 | Loss: 352.5732\n",
      "Epoch: 047/513 Train Loss: 274.6457\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 048/513 | Batch 000/059 | Loss: 445.1181\n",
      "Epoch: 048/513 Train Loss: 272.8834\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 049/513 | Batch 000/059 | Loss: 213.9823\n",
      "Epoch: 049/513 Train Loss: 271.1428\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 050/513 | Batch 000/059 | Loss: 313.3760\n",
      "Epoch: 050/513 Train Loss: 269.0768\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 051/513 | Batch 000/059 | Loss: 185.4419\n",
      "Epoch: 051/513 Train Loss: 268.7922\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 052/513 | Batch 000/059 | Loss: 314.7004\n",
      "Epoch: 052/513 Train Loss: 266.5537\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 053/513 | Batch 000/059 | Loss: 225.9957\n",
      "Epoch: 053/513 Train Loss: 265.6343\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 054/513 | Batch 000/059 | Loss: 274.7402\n",
      "Epoch: 054/513 Train Loss: 262.5269\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 055/513 | Batch 000/059 | Loss: 281.5645\n",
      "Epoch: 055/513 Train Loss: 262.5778\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 056/513 | Batch 000/059 | Loss: 218.5529\n",
      "Epoch: 056/513 Train Loss: 261.0604\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 057/513 | Batch 000/059 | Loss: 556.7665\n",
      "Epoch: 057/513 Train Loss: 258.5140\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 058/513 | Batch 000/059 | Loss: 257.2056\n",
      "Epoch: 058/513 Train Loss: 268.1141\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 059/513 | Batch 000/059 | Loss: 152.5772\n",
      "Epoch: 059/513 Train Loss: 256.2210\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 060/513 | Batch 000/059 | Loss: 301.4892\n",
      "Epoch: 060/513 Train Loss: 254.2634\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 061/513 | Batch 000/059 | Loss: 287.2360\n",
      "Epoch: 061/513 Train Loss: 255.3028\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 062/513 | Batch 000/059 | Loss: 403.8082\n",
      "Epoch: 062/513 Train Loss: 260.2660\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 063/513 | Batch 000/059 | Loss: 234.8430\n",
      "Epoch: 063/513 Train Loss: 251.8975\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 064/513 | Batch 000/059 | Loss: 321.1124\n",
      "Epoch: 064/513 Train Loss: 250.1130\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 065/513 | Batch 000/059 | Loss: 230.9110\n",
      "Epoch: 065/513 Train Loss: 248.5484\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 066/513 | Batch 000/059 | Loss: 139.4381\n",
      "Epoch: 066/513 Train Loss: 254.1949\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 067/513 | Batch 000/059 | Loss: 271.6014\n",
      "Epoch: 067/513 Train Loss: 249.7429\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 068/513 | Batch 000/059 | Loss: 233.5188\n",
      "Epoch: 068/513 Train Loss: 247.9227\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 069/513 | Batch 000/059 | Loss: 203.1330\n",
      "Epoch: 069/513 Train Loss: 244.2703\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 070/513 | Batch 000/059 | Loss: 198.0256\n",
      "Epoch: 070/513 Train Loss: 242.7099\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 071/513 | Batch 000/059 | Loss: 234.8590\n",
      "Epoch: 071/513 Train Loss: 241.6973\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 072/513 | Batch 000/059 | Loss: 258.9986\n",
      "Epoch: 072/513 Train Loss: 241.9170\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 073/513 | Batch 000/059 | Loss: 292.3202\n",
      "Epoch: 073/513 Train Loss: 242.1294\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 074/513 | Batch 000/059 | Loss: 250.1356\n",
      "Epoch: 074/513 Train Loss: 243.1947\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 075/513 | Batch 000/059 | Loss: 329.5183\n",
      "Epoch: 075/513 Train Loss: 238.7571\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 076/513 | Batch 000/059 | Loss: 155.4566\n",
      "Epoch: 076/513 Train Loss: 238.0362\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 077/513 | Batch 000/059 | Loss: 292.9260\n",
      "Epoch: 077/513 Train Loss: 239.4969\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 078/513 | Batch 000/059 | Loss: 191.7483\n",
      "Epoch: 078/513 Train Loss: 239.8986\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 079/513 | Batch 000/059 | Loss: 259.9590\n",
      "Epoch: 079/513 Train Loss: 236.0934\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 080/513 | Batch 000/059 | Loss: 338.1983\n",
      "Epoch: 080/513 Train Loss: 235.4263\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 081/513 | Batch 000/059 | Loss: 198.4103\n",
      "Epoch: 081/513 Train Loss: 235.0224\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 082/513 | Batch 000/059 | Loss: 222.2220\n",
      "Epoch: 082/513 Train Loss: 234.8640\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 083/513 | Batch 000/059 | Loss: 331.9307\n",
      "Epoch: 083/513 Train Loss: 233.6520\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 084/513 | Batch 000/059 | Loss: 149.8635\n",
      "Epoch: 084/513 Train Loss: 232.7515\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 085/513 | Batch 000/059 | Loss: 317.1250\n",
      "Epoch: 085/513 Train Loss: 239.0024\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 086/513 | Batch 000/059 | Loss: 293.5154\n",
      "Epoch: 086/513 Train Loss: 231.7582\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 087/513 | Batch 000/059 | Loss: 236.4611\n",
      "Epoch: 087/513 Train Loss: 236.3833\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 088/513 | Batch 000/059 | Loss: 250.1338\n",
      "Epoch: 088/513 Train Loss: 231.2349\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 089/513 | Batch 000/059 | Loss: 286.8286\n",
      "Epoch: 089/513 Train Loss: 231.4956\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 090/513 | Batch 000/059 | Loss: 191.6174\n",
      "Epoch: 090/513 Train Loss: 230.0197\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 091/513 | Batch 000/059 | Loss: 259.3766\n",
      "Epoch: 091/513 Train Loss: 231.9387\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 092/513 | Batch 000/059 | Loss: 323.3383\n",
      "Epoch: 092/513 Train Loss: 229.3867\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 093/513 | Batch 000/059 | Loss: 220.9344\n",
      "Epoch: 093/513 Train Loss: 228.7562\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 094/513 | Batch 000/059 | Loss: 253.2230\n",
      "Epoch: 094/513 Train Loss: 230.0244\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 095/513 | Batch 000/059 | Loss: 270.0742\n",
      "Epoch: 095/513 Train Loss: 228.7584\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 096/513 | Batch 000/059 | Loss: 223.9474\n",
      "Epoch: 096/513 Train Loss: 227.6524\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 097/513 | Batch 000/059 | Loss: 282.9106\n",
      "Epoch: 097/513 Train Loss: 229.2788\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 098/513 | Batch 000/059 | Loss: 237.7964\n",
      "Epoch: 098/513 Train Loss: 227.0278\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 099/513 | Batch 000/059 | Loss: 194.0768\n",
      "Epoch: 099/513 Train Loss: 235.4379\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 100/513 | Batch 000/059 | Loss: 277.9615\n",
      "Epoch: 100/513 Train Loss: 227.1667\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 101/513 | Batch 000/059 | Loss: 390.3356\n",
      "Epoch: 101/513 Train Loss: 234.2594\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 102/513 | Batch 000/059 | Loss: 277.9653\n",
      "Epoch: 102/513 Train Loss: 226.3619\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 103/513 | Batch 000/059 | Loss: 296.9099\n",
      "Epoch: 103/513 Train Loss: 234.3918\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 104/513 | Batch 000/059 | Loss: 164.5436\n",
      "Epoch: 104/513 Train Loss: 226.5890\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 105/513 | Batch 000/059 | Loss: 152.9955\n",
      "Epoch: 105/513 Train Loss: 225.3399\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 106/513 | Batch 000/059 | Loss: 198.2914\n",
      "Epoch: 106/513 Train Loss: 225.4003\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 107/513 | Batch 000/059 | Loss: 245.2552\n",
      "Epoch: 107/513 Train Loss: 225.9279\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 108/513 | Batch 000/059 | Loss: 296.5359\n",
      "Epoch: 108/513 Train Loss: 227.4907\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 109/513 | Batch 000/059 | Loss: 256.2036\n",
      "Epoch: 109/513 Train Loss: 225.1084\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 110/513 | Batch 000/059 | Loss: 254.5651\n",
      "Epoch: 110/513 Train Loss: 225.2557\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 111/513 | Batch 000/059 | Loss: 182.8098\n",
      "Epoch: 111/513 Train Loss: 225.4239\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 112/513 | Batch 000/059 | Loss: 370.0793\n",
      "Epoch: 112/513 Train Loss: 223.4181\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 113/513 | Batch 000/059 | Loss: 184.7207\n",
      "Epoch: 113/513 Train Loss: 226.9924\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 114/513 | Batch 000/059 | Loss: 231.3790\n",
      "Epoch: 114/513 Train Loss: 225.6435\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 115/513 | Batch 000/059 | Loss: 194.8200\n",
      "Epoch: 115/513 Train Loss: 222.9957\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 116/513 | Batch 000/059 | Loss: 196.6615\n",
      "Epoch: 116/513 Train Loss: 223.1310\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 117/513 | Batch 000/059 | Loss: 171.0669\n",
      "Epoch: 117/513 Train Loss: 222.5236\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 118/513 | Batch 000/059 | Loss: 247.9138\n",
      "Epoch: 118/513 Train Loss: 224.0329\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 119/513 | Batch 000/059 | Loss: 199.0434\n",
      "Epoch: 119/513 Train Loss: 222.6573\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 120/513 | Batch 000/059 | Loss: 263.9795\n",
      "Epoch: 120/513 Train Loss: 227.2078\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 121/513 | Batch 000/059 | Loss: 346.5703\n",
      "Epoch: 121/513 Train Loss: 223.8238\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 122/513 | Batch 000/059 | Loss: 379.3541\n",
      "Epoch: 122/513 Train Loss: 222.0066\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 123/513 | Batch 000/059 | Loss: 402.5180\n",
      "Epoch: 123/513 Train Loss: 228.4548\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 124/513 | Batch 000/059 | Loss: 251.0310\n",
      "Epoch: 124/513 Train Loss: 222.1528\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 125/513 | Batch 000/059 | Loss: 270.0482\n",
      "Epoch: 125/513 Train Loss: 230.6318\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 126/513 | Batch 000/059 | Loss: 298.9142\n",
      "Epoch: 126/513 Train Loss: 231.5106\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 127/513 | Batch 000/059 | Loss: 202.5095\n",
      "Epoch: 127/513 Train Loss: 226.5134\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 128/513 | Batch 000/059 | Loss: 328.8929\n",
      "Epoch: 128/513 Train Loss: 224.5649\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 129/513 | Batch 000/059 | Loss: 238.1982\n",
      "Epoch: 129/513 Train Loss: 220.4886\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 130/513 | Batch 000/059 | Loss: 422.9296\n",
      "Epoch: 130/513 Train Loss: 239.3133\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 131/513 | Batch 000/059 | Loss: 176.0218\n",
      "Epoch: 131/513 Train Loss: 220.9271\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 132/513 | Batch 000/059 | Loss: 303.3693\n",
      "Epoch: 132/513 Train Loss: 220.1024\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 133/513 | Batch 000/059 | Loss: 136.3327\n",
      "Epoch: 133/513 Train Loss: 220.2981\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 134/513 | Batch 000/059 | Loss: 221.5700\n",
      "Epoch: 134/513 Train Loss: 223.4300\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 135/513 | Batch 000/059 | Loss: 243.1321\n",
      "Epoch: 135/513 Train Loss: 219.9051\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 136/513 | Batch 000/059 | Loss: 173.3847\n",
      "Epoch: 136/513 Train Loss: 223.5517\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 137/513 | Batch 000/059 | Loss: 298.7616\n",
      "Epoch: 137/513 Train Loss: 219.8036\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 138/513 | Batch 000/059 | Loss: 137.7829\n",
      "Epoch: 138/513 Train Loss: 219.2709\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 139/513 | Batch 000/059 | Loss: 190.9536\n",
      "Epoch: 139/513 Train Loss: 219.4943\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 140/513 | Batch 000/059 | Loss: 273.1932\n",
      "Epoch: 140/513 Train Loss: 224.3726\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 141/513 | Batch 000/059 | Loss: 242.5075\n",
      "Epoch: 141/513 Train Loss: 220.2672\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 142/513 | Batch 000/059 | Loss: 324.6129\n",
      "Epoch: 142/513 Train Loss: 220.6684\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 143/513 | Batch 000/059 | Loss: 233.8328\n",
      "Epoch: 143/513 Train Loss: 221.5621\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 144/513 | Batch 000/059 | Loss: 208.1112\n",
      "Epoch: 144/513 Train Loss: 218.8691\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 145/513 | Batch 000/059 | Loss: 289.3223\n",
      "Epoch: 145/513 Train Loss: 221.1993\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 146/513 | Batch 000/059 | Loss: 198.4693\n",
      "Epoch: 146/513 Train Loss: 219.7045\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 147/513 | Batch 000/059 | Loss: 244.1085\n",
      "Epoch: 147/513 Train Loss: 220.6639\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 148/513 | Batch 000/059 | Loss: 212.7481\n",
      "Epoch: 148/513 Train Loss: 219.4667\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 149/513 | Batch 000/059 | Loss: 258.3920\n",
      "Epoch: 149/513 Train Loss: 218.1963\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 150/513 | Batch 000/059 | Loss: 191.8509\n",
      "Epoch: 150/513 Train Loss: 218.8711\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 151/513 | Batch 000/059 | Loss: 231.6606\n",
      "Epoch: 151/513 Train Loss: 219.0143\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 152/513 | Batch 000/059 | Loss: 292.3422\n",
      "Epoch: 152/513 Train Loss: 218.4697\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 153/513 | Batch 000/059 | Loss: 161.1149\n",
      "Epoch: 153/513 Train Loss: 219.7637\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 154/513 | Batch 000/059 | Loss: 243.5735\n",
      "Epoch: 154/513 Train Loss: 223.7708\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 155/513 | Batch 000/059 | Loss: 302.9169\n",
      "Epoch: 155/513 Train Loss: 230.5411\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 156/513 | Batch 000/059 | Loss: 268.5354\n",
      "Epoch: 156/513 Train Loss: 227.5931\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 157/513 | Batch 000/059 | Loss: 260.6400\n",
      "Epoch: 157/513 Train Loss: 217.8998\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 158/513 | Batch 000/059 | Loss: 272.5817\n",
      "Epoch: 158/513 Train Loss: 219.7440\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 159/513 | Batch 000/059 | Loss: 129.4775\n",
      "Epoch: 159/513 Train Loss: 217.9974\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 160/513 | Batch 000/059 | Loss: 275.2051\n",
      "Epoch: 160/513 Train Loss: 231.1598\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 161/513 | Batch 000/059 | Loss: 289.3807\n",
      "Epoch: 161/513 Train Loss: 217.9214\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 162/513 | Batch 000/059 | Loss: 180.3472\n",
      "Epoch: 162/513 Train Loss: 218.2658\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 163/513 | Batch 000/059 | Loss: 283.8380\n",
      "Epoch: 163/513 Train Loss: 223.5270\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 164/513 | Batch 000/059 | Loss: 138.6749\n",
      "Epoch: 164/513 Train Loss: 217.6711\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 165/513 | Batch 000/059 | Loss: 270.5033\n",
      "Epoch: 165/513 Train Loss: 219.2414\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 166/513 | Batch 000/059 | Loss: 216.7866\n",
      "Epoch: 166/513 Train Loss: 217.2888\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 167/513 | Batch 000/059 | Loss: 169.0124\n",
      "Epoch: 167/513 Train Loss: 218.1785\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 168/513 | Batch 000/059 | Loss: 279.7487\n",
      "Epoch: 168/513 Train Loss: 216.9872\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 169/513 | Batch 000/059 | Loss: 183.6659\n",
      "Epoch: 169/513 Train Loss: 217.0662\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 170/513 | Batch 000/059 | Loss: 190.4317\n",
      "Epoch: 170/513 Train Loss: 220.5857\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 171/513 | Batch 000/059 | Loss: 257.7411\n",
      "Epoch: 171/513 Train Loss: 217.3997\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 172/513 | Batch 000/059 | Loss: 133.6851\n",
      "Epoch: 172/513 Train Loss: 217.3550\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 173/513 | Batch 000/059 | Loss: 291.2697\n",
      "Epoch: 173/513 Train Loss: 218.9196\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 174/513 | Batch 000/059 | Loss: 172.7210\n",
      "Epoch: 174/513 Train Loss: 219.2870\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 175/513 | Batch 000/059 | Loss: 210.6097\n",
      "Epoch: 175/513 Train Loss: 216.9268\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 176/513 | Batch 000/059 | Loss: 142.9939\n",
      "Epoch: 176/513 Train Loss: 229.9168\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 177/513 | Batch 000/059 | Loss: 330.2316\n",
      "Epoch: 177/513 Train Loss: 219.8543\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 178/513 | Batch 000/059 | Loss: 185.3504\n",
      "Epoch: 178/513 Train Loss: 216.4127\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 179/513 | Batch 000/059 | Loss: 370.9926\n",
      "Epoch: 179/513 Train Loss: 218.7148\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 180/513 | Batch 000/059 | Loss: 275.8214\n",
      "Epoch: 180/513 Train Loss: 216.6983\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 181/513 | Batch 000/059 | Loss: 181.2375\n",
      "Epoch: 181/513 Train Loss: 217.5835\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 182/513 | Batch 000/059 | Loss: 147.8196\n",
      "Epoch: 182/513 Train Loss: 218.4536\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 183/513 | Batch 000/059 | Loss: 200.6263\n",
      "Epoch: 183/513 Train Loss: 221.2197\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 184/513 | Batch 000/059 | Loss: 125.6004\n",
      "Epoch: 184/513 Train Loss: 216.8904\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 185/513 | Batch 000/059 | Loss: 152.1912\n",
      "Epoch: 185/513 Train Loss: 217.6592\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 186/513 | Batch 000/059 | Loss: 237.9120\n",
      "Epoch: 186/513 Train Loss: 217.1027\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 187/513 | Batch 000/059 | Loss: 276.1472\n",
      "Epoch: 187/513 Train Loss: 219.3774\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 188/513 | Batch 000/059 | Loss: 196.9071\n",
      "Epoch: 188/513 Train Loss: 216.0292\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 189/513 | Batch 000/059 | Loss: 158.1742\n",
      "Epoch: 189/513 Train Loss: 222.5431\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 190/513 | Batch 000/059 | Loss: 357.3084\n",
      "Epoch: 190/513 Train Loss: 224.8331\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 191/513 | Batch 000/059 | Loss: 261.0844\n",
      "Epoch: 191/513 Train Loss: 218.3035\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 192/513 | Batch 000/059 | Loss: 235.0696\n",
      "Epoch: 192/513 Train Loss: 215.9392\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 193/513 | Batch 000/059 | Loss: 157.7418\n",
      "Epoch: 193/513 Train Loss: 217.0351\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 194/513 | Batch 000/059 | Loss: 223.4605\n",
      "Epoch: 194/513 Train Loss: 223.3066\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 195/513 | Batch 000/059 | Loss: 89.5786\n",
      "Epoch: 195/513 Train Loss: 218.1099\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 196/513 | Batch 000/059 | Loss: 190.7746\n",
      "Epoch: 196/513 Train Loss: 221.9280\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 197/513 | Batch 000/059 | Loss: 150.6490\n",
      "Epoch: 197/513 Train Loss: 222.8410\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 198/513 | Batch 000/059 | Loss: 155.9101\n",
      "Epoch: 198/513 Train Loss: 217.2182\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 199/513 | Batch 000/059 | Loss: 153.0327\n",
      "Epoch: 199/513 Train Loss: 215.5808\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 200/513 | Batch 000/059 | Loss: 265.3730\n",
      "Epoch: 200/513 Train Loss: 227.8343\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 201/513 | Batch 000/059 | Loss: 241.6768\n",
      "Epoch: 201/513 Train Loss: 215.1567\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 202/513 | Batch 000/059 | Loss: 210.8535\n",
      "Epoch: 202/513 Train Loss: 217.9922\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 203/513 | Batch 000/059 | Loss: 232.4107\n",
      "Epoch: 203/513 Train Loss: 217.6702\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 204/513 | Batch 000/059 | Loss: 125.8929\n",
      "Epoch: 204/513 Train Loss: 218.4361\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 205/513 | Batch 000/059 | Loss: 249.0739\n",
      "Epoch: 205/513 Train Loss: 215.5558\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 206/513 | Batch 000/059 | Loss: 202.6623\n",
      "Epoch: 206/513 Train Loss: 216.7529\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 207/513 | Batch 000/059 | Loss: 207.2567\n",
      "Epoch: 207/513 Train Loss: 216.4532\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 208/513 | Batch 000/059 | Loss: 281.9599\n",
      "Epoch: 208/513 Train Loss: 215.3596\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 209/513 | Batch 000/059 | Loss: 283.8492\n",
      "Epoch: 209/513 Train Loss: 217.0130\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 210/513 | Batch 000/059 | Loss: 372.5714\n",
      "Epoch: 210/513 Train Loss: 219.7465\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 211/513 | Batch 000/059 | Loss: 368.2287\n",
      "Epoch: 211/513 Train Loss: 216.1503\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 212/513 | Batch 000/059 | Loss: 350.9989\n",
      "Epoch: 212/513 Train Loss: 214.9418\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 213/513 | Batch 000/059 | Loss: 482.7217\n",
      "Epoch: 213/513 Train Loss: 214.9067\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 214/513 | Batch 000/059 | Loss: 257.1447\n",
      "Epoch: 214/513 Train Loss: 217.9154\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 215/513 | Batch 000/059 | Loss: 179.9129\n",
      "Epoch: 215/513 Train Loss: 219.9504\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 216/513 | Batch 000/059 | Loss: 281.1473\n",
      "Epoch: 216/513 Train Loss: 216.7132\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 217/513 | Batch 000/059 | Loss: 223.2814\n",
      "Epoch: 217/513 Train Loss: 215.0287\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 218/513 | Batch 000/059 | Loss: 255.1581\n",
      "Epoch: 218/513 Train Loss: 214.6953\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 219/513 | Batch 000/059 | Loss: 227.1808\n",
      "Epoch: 219/513 Train Loss: 217.8123\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 220/513 | Batch 000/059 | Loss: 287.4496\n",
      "Epoch: 220/513 Train Loss: 214.9792\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 221/513 | Batch 000/059 | Loss: 208.5522\n",
      "Epoch: 221/513 Train Loss: 218.6911\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 222/513 | Batch 000/059 | Loss: 232.5986\n",
      "Epoch: 222/513 Train Loss: 217.9318\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 223/513 | Batch 000/059 | Loss: 199.3468\n",
      "Epoch: 223/513 Train Loss: 216.4004\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 224/513 | Batch 000/059 | Loss: 212.2767\n",
      "Epoch: 224/513 Train Loss: 216.7135\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 225/513 | Batch 000/059 | Loss: 117.3583\n",
      "Epoch: 225/513 Train Loss: 214.5058\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 226/513 | Batch 000/059 | Loss: 143.8245\n",
      "Epoch: 226/513 Train Loss: 216.4072\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 227/513 | Batch 000/059 | Loss: 200.1809\n",
      "Epoch: 227/513 Train Loss: 214.3906\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 228/513 | Batch 000/059 | Loss: 226.4734\n",
      "Epoch: 228/513 Train Loss: 215.8618\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 229/513 | Batch 000/059 | Loss: 340.9620\n",
      "Epoch: 229/513 Train Loss: 225.6604\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 230/513 | Batch 000/059 | Loss: 200.3003\n",
      "Epoch: 230/513 Train Loss: 218.9037\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 231/513 | Batch 000/059 | Loss: 211.2382\n",
      "Epoch: 231/513 Train Loss: 215.8933\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 232/513 | Batch 000/059 | Loss: 192.6183\n",
      "Epoch: 232/513 Train Loss: 214.5399\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 233/513 | Batch 000/059 | Loss: 343.6826\n",
      "Epoch: 233/513 Train Loss: 215.3377\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 234/513 | Batch 000/059 | Loss: 327.9515\n",
      "Epoch: 234/513 Train Loss: 217.5835\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 235/513 | Batch 000/059 | Loss: 227.1683\n",
      "Epoch: 235/513 Train Loss: 216.1249\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 236/513 | Batch 000/059 | Loss: 348.8158\n",
      "Epoch: 236/513 Train Loss: 214.7744\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 237/513 | Batch 000/059 | Loss: 208.0094\n",
      "Epoch: 237/513 Train Loss: 214.1049\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 238/513 | Batch 000/059 | Loss: 121.6563\n",
      "Epoch: 238/513 Train Loss: 216.3442\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 239/513 | Batch 000/059 | Loss: 162.5046\n",
      "Epoch: 239/513 Train Loss: 214.3172\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 240/513 | Batch 000/059 | Loss: 181.3662\n",
      "Epoch: 240/513 Train Loss: 218.5282\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 241/513 | Batch 000/059 | Loss: 221.2601\n",
      "Epoch: 241/513 Train Loss: 214.3241\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 242/513 | Batch 000/059 | Loss: 142.2404\n",
      "Epoch: 242/513 Train Loss: 224.8187\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 243/513 | Batch 000/059 | Loss: 418.4415\n",
      "Epoch: 243/513 Train Loss: 215.5222\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 244/513 | Batch 000/059 | Loss: 167.8324\n",
      "Epoch: 244/513 Train Loss: 215.3210\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 245/513 | Batch 000/059 | Loss: 119.9838\n",
      "Epoch: 245/513 Train Loss: 214.0327\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 246/513 | Batch 000/059 | Loss: 242.0837\n",
      "Epoch: 246/513 Train Loss: 230.5479\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 247/513 | Batch 000/059 | Loss: 311.1152\n",
      "Epoch: 247/513 Train Loss: 218.2294\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 248/513 | Batch 000/059 | Loss: 120.5697\n",
      "Epoch: 248/513 Train Loss: 214.4781\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 249/513 | Batch 000/059 | Loss: 143.2028\n",
      "Epoch: 249/513 Train Loss: 220.6816\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 250/513 | Batch 000/059 | Loss: 181.3467\n",
      "Epoch: 250/513 Train Loss: 214.9497\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 251/513 | Batch 000/059 | Loss: 329.1173\n",
      "Epoch: 251/513 Train Loss: 213.9345\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 252/513 | Batch 000/059 | Loss: 305.5634\n",
      "Epoch: 252/513 Train Loss: 220.2987\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 253/513 | Batch 000/059 | Loss: 242.6603\n",
      "Epoch: 253/513 Train Loss: 213.9586\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 254/513 | Batch 000/059 | Loss: 195.1262\n",
      "Epoch: 254/513 Train Loss: 215.4867\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 255/513 | Batch 000/059 | Loss: 162.1417\n",
      "Epoch: 255/513 Train Loss: 215.4575\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 256/513 | Batch 000/059 | Loss: 274.8506\n",
      "Epoch: 256/513 Train Loss: 215.9266\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 257/513 | Batch 000/059 | Loss: 138.1171\n",
      "Epoch: 257/513 Train Loss: 213.8885\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 258/513 | Batch 000/059 | Loss: 231.2026\n",
      "Epoch: 258/513 Train Loss: 213.7529\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 259/513 | Batch 000/059 | Loss: 156.6312\n",
      "Epoch: 259/513 Train Loss: 217.3065\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 260/513 | Batch 000/059 | Loss: 239.7388\n",
      "Epoch: 260/513 Train Loss: 226.2388\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 261/513 | Batch 000/059 | Loss: 399.4496\n",
      "Epoch: 261/513 Train Loss: 214.8498\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 262/513 | Batch 000/059 | Loss: 165.0231\n",
      "Epoch: 262/513 Train Loss: 216.6973\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 263/513 | Batch 000/059 | Loss: 176.5236\n",
      "Epoch: 263/513 Train Loss: 214.4328\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 264/513 | Batch 000/059 | Loss: 144.0142\n",
      "Epoch: 264/513 Train Loss: 213.8907\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 265/513 | Batch 000/059 | Loss: 303.0202\n",
      "Epoch: 265/513 Train Loss: 213.8767\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 266/513 | Batch 000/059 | Loss: 205.6660\n",
      "Epoch: 266/513 Train Loss: 214.4378\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 267/513 | Batch 000/059 | Loss: 219.0333\n",
      "Epoch: 267/513 Train Loss: 213.4441\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 268/513 | Batch 000/059 | Loss: 293.4988\n",
      "Epoch: 268/513 Train Loss: 214.0154\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 269/513 | Batch 000/059 | Loss: 183.4937\n",
      "Epoch: 269/513 Train Loss: 214.5665\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 270/513 | Batch 000/059 | Loss: 282.4572\n",
      "Epoch: 270/513 Train Loss: 214.0598\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 271/513 | Batch 000/059 | Loss: 227.1902\n",
      "Epoch: 271/513 Train Loss: 213.4462\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 272/513 | Batch 000/059 | Loss: 195.4177\n",
      "Epoch: 272/513 Train Loss: 214.7396\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 273/513 | Batch 000/059 | Loss: 392.6031\n",
      "Epoch: 273/513 Train Loss: 213.2628\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 274/513 | Batch 000/059 | Loss: 150.8078\n",
      "Epoch: 274/513 Train Loss: 215.7672\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 275/513 | Batch 000/059 | Loss: 122.3102\n",
      "Epoch: 275/513 Train Loss: 214.2242\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 276/513 | Batch 000/059 | Loss: 368.4136\n",
      "Epoch: 276/513 Train Loss: 213.2498\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 277/513 | Batch 000/059 | Loss: 110.8296\n",
      "Epoch: 277/513 Train Loss: 216.5889\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 278/513 | Batch 000/059 | Loss: 187.5865\n",
      "Epoch: 278/513 Train Loss: 213.2789\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 279/513 | Batch 000/059 | Loss: 198.8890\n",
      "Epoch: 279/513 Train Loss: 214.8276\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 280/513 | Batch 000/059 | Loss: 387.8436\n",
      "Epoch: 280/513 Train Loss: 214.2353\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 281/513 | Batch 000/059 | Loss: 246.9536\n",
      "Epoch: 281/513 Train Loss: 224.8006\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 282/513 | Batch 000/059 | Loss: 247.5257\n",
      "Epoch: 282/513 Train Loss: 213.6321\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 283/513 | Batch 000/059 | Loss: 353.9446\n",
      "Epoch: 283/513 Train Loss: 213.2794\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 284/513 | Batch 000/059 | Loss: 134.4867\n",
      "Epoch: 284/513 Train Loss: 214.6936\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 285/513 | Batch 000/059 | Loss: 191.1125\n",
      "Epoch: 285/513 Train Loss: 213.9479\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 286/513 | Batch 000/059 | Loss: 211.8009\n",
      "Epoch: 286/513 Train Loss: 213.3345\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 287/513 | Batch 000/059 | Loss: 198.8871\n",
      "Epoch: 287/513 Train Loss: 213.9524\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 288/513 | Batch 000/059 | Loss: 171.7607\n",
      "Epoch: 288/513 Train Loss: 212.9299\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 289/513 | Batch 000/059 | Loss: 378.1192\n",
      "Epoch: 289/513 Train Loss: 220.2555\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 290/513 | Batch 000/059 | Loss: 215.8238\n",
      "Epoch: 290/513 Train Loss: 215.5376\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 291/513 | Batch 000/059 | Loss: 201.8979\n",
      "Epoch: 291/513 Train Loss: 212.9800\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 292/513 | Batch 000/059 | Loss: 291.6606\n",
      "Epoch: 292/513 Train Loss: 213.1809\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 293/513 | Batch 000/059 | Loss: 163.0355\n",
      "Epoch: 293/513 Train Loss: 214.8407\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 294/513 | Batch 000/059 | Loss: 207.9326\n",
      "Epoch: 294/513 Train Loss: 213.2410\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 295/513 | Batch 000/059 | Loss: 143.4147\n",
      "Epoch: 295/513 Train Loss: 215.4319\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 296/513 | Batch 000/059 | Loss: 256.3691\n",
      "Epoch: 296/513 Train Loss: 213.1031\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 297/513 | Batch 000/059 | Loss: 148.6685\n",
      "Epoch: 297/513 Train Loss: 212.9388\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 298/513 | Batch 000/059 | Loss: 173.0014\n",
      "Epoch: 298/513 Train Loss: 213.0291\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 299/513 | Batch 000/059 | Loss: 248.1578\n",
      "Epoch: 299/513 Train Loss: 213.1132\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 300/513 | Batch 000/059 | Loss: 195.3300\n",
      "Epoch: 300/513 Train Loss: 212.6351\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 301/513 | Batch 000/059 | Loss: 175.1244\n",
      "Epoch: 301/513 Train Loss: 216.7340\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 302/513 | Batch 000/059 | Loss: 277.6902\n",
      "Epoch: 302/513 Train Loss: 215.9180\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 303/513 | Batch 000/059 | Loss: 308.1062\n",
      "Epoch: 303/513 Train Loss: 212.9977\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 304/513 | Batch 000/059 | Loss: 126.3128\n",
      "Epoch: 304/513 Train Loss: 213.5716\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 305/513 | Batch 000/059 | Loss: 238.7348\n",
      "Epoch: 305/513 Train Loss: 213.2367\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 306/513 | Batch 000/059 | Loss: 198.0146\n",
      "Epoch: 306/513 Train Loss: 214.6429\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 307/513 | Batch 000/059 | Loss: 285.9383\n",
      "Epoch: 307/513 Train Loss: 217.3712\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 308/513 | Batch 000/059 | Loss: 229.6572\n",
      "Epoch: 308/513 Train Loss: 212.9600\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 309/513 | Batch 000/059 | Loss: 316.9041\n",
      "Epoch: 309/513 Train Loss: 214.3439\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 310/513 | Batch 000/059 | Loss: 251.5007\n",
      "Epoch: 310/513 Train Loss: 212.6600\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 311/513 | Batch 000/059 | Loss: 242.2402\n",
      "Epoch: 311/513 Train Loss: 213.7342\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 312/513 | Batch 000/059 | Loss: 164.2892\n",
      "Epoch: 312/513 Train Loss: 215.1335\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 313/513 | Batch 000/059 | Loss: 158.5542\n",
      "Epoch: 313/513 Train Loss: 212.6544\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 314/513 | Batch 000/059 | Loss: 223.9998\n",
      "Epoch: 314/513 Train Loss: 216.9873\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 315/513 | Batch 000/059 | Loss: 225.8344\n",
      "Epoch: 315/513 Train Loss: 213.2802\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 316/513 | Batch 000/059 | Loss: 201.4428\n",
      "Epoch: 316/513 Train Loss: 226.4445\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 317/513 | Batch 000/059 | Loss: 219.5298\n",
      "Epoch: 317/513 Train Loss: 213.7205\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 318/513 | Batch 000/059 | Loss: 282.6209\n",
      "Epoch: 318/513 Train Loss: 213.3489\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 319/513 | Batch 000/059 | Loss: 196.4429\n",
      "Epoch: 319/513 Train Loss: 212.9048\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 320/513 | Batch 000/059 | Loss: 276.2208\n",
      "Epoch: 320/513 Train Loss: 215.5390\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 321/513 | Batch 000/059 | Loss: 205.9144\n",
      "Epoch: 321/513 Train Loss: 213.5949\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 322/513 | Batch 000/059 | Loss: 326.0546\n",
      "Epoch: 322/513 Train Loss: 213.8201\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 323/513 | Batch 000/059 | Loss: 190.8800\n",
      "Epoch: 323/513 Train Loss: 223.2218\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 324/513 | Batch 000/059 | Loss: 306.9964\n",
      "Epoch: 324/513 Train Loss: 212.5831\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 325/513 | Batch 000/059 | Loss: 219.0761\n",
      "Epoch: 325/513 Train Loss: 213.2050\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 326/513 | Batch 000/059 | Loss: 211.9827\n",
      "Epoch: 326/513 Train Loss: 212.4696\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 327/513 | Batch 000/059 | Loss: 242.4667\n",
      "Epoch: 327/513 Train Loss: 213.0322\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 328/513 | Batch 000/059 | Loss: 169.7830\n",
      "Epoch: 328/513 Train Loss: 213.3555\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 329/513 | Batch 000/059 | Loss: 260.9178\n",
      "Epoch: 329/513 Train Loss: 213.8408\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 330/513 | Batch 000/059 | Loss: 308.2451\n",
      "Epoch: 330/513 Train Loss: 214.7556\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 331/513 | Batch 000/059 | Loss: 116.6341\n",
      "Epoch: 331/513 Train Loss: 214.0215\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 332/513 | Batch 000/059 | Loss: 230.7609\n",
      "Epoch: 332/513 Train Loss: 212.5243\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 333/513 | Batch 000/059 | Loss: 180.2389\n",
      "Epoch: 333/513 Train Loss: 212.7913\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 334/513 | Batch 000/059 | Loss: 193.2347\n",
      "Epoch: 334/513 Train Loss: 212.7699\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 335/513 | Batch 000/059 | Loss: 212.9803\n",
      "Epoch: 335/513 Train Loss: 215.0999\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 336/513 | Batch 000/059 | Loss: 209.9832\n",
      "Epoch: 336/513 Train Loss: 216.7961\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 337/513 | Batch 000/059 | Loss: 272.4320\n",
      "Epoch: 337/513 Train Loss: 227.1219\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 338/513 | Batch 000/059 | Loss: 117.7767\n",
      "Epoch: 338/513 Train Loss: 217.5325\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 339/513 | Batch 000/059 | Loss: 213.6917\n",
      "Epoch: 339/513 Train Loss: 214.3503\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 340/513 | Batch 000/059 | Loss: 306.1165\n",
      "Epoch: 340/513 Train Loss: 217.7676\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 341/513 | Batch 000/059 | Loss: 159.7920\n",
      "Epoch: 341/513 Train Loss: 214.7640\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 342/513 | Batch 000/059 | Loss: 206.3297\n",
      "Epoch: 342/513 Train Loss: 213.3148\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 343/513 | Batch 000/059 | Loss: 196.3886\n",
      "Epoch: 343/513 Train Loss: 215.9996\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 344/513 | Batch 000/059 | Loss: 195.8331\n",
      "Epoch: 344/513 Train Loss: 212.2893\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 345/513 | Batch 000/059 | Loss: 172.8844\n",
      "Epoch: 345/513 Train Loss: 215.4727\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 346/513 | Batch 000/059 | Loss: 128.3738\n",
      "Epoch: 346/513 Train Loss: 214.4364\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 347/513 | Batch 000/059 | Loss: 223.2559\n",
      "Epoch: 347/513 Train Loss: 213.9724\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 348/513 | Batch 000/059 | Loss: 326.1283\n",
      "Epoch: 348/513 Train Loss: 229.7568\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 349/513 | Batch 000/059 | Loss: 362.9691\n",
      "Epoch: 349/513 Train Loss: 211.8876\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 350/513 | Batch 000/059 | Loss: 278.5496\n",
      "Epoch: 350/513 Train Loss: 212.2898\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 351/513 | Batch 000/059 | Loss: 216.0786\n",
      "Epoch: 351/513 Train Loss: 215.3274\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 352/513 | Batch 000/059 | Loss: 362.2951\n",
      "Epoch: 352/513 Train Loss: 212.8401\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 353/513 | Batch 000/059 | Loss: 362.0254\n",
      "Epoch: 353/513 Train Loss: 213.3732\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 354/513 | Batch 000/059 | Loss: 188.0989\n",
      "Epoch: 354/513 Train Loss: 212.8149\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 355/513 | Batch 000/059 | Loss: 246.9679\n",
      "Epoch: 355/513 Train Loss: 213.6522\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 356/513 | Batch 000/059 | Loss: 251.3687\n",
      "Epoch: 356/513 Train Loss: 212.4677\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 357/513 | Batch 000/059 | Loss: 88.7845\n",
      "Epoch: 357/513 Train Loss: 218.5951\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 358/513 | Batch 000/059 | Loss: 237.2827\n",
      "Epoch: 358/513 Train Loss: 213.4345\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 359/513 | Batch 000/059 | Loss: 183.2997\n",
      "Epoch: 359/513 Train Loss: 212.3051\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 360/513 | Batch 000/059 | Loss: 213.4311\n",
      "Epoch: 360/513 Train Loss: 213.3354\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 361/513 | Batch 000/059 | Loss: 232.3130\n",
      "Epoch: 361/513 Train Loss: 212.0776\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 362/513 | Batch 000/059 | Loss: 245.0149\n",
      "Epoch: 362/513 Train Loss: 212.2235\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 363/513 | Batch 000/059 | Loss: 105.0146\n",
      "Epoch: 363/513 Train Loss: 220.7373\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 364/513 | Batch 000/059 | Loss: 204.9075\n",
      "Epoch: 364/513 Train Loss: 215.5743\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 365/513 | Batch 000/059 | Loss: 178.6669\n",
      "Epoch: 365/513 Train Loss: 212.7290\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 366/513 | Batch 000/059 | Loss: 160.4099\n",
      "Epoch: 366/513 Train Loss: 211.9341\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 367/513 | Batch 000/059 | Loss: 131.6179\n",
      "Epoch: 367/513 Train Loss: 214.9604\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 368/513 | Batch 000/059 | Loss: 263.2568\n",
      "Epoch: 368/513 Train Loss: 212.5424\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 369/513 | Batch 000/059 | Loss: 146.9248\n",
      "Epoch: 369/513 Train Loss: 214.1919\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 370/513 | Batch 000/059 | Loss: 174.0390\n",
      "Epoch: 370/513 Train Loss: 213.0896\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 371/513 | Batch 000/059 | Loss: 268.6359\n",
      "Epoch: 371/513 Train Loss: 212.9160\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 372/513 | Batch 000/059 | Loss: 124.3401\n",
      "Epoch: 372/513 Train Loss: 214.0304\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 373/513 | Batch 000/059 | Loss: 210.7885\n",
      "Epoch: 373/513 Train Loss: 212.8905\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 374/513 | Batch 000/059 | Loss: 218.7448\n",
      "Epoch: 374/513 Train Loss: 215.7713\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 375/513 | Batch 000/059 | Loss: 318.3271\n",
      "Epoch: 375/513 Train Loss: 213.9030\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 376/513 | Batch 000/059 | Loss: 279.6855\n",
      "Epoch: 376/513 Train Loss: 212.7946\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 377/513 | Batch 000/059 | Loss: 204.2671\n",
      "Epoch: 377/513 Train Loss: 214.3202\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 378/513 | Batch 000/059 | Loss: 216.2450\n",
      "Epoch: 378/513 Train Loss: 216.5990\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 379/513 | Batch 000/059 | Loss: 250.8290\n",
      "Epoch: 379/513 Train Loss: 215.3658\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 380/513 | Batch 000/059 | Loss: 159.2746\n",
      "Epoch: 380/513 Train Loss: 213.0129\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 381/513 | Batch 000/059 | Loss: 276.5569\n",
      "Epoch: 381/513 Train Loss: 213.9126\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 382/513 | Batch 000/059 | Loss: 179.3635\n",
      "Epoch: 382/513 Train Loss: 217.1135\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 383/513 | Batch 000/059 | Loss: 251.7842\n",
      "Epoch: 383/513 Train Loss: 211.7058\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 384/513 | Batch 000/059 | Loss: 149.3166\n",
      "Epoch: 384/513 Train Loss: 214.2486\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 385/513 | Batch 000/059 | Loss: 194.5354\n",
      "Epoch: 385/513 Train Loss: 213.4805\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 386/513 | Batch 000/059 | Loss: 242.6574\n",
      "Epoch: 386/513 Train Loss: 212.4030\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 387/513 | Batch 000/059 | Loss: 312.3068\n",
      "Epoch: 387/513 Train Loss: 213.0201\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 388/513 | Batch 000/059 | Loss: 245.0656\n",
      "Epoch: 388/513 Train Loss: 212.5584\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 389/513 | Batch 000/059 | Loss: 133.1230\n",
      "Epoch: 389/513 Train Loss: 211.6407\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 390/513 | Batch 000/059 | Loss: 144.2490\n",
      "Epoch: 390/513 Train Loss: 214.2736\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 391/513 | Batch 000/059 | Loss: 236.0512\n",
      "Epoch: 391/513 Train Loss: 212.8111\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 392/513 | Batch 000/059 | Loss: 321.4726\n",
      "Epoch: 392/513 Train Loss: 214.5814\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 393/513 | Batch 000/059 | Loss: 371.9664\n",
      "Epoch: 393/513 Train Loss: 211.9741\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 394/513 | Batch 000/059 | Loss: 197.2754\n",
      "Epoch: 394/513 Train Loss: 213.0714\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 395/513 | Batch 000/059 | Loss: 162.6147\n",
      "Epoch: 395/513 Train Loss: 212.4232\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 396/513 | Batch 000/059 | Loss: 231.4134\n",
      "Epoch: 396/513 Train Loss: 211.9031\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 397/513 | Batch 000/059 | Loss: 293.6939\n",
      "Epoch: 397/513 Train Loss: 211.6221\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 398/513 | Batch 000/059 | Loss: 151.8756\n",
      "Epoch: 398/513 Train Loss: 215.9975\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 399/513 | Batch 000/059 | Loss: 173.7312\n",
      "Epoch: 399/513 Train Loss: 212.8877\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 400/513 | Batch 000/059 | Loss: 137.7883\n",
      "Epoch: 400/513 Train Loss: 219.9990\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 401/513 | Batch 000/059 | Loss: 188.6369\n",
      "Epoch: 401/513 Train Loss: 211.7145\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 402/513 | Batch 000/059 | Loss: 294.2512\n",
      "Epoch: 402/513 Train Loss: 213.8185\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 403/513 | Batch 000/059 | Loss: 207.7442\n",
      "Epoch: 403/513 Train Loss: 213.3105\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 404/513 | Batch 000/059 | Loss: 166.7570\n",
      "Epoch: 404/513 Train Loss: 213.1601\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 405/513 | Batch 000/059 | Loss: 367.5929\n",
      "Epoch: 405/513 Train Loss: 212.5155\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 406/513 | Batch 000/059 | Loss: 329.5620\n",
      "Epoch: 406/513 Train Loss: 212.5949\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 407/513 | Batch 000/059 | Loss: 144.4356\n",
      "Epoch: 407/513 Train Loss: 214.4336\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 408/513 | Batch 000/059 | Loss: 394.6266\n",
      "Epoch: 408/513 Train Loss: 220.0216\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 409/513 | Batch 000/059 | Loss: 289.1526\n",
      "Epoch: 409/513 Train Loss: 211.7500\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 410/513 | Batch 000/059 | Loss: 261.6193\n",
      "Epoch: 410/513 Train Loss: 212.2753\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 411/513 | Batch 000/059 | Loss: 182.1306\n",
      "Epoch: 411/513 Train Loss: 216.6146\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 412/513 | Batch 000/059 | Loss: 298.1354\n",
      "Epoch: 412/513 Train Loss: 214.9596\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 413/513 | Batch 000/059 | Loss: 102.9630\n",
      "Epoch: 413/513 Train Loss: 211.6329\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 414/513 | Batch 000/059 | Loss: 178.9598\n",
      "Epoch: 414/513 Train Loss: 211.4841\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 415/513 | Batch 000/059 | Loss: 409.0006\n",
      "Epoch: 415/513 Train Loss: 214.2316\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 416/513 | Batch 000/059 | Loss: 141.9975\n",
      "Epoch: 416/513 Train Loss: 214.4440\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 417/513 | Batch 000/059 | Loss: 190.9402\n",
      "Epoch: 417/513 Train Loss: 214.8682\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 418/513 | Batch 000/059 | Loss: 243.0388\n",
      "Epoch: 418/513 Train Loss: 212.3946\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 419/513 | Batch 000/059 | Loss: 199.6120\n",
      "Epoch: 419/513 Train Loss: 219.6125\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 420/513 | Batch 000/059 | Loss: 112.6482\n",
      "Epoch: 420/513 Train Loss: 215.1841\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 421/513 | Batch 000/059 | Loss: 120.8622\n",
      "Epoch: 421/513 Train Loss: 212.4645\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 422/513 | Batch 000/059 | Loss: 152.4697\n",
      "Epoch: 422/513 Train Loss: 214.3431\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 423/513 | Batch 000/059 | Loss: 100.7772\n",
      "Epoch: 423/513 Train Loss: 211.2891\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 424/513 | Batch 000/059 | Loss: 330.9776\n",
      "Epoch: 424/513 Train Loss: 220.4044\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 425/513 | Batch 000/059 | Loss: 285.0135\n",
      "Epoch: 425/513 Train Loss: 212.1226\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 426/513 | Batch 000/059 | Loss: 326.6685\n",
      "Epoch: 426/513 Train Loss: 211.6041\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 427/513 | Batch 000/059 | Loss: 237.0210\n",
      "Epoch: 427/513 Train Loss: 212.2715\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 428/513 | Batch 000/059 | Loss: 252.4510\n",
      "Epoch: 428/513 Train Loss: 211.7634\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 429/513 | Batch 000/059 | Loss: 161.6409\n",
      "Epoch: 429/513 Train Loss: 227.2839\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 430/513 | Batch 000/059 | Loss: 236.1735\n",
      "Epoch: 430/513 Train Loss: 211.4467\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 431/513 | Batch 000/059 | Loss: 199.4386\n",
      "Epoch: 431/513 Train Loss: 211.1311\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 432/513 | Batch 000/059 | Loss: 318.3035\n",
      "Epoch: 432/513 Train Loss: 213.5111\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 433/513 | Batch 000/059 | Loss: 210.5404\n",
      "Epoch: 433/513 Train Loss: 211.4660\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 434/513 | Batch 000/059 | Loss: 219.8526\n",
      "Epoch: 434/513 Train Loss: 212.0192\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 435/513 | Batch 000/059 | Loss: 192.8190\n",
      "Epoch: 435/513 Train Loss: 211.5554\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 436/513 | Batch 000/059 | Loss: 239.8253\n",
      "Epoch: 436/513 Train Loss: 212.5637\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 437/513 | Batch 000/059 | Loss: 194.9839\n",
      "Epoch: 437/513 Train Loss: 212.3156\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 438/513 | Batch 000/059 | Loss: 186.2068\n",
      "Epoch: 438/513 Train Loss: 211.1587\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 439/513 | Batch 000/059 | Loss: 155.4457\n",
      "Epoch: 439/513 Train Loss: 212.6637\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 440/513 | Batch 000/059 | Loss: 131.8351\n",
      "Epoch: 440/513 Train Loss: 213.2275\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 441/513 | Batch 000/059 | Loss: 149.2087\n",
      "Epoch: 441/513 Train Loss: 211.6308\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 442/513 | Batch 000/059 | Loss: 196.0763\n",
      "Epoch: 442/513 Train Loss: 212.0821\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 443/513 | Batch 000/059 | Loss: 175.3587\n",
      "Epoch: 443/513 Train Loss: 218.6680\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 444/513 | Batch 000/059 | Loss: 223.3569\n",
      "Epoch: 444/513 Train Loss: 211.7804\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 445/513 | Batch 000/059 | Loss: 150.5498\n",
      "Epoch: 445/513 Train Loss: 214.7509\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 446/513 | Batch 000/059 | Loss: 240.2073\n",
      "Epoch: 446/513 Train Loss: 218.5308\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 447/513 | Batch 000/059 | Loss: 320.6416\n",
      "Epoch: 447/513 Train Loss: 212.5434\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 448/513 | Batch 000/059 | Loss: 395.6012\n",
      "Epoch: 448/513 Train Loss: 213.6233\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 449/513 | Batch 000/059 | Loss: 131.6193\n",
      "Epoch: 449/513 Train Loss: 211.2725\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 450/513 | Batch 000/059 | Loss: 456.5587\n",
      "Epoch: 450/513 Train Loss: 224.0938\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 451/513 | Batch 000/059 | Loss: 201.1978\n",
      "Epoch: 451/513 Train Loss: 211.4137\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 452/513 | Batch 000/059 | Loss: 141.4644\n",
      "Epoch: 452/513 Train Loss: 211.7553\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 453/513 | Batch 000/059 | Loss: 240.7126\n",
      "Epoch: 453/513 Train Loss: 211.4668\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 454/513 | Batch 000/059 | Loss: 153.0149\n",
      "Epoch: 454/513 Train Loss: 211.7029\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 455/513 | Batch 000/059 | Loss: 229.4391\n",
      "Epoch: 455/513 Train Loss: 217.0259\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 456/513 | Batch 000/059 | Loss: 312.5364\n",
      "Epoch: 456/513 Train Loss: 217.4615\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 457/513 | Batch 000/059 | Loss: 120.2513\n",
      "Epoch: 457/513 Train Loss: 211.7476\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 458/513 | Batch 000/059 | Loss: 209.7208\n",
      "Epoch: 458/513 Train Loss: 211.2899\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 459/513 | Batch 000/059 | Loss: 208.3938\n",
      "Epoch: 459/513 Train Loss: 210.9086\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 460/513 | Batch 000/059 | Loss: 234.4837\n",
      "Epoch: 460/513 Train Loss: 211.5274\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 461/513 | Batch 000/059 | Loss: 303.1758\n",
      "Epoch: 461/513 Train Loss: 213.7221\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 462/513 | Batch 000/059 | Loss: 167.8774\n",
      "Epoch: 462/513 Train Loss: 211.1429\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 463/513 | Batch 000/059 | Loss: 181.9983\n",
      "Epoch: 463/513 Train Loss: 211.5852\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 464/513 | Batch 000/059 | Loss: 191.4546\n",
      "Epoch: 464/513 Train Loss: 215.1052\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 465/513 | Batch 000/059 | Loss: 226.2537\n",
      "Epoch: 465/513 Train Loss: 211.1637\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 466/513 | Batch 000/059 | Loss: 127.2806\n",
      "Epoch: 466/513 Train Loss: 211.0539\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 467/513 | Batch 000/059 | Loss: 181.1062\n",
      "Epoch: 467/513 Train Loss: 213.6277\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 468/513 | Batch 000/059 | Loss: 208.9975\n",
      "Epoch: 468/513 Train Loss: 212.3808\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 469/513 | Batch 000/059 | Loss: 194.0154\n",
      "Epoch: 469/513 Train Loss: 212.1031\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 470/513 | Batch 000/059 | Loss: 238.8760\n",
      "Epoch: 470/513 Train Loss: 211.4688\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 471/513 | Batch 000/059 | Loss: 308.3950\n",
      "Epoch: 471/513 Train Loss: 212.7249\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 472/513 | Batch 000/059 | Loss: 224.5645\n",
      "Epoch: 472/513 Train Loss: 212.6417\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 473/513 | Batch 000/059 | Loss: 177.0065\n",
      "Epoch: 473/513 Train Loss: 211.9631\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 474/513 | Batch 000/059 | Loss: 322.8875\n",
      "Epoch: 474/513 Train Loss: 211.5677\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 475/513 | Batch 000/059 | Loss: 208.7116\n",
      "Epoch: 475/513 Train Loss: 211.2724\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 476/513 | Batch 000/059 | Loss: 121.8781\n",
      "Epoch: 476/513 Train Loss: 219.4026\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 477/513 | Batch 000/059 | Loss: 226.2057\n",
      "Epoch: 477/513 Train Loss: 210.9713\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 478/513 | Batch 000/059 | Loss: 310.1692\n",
      "Epoch: 478/513 Train Loss: 211.5953\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 479/513 | Batch 000/059 | Loss: 134.1257\n",
      "Epoch: 479/513 Train Loss: 211.8337\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 480/513 | Batch 000/059 | Loss: 160.6626\n",
      "Epoch: 480/513 Train Loss: 212.1768\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 481/513 | Batch 000/059 | Loss: 385.8522\n",
      "Epoch: 481/513 Train Loss: 211.0848\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 482/513 | Batch 000/059 | Loss: 126.9975\n",
      "Epoch: 482/513 Train Loss: 211.5417\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 483/513 | Batch 000/059 | Loss: 206.0399\n",
      "Epoch: 483/513 Train Loss: 213.6923\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 484/513 | Batch 000/059 | Loss: 239.0284\n",
      "Epoch: 484/513 Train Loss: 211.1986\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 485/513 | Batch 000/059 | Loss: 204.2415\n",
      "Epoch: 485/513 Train Loss: 211.3042\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 486/513 | Batch 000/059 | Loss: 272.2458\n",
      "Epoch: 486/513 Train Loss: 211.3871\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 487/513 | Batch 000/059 | Loss: 244.7493\n",
      "Epoch: 487/513 Train Loss: 211.0196\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 488/513 | Batch 000/059 | Loss: 280.2801\n",
      "Epoch: 488/513 Train Loss: 211.1967\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 489/513 | Batch 000/059 | Loss: 212.6365\n",
      "Epoch: 489/513 Train Loss: 212.7174\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 490/513 | Batch 000/059 | Loss: 347.3866\n",
      "Epoch: 490/513 Train Loss: 212.0271\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 491/513 | Batch 000/059 | Loss: 358.2497\n",
      "Epoch: 491/513 Train Loss: 213.6905\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 492/513 | Batch 000/059 | Loss: 286.5276\n",
      "Epoch: 492/513 Train Loss: 218.9557\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 493/513 | Batch 000/059 | Loss: 142.6604\n",
      "Epoch: 493/513 Train Loss: 210.7971\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 494/513 | Batch 000/059 | Loss: 106.2942\n",
      "Epoch: 494/513 Train Loss: 211.8916\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 495/513 | Batch 000/059 | Loss: 218.0862\n",
      "Epoch: 495/513 Train Loss: 212.8102\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 496/513 | Batch 000/059 | Loss: 225.7902\n",
      "Epoch: 496/513 Train Loss: 211.0708\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 497/513 | Batch 000/059 | Loss: 248.2755\n",
      "Epoch: 497/513 Train Loss: 219.7156\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 498/513 | Batch 000/059 | Loss: 201.9339\n",
      "Epoch: 498/513 Train Loss: 218.9726\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 499/513 | Batch 000/059 | Loss: 262.3845\n",
      "Epoch: 499/513 Train Loss: 213.1876\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 500/513 | Batch 000/059 | Loss: 118.8698\n",
      "Epoch: 500/513 Train Loss: 213.8320\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 501/513 | Batch 000/059 | Loss: 334.7231\n",
      "Epoch: 501/513 Train Loss: 213.9126\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 502/513 | Batch 000/059 | Loss: 299.0159\n",
      "Epoch: 502/513 Train Loss: 211.0050\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 503/513 | Batch 000/059 | Loss: 239.5965\n",
      "Epoch: 503/513 Train Loss: 210.7498\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 504/513 | Batch 000/059 | Loss: 275.5396\n",
      "Epoch: 504/513 Train Loss: 211.6656\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 505/513 | Batch 000/059 | Loss: 193.6943\n",
      "Epoch: 505/513 Train Loss: 211.2713\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 506/513 | Batch 000/059 | Loss: 349.3303\n",
      "Epoch: 506/513 Train Loss: 210.6095\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 507/513 | Batch 000/059 | Loss: 212.1097\n",
      "Epoch: 507/513 Train Loss: 214.3321\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 508/513 | Batch 000/059 | Loss: 270.2831\n",
      "Epoch: 508/513 Train Loss: 210.5342\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 509/513 | Batch 000/059 | Loss: 328.3268\n",
      "Epoch: 509/513 Train Loss: 225.6111\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 510/513 | Batch 000/059 | Loss: 278.5871\n",
      "Epoch: 510/513 Train Loss: 214.8754\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 511/513 | Batch 000/059 | Loss: 159.0470\n",
      "Epoch: 511/513 Train Loss: 213.9751\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 512/513 | Batch 000/059 | Loss: 359.1258\n",
      "Epoch: 512/513 Train Loss: 211.7118\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 513/513 | Batch 000/059 | Loss: 158.3360\n",
      "Epoch: 513/513 Train Loss: 211.8012\n",
      "Time elapsed: 1.09 min\n",
      "Total Training Time: 1.09 min\n",
      "Training Loss: 211.80\n",
      "Test Loss: 224.20\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "minibatch_cost = []\n",
    "epoch_cost = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(device)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        outputs = model(features)\n",
    "        \n",
    "        cost = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        minibatch_cost.append(cost)\n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 1000:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "       \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        cost = compute_loss(model, train_loader)\n",
    "        epoch_cost.append(cost)\n",
    "        print('Epoch: %03d/%03d Train Loss: %.4f' % (\n",
    "                epoch+1, num_epochs, cost))\n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "        \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "    print('Training Loss: %.2f' % compute_loss(model, train_loader))\n",
    "    print('Test Loss: %.2f' % compute_loss(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "lcqy3PAYnb4K",
    "outputId": "857d670a-f31e-4c25-9604-bdeec3b8a73e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the loss curve\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(len(epoch_cost)), epoch_cost)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross entropy loss\")\n",
    "plt.title(\"Loss of training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIFxFNUnO9oO"
   },
   "source": [
    "## 2. SGD Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkG1JqxmoRvw"
   },
   "source": [
    "### Model B: 1 Hidden Layer Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hA0WBuO_oRvw"
   },
   "source": [
    "### Create model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vy_ujreMoRvx",
    "outputId": "52d2d97c-0ebd-4ae9-eeb7-c4a14cfa804b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "n_iters = 30000\n",
    "num_epochs = n_iters / (len(train_data) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "print(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "kGf6dj7ToRvy"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # create your dataloader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "id": "4uhYm6FmoRvy"
   },
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, output_dim):\n",
    "        super(FNN, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear4 = nn.Linear(hidden_dim1, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.linear4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "id": "k1VRnq0ZoRvy"
   },
   "outputs": [],
   "source": [
    "input_dim = tensor_x.shape[1]\n",
    "output_dim = 1\n",
    "hidden_dim1 = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "id": "10mYt7yKoRvy"
   },
   "outputs": [],
   "source": [
    "model = FNN(input_dim, hidden_dim1, output_dim)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eJIqqTnmoRvy",
    "outputId": "f8ca492e-ee4a-4716-ba29-619aead34e79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 256]           2,816\n",
      "           Dropout-2               [-1, 1, 256]               0\n",
      "              ReLU-3               [-1, 1, 256]               0\n",
      "            Linear-4                 [-1, 1, 1]             257\n",
      "================================================================\n",
      "Total params: 3,073\n",
      "Trainable params: 3,073\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1,tensor_x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "id": "dAasWbuUoRvy"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 0.0001\n",
    "LAMBDA = 0.05\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=LAMBDA, momentum=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rmp0EE5_oRvz"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rAG43hcnoRvz",
    "outputId": "8405cc5a-11ce-4f42-c859-3d41198e35c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/513 | Batch 000/059 | Loss: 7039.9849\n",
      "Epoch: 001/513 Train Loss: 345.1431\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 002/513 | Batch 000/059 | Loss: 254.8773\n",
      "Epoch: 002/513 Train Loss: 335.0442\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 003/513 | Batch 000/059 | Loss: 358.9052\n",
      "Epoch: 003/513 Train Loss: 329.6915\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 004/513 | Batch 000/059 | Loss: 326.0017\n",
      "Epoch: 004/513 Train Loss: 329.7892\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 005/513 | Batch 000/059 | Loss: 261.2112\n",
      "Epoch: 005/513 Train Loss: 322.7970\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 006/513 | Batch 000/059 | Loss: 430.2898\n",
      "Epoch: 006/513 Train Loss: 316.1667\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 007/513 | Batch 000/059 | Loss: 441.9327\n",
      "Epoch: 007/513 Train Loss: 313.8065\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 008/513 | Batch 000/059 | Loss: 343.7231\n",
      "Epoch: 008/513 Train Loss: 311.2598\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 009/513 | Batch 000/059 | Loss: 628.4368\n",
      "Epoch: 009/513 Train Loss: 305.3087\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 010/513 | Batch 000/059 | Loss: 337.4804\n",
      "Epoch: 010/513 Train Loss: 304.4611\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 011/513 | Batch 000/059 | Loss: 227.6644\n",
      "Epoch: 011/513 Train Loss: 297.1839\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 012/513 | Batch 000/059 | Loss: 152.1992\n",
      "Epoch: 012/513 Train Loss: 295.5914\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 013/513 | Batch 000/059 | Loss: 297.9842\n",
      "Epoch: 013/513 Train Loss: 288.9297\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 014/513 | Batch 000/059 | Loss: 168.4051\n",
      "Epoch: 014/513 Train Loss: 286.9397\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 015/513 | Batch 000/059 | Loss: 215.0193\n",
      "Epoch: 015/513 Train Loss: 281.9415\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 016/513 | Batch 000/059 | Loss: 224.0673\n",
      "Epoch: 016/513 Train Loss: 277.3108\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 017/513 | Batch 000/059 | Loss: 301.4246\n",
      "Epoch: 017/513 Train Loss: 275.4533\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 018/513 | Batch 000/059 | Loss: 327.5295\n",
      "Epoch: 018/513 Train Loss: 272.1999\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 019/513 | Batch 000/059 | Loss: 300.6595\n",
      "Epoch: 019/513 Train Loss: 266.0871\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 020/513 | Batch 000/059 | Loss: 229.8492\n",
      "Epoch: 020/513 Train Loss: 269.3104\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 021/513 | Batch 000/059 | Loss: 240.5340\n",
      "Epoch: 021/513 Train Loss: 272.7382\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 022/513 | Batch 000/059 | Loss: 203.0310\n",
      "Epoch: 022/513 Train Loss: 271.0161\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 023/513 | Batch 000/059 | Loss: 280.5907\n",
      "Epoch: 023/513 Train Loss: 254.6018\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 024/513 | Batch 000/059 | Loss: 193.0745\n",
      "Epoch: 024/513 Train Loss: 249.7879\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 025/513 | Batch 000/059 | Loss: 145.2337\n",
      "Epoch: 025/513 Train Loss: 247.4762\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 026/513 | Batch 000/059 | Loss: 321.2232\n",
      "Epoch: 026/513 Train Loss: 247.3561\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 027/513 | Batch 000/059 | Loss: 284.8184\n",
      "Epoch: 027/513 Train Loss: 245.7587\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 028/513 | Batch 000/059 | Loss: 289.2078\n",
      "Epoch: 028/513 Train Loss: 243.7190\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 029/513 | Batch 000/059 | Loss: 214.4700\n",
      "Epoch: 029/513 Train Loss: 242.0422\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 030/513 | Batch 000/059 | Loss: 248.8022\n",
      "Epoch: 030/513 Train Loss: 245.5174\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 031/513 | Batch 000/059 | Loss: 346.3975\n",
      "Epoch: 031/513 Train Loss: 240.1716\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 032/513 | Batch 000/059 | Loss: 172.4563\n",
      "Epoch: 032/513 Train Loss: 244.7764\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 033/513 | Batch 000/059 | Loss: 212.7281\n",
      "Epoch: 033/513 Train Loss: 233.5478\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 034/513 | Batch 000/059 | Loss: 323.1267\n",
      "Epoch: 034/513 Train Loss: 232.2602\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 035/513 | Batch 000/059 | Loss: 263.4547\n",
      "Epoch: 035/513 Train Loss: 257.2389\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 036/513 | Batch 000/059 | Loss: 334.8628\n",
      "Epoch: 036/513 Train Loss: 232.7211\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 037/513 | Batch 000/059 | Loss: 288.1982\n",
      "Epoch: 037/513 Train Loss: 243.1437\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 038/513 | Batch 000/059 | Loss: 241.4657\n",
      "Epoch: 038/513 Train Loss: 235.3630\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 039/513 | Batch 000/059 | Loss: 233.6199\n",
      "Epoch: 039/513 Train Loss: 236.4588\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 040/513 | Batch 000/059 | Loss: 216.3164\n",
      "Epoch: 040/513 Train Loss: 238.5752\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 041/513 | Batch 000/059 | Loss: 110.5098\n",
      "Epoch: 041/513 Train Loss: 227.4298\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 042/513 | Batch 000/059 | Loss: 207.6656\n",
      "Epoch: 042/513 Train Loss: 227.8053\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 043/513 | Batch 000/059 | Loss: 216.0578\n",
      "Epoch: 043/513 Train Loss: 248.4749\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 044/513 | Batch 000/059 | Loss: 199.8851\n",
      "Epoch: 044/513 Train Loss: 228.7194\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 045/513 | Batch 000/059 | Loss: 221.9311\n",
      "Epoch: 045/513 Train Loss: 225.3578\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 046/513 | Batch 000/059 | Loss: 150.7890\n",
      "Epoch: 046/513 Train Loss: 224.3513\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 047/513 | Batch 000/059 | Loss: 146.0216\n",
      "Epoch: 047/513 Train Loss: 234.3756\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 048/513 | Batch 000/059 | Loss: 170.5220\n",
      "Epoch: 048/513 Train Loss: 228.8393\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 049/513 | Batch 000/059 | Loss: 411.8754\n",
      "Epoch: 049/513 Train Loss: 225.9325\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 050/513 | Batch 000/059 | Loss: 422.7656\n",
      "Epoch: 050/513 Train Loss: 222.5433\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 051/513 | Batch 000/059 | Loss: 154.7952\n",
      "Epoch: 051/513 Train Loss: 222.3878\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 052/513 | Batch 000/059 | Loss: 215.4041\n",
      "Epoch: 052/513 Train Loss: 228.7564\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 053/513 | Batch 000/059 | Loss: 250.2379\n",
      "Epoch: 053/513 Train Loss: 229.7165\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 054/513 | Batch 000/059 | Loss: 160.6637\n",
      "Epoch: 054/513 Train Loss: 221.8715\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 055/513 | Batch 000/059 | Loss: 175.1646\n",
      "Epoch: 055/513 Train Loss: 223.5245\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 056/513 | Batch 000/059 | Loss: 225.2952\n",
      "Epoch: 056/513 Train Loss: 226.5049\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 057/513 | Batch 000/059 | Loss: 251.4764\n",
      "Epoch: 057/513 Train Loss: 221.0520\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 058/513 | Batch 000/059 | Loss: 155.5404\n",
      "Epoch: 058/513 Train Loss: 226.2071\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 059/513 | Batch 000/059 | Loss: 277.6884\n",
      "Epoch: 059/513 Train Loss: 246.6082\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 060/513 | Batch 000/059 | Loss: 277.7392\n",
      "Epoch: 060/513 Train Loss: 233.0274\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 061/513 | Batch 000/059 | Loss: 217.9782\n",
      "Epoch: 061/513 Train Loss: 231.0518\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 062/513 | Batch 000/059 | Loss: 270.3670\n",
      "Epoch: 062/513 Train Loss: 226.3643\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 063/513 | Batch 000/059 | Loss: 87.4019\n",
      "Epoch: 063/513 Train Loss: 228.2137\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 064/513 | Batch 000/059 | Loss: 272.2443\n",
      "Epoch: 064/513 Train Loss: 273.8563\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 065/513 | Batch 000/059 | Loss: 276.9517\n",
      "Epoch: 065/513 Train Loss: 227.4295\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 066/513 | Batch 000/059 | Loss: 186.1879\n",
      "Epoch: 066/513 Train Loss: 228.1630\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 067/513 | Batch 000/059 | Loss: 214.4053\n",
      "Epoch: 067/513 Train Loss: 220.3107\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 068/513 | Batch 000/059 | Loss: 241.4269\n",
      "Epoch: 068/513 Train Loss: 225.6961\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 069/513 | Batch 000/059 | Loss: 184.3606\n",
      "Epoch: 069/513 Train Loss: 218.5516\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 070/513 | Batch 000/059 | Loss: 237.8545\n",
      "Epoch: 070/513 Train Loss: 225.3969\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 071/513 | Batch 000/059 | Loss: 342.8094\n",
      "Epoch: 071/513 Train Loss: 218.8718\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 072/513 | Batch 000/059 | Loss: 150.2412\n",
      "Epoch: 072/513 Train Loss: 224.5772\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 073/513 | Batch 000/059 | Loss: 182.5206\n",
      "Epoch: 073/513 Train Loss: 223.5944\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 074/513 | Batch 000/059 | Loss: 197.0191\n",
      "Epoch: 074/513 Train Loss: 221.7597\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 075/513 | Batch 000/059 | Loss: 181.1686\n",
      "Epoch: 075/513 Train Loss: 227.5668\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 076/513 | Batch 000/059 | Loss: 290.3872\n",
      "Epoch: 076/513 Train Loss: 223.0507\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 077/513 | Batch 000/059 | Loss: 269.7020\n",
      "Epoch: 077/513 Train Loss: 228.0825\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 078/513 | Batch 000/059 | Loss: 179.4158\n",
      "Epoch: 078/513 Train Loss: 235.4891\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 079/513 | Batch 000/059 | Loss: 182.6260\n",
      "Epoch: 079/513 Train Loss: 218.1356\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 080/513 | Batch 000/059 | Loss: 164.4309\n",
      "Epoch: 080/513 Train Loss: 219.9883\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 081/513 | Batch 000/059 | Loss: 226.9003\n",
      "Epoch: 081/513 Train Loss: 224.7554\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 082/513 | Batch 000/059 | Loss: 213.7739\n",
      "Epoch: 082/513 Train Loss: 220.3693\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 083/513 | Batch 000/059 | Loss: 113.3443\n",
      "Epoch: 083/513 Train Loss: 218.3041\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 084/513 | Batch 000/059 | Loss: 297.1028\n",
      "Epoch: 084/513 Train Loss: 231.0354\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 085/513 | Batch 000/059 | Loss: 207.6136\n",
      "Epoch: 085/513 Train Loss: 220.5530\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 086/513 | Batch 000/059 | Loss: 209.6740\n",
      "Epoch: 086/513 Train Loss: 222.7420\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 087/513 | Batch 000/059 | Loss: 268.7759\n",
      "Epoch: 087/513 Train Loss: 218.1262\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 088/513 | Batch 000/059 | Loss: 173.8136\n",
      "Epoch: 088/513 Train Loss: 235.1671\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 089/513 | Batch 000/059 | Loss: 260.5697\n",
      "Epoch: 089/513 Train Loss: 220.1955\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 090/513 | Batch 000/059 | Loss: 358.0167\n",
      "Epoch: 090/513 Train Loss: 231.4093\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 091/513 | Batch 000/059 | Loss: 244.5293\n",
      "Epoch: 091/513 Train Loss: 217.6220\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 092/513 | Batch 000/059 | Loss: 168.6872\n",
      "Epoch: 092/513 Train Loss: 220.9647\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 093/513 | Batch 000/059 | Loss: 177.5842\n",
      "Epoch: 093/513 Train Loss: 217.5899\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 094/513 | Batch 000/059 | Loss: 253.6160\n",
      "Epoch: 094/513 Train Loss: 226.1528\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 095/513 | Batch 000/059 | Loss: 328.1678\n",
      "Epoch: 095/513 Train Loss: 221.3419\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 096/513 | Batch 000/059 | Loss: 306.4998\n",
      "Epoch: 096/513 Train Loss: 246.2210\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 097/513 | Batch 000/059 | Loss: 139.4093\n",
      "Epoch: 097/513 Train Loss: 220.0860\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 098/513 | Batch 000/059 | Loss: 254.1424\n",
      "Epoch: 098/513 Train Loss: 216.0739\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 099/513 | Batch 000/059 | Loss: 200.8297\n",
      "Epoch: 099/513 Train Loss: 217.2438\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 100/513 | Batch 000/059 | Loss: 244.5505\n",
      "Epoch: 100/513 Train Loss: 231.4451\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 101/513 | Batch 000/059 | Loss: 185.5978\n",
      "Epoch: 101/513 Train Loss: 215.9851\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 102/513 | Batch 000/059 | Loss: 312.2134\n",
      "Epoch: 102/513 Train Loss: 218.3239\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 103/513 | Batch 000/059 | Loss: 361.9417\n",
      "Epoch: 103/513 Train Loss: 218.2682\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 104/513 | Batch 000/059 | Loss: 218.1721\n",
      "Epoch: 104/513 Train Loss: 220.5454\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 105/513 | Batch 000/059 | Loss: 237.6857\n",
      "Epoch: 105/513 Train Loss: 224.6371\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 106/513 | Batch 000/059 | Loss: 350.5973\n",
      "Epoch: 106/513 Train Loss: 223.1547\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 107/513 | Batch 000/059 | Loss: 217.4006\n",
      "Epoch: 107/513 Train Loss: 221.0382\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 108/513 | Batch 000/059 | Loss: 167.0269\n",
      "Epoch: 108/513 Train Loss: 218.6646\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 109/513 | Batch 000/059 | Loss: 129.0517\n",
      "Epoch: 109/513 Train Loss: 216.6420\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 110/513 | Batch 000/059 | Loss: 287.4681\n",
      "Epoch: 110/513 Train Loss: 217.2643\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 111/513 | Batch 000/059 | Loss: 146.4778\n",
      "Epoch: 111/513 Train Loss: 225.5372\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 112/513 | Batch 000/059 | Loss: 173.4631\n",
      "Epoch: 112/513 Train Loss: 242.5905\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 113/513 | Batch 000/059 | Loss: 209.9706\n",
      "Epoch: 113/513 Train Loss: 215.9883\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 114/513 | Batch 000/059 | Loss: 186.2331\n",
      "Epoch: 114/513 Train Loss: 214.9943\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 115/513 | Batch 000/059 | Loss: 293.9488\n",
      "Epoch: 115/513 Train Loss: 217.7251\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 116/513 | Batch 000/059 | Loss: 304.3747\n",
      "Epoch: 116/513 Train Loss: 214.8249\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 117/513 | Batch 000/059 | Loss: 216.5145\n",
      "Epoch: 117/513 Train Loss: 214.6059\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 118/513 | Batch 000/059 | Loss: 137.1244\n",
      "Epoch: 118/513 Train Loss: 217.2455\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 119/513 | Batch 000/059 | Loss: 216.4475\n",
      "Epoch: 119/513 Train Loss: 225.6829\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 120/513 | Batch 000/059 | Loss: 383.0106\n",
      "Epoch: 120/513 Train Loss: 218.9171\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 121/513 | Batch 000/059 | Loss: 158.4788\n",
      "Epoch: 121/513 Train Loss: 217.2378\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 122/513 | Batch 000/059 | Loss: 247.3006\n",
      "Epoch: 122/513 Train Loss: 220.5107\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 123/513 | Batch 000/059 | Loss: 258.6384\n",
      "Epoch: 123/513 Train Loss: 215.0607\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 124/513 | Batch 000/059 | Loss: 139.0162\n",
      "Epoch: 124/513 Train Loss: 221.9660\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 125/513 | Batch 000/059 | Loss: 277.0309\n",
      "Epoch: 125/513 Train Loss: 218.8309\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 126/513 | Batch 000/059 | Loss: 176.9418\n",
      "Epoch: 126/513 Train Loss: 216.6587\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 127/513 | Batch 000/059 | Loss: 162.3362\n",
      "Epoch: 127/513 Train Loss: 230.5233\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 128/513 | Batch 000/059 | Loss: 166.7332\n",
      "Epoch: 128/513 Train Loss: 227.0793\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 129/513 | Batch 000/059 | Loss: 199.8493\n",
      "Epoch: 129/513 Train Loss: 219.5004\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 130/513 | Batch 000/059 | Loss: 407.2787\n",
      "Epoch: 130/513 Train Loss: 227.7309\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 131/513 | Batch 000/059 | Loss: 291.7344\n",
      "Epoch: 131/513 Train Loss: 214.4162\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 132/513 | Batch 000/059 | Loss: 180.6514\n",
      "Epoch: 132/513 Train Loss: 216.6980\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 133/513 | Batch 000/059 | Loss: 198.0932\n",
      "Epoch: 133/513 Train Loss: 225.9746\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 134/513 | Batch 000/059 | Loss: 165.2926\n",
      "Epoch: 134/513 Train Loss: 213.5949\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 135/513 | Batch 000/059 | Loss: 268.8338\n",
      "Epoch: 135/513 Train Loss: 225.6311\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 136/513 | Batch 000/059 | Loss: 128.3116\n",
      "Epoch: 136/513 Train Loss: 214.8683\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 137/513 | Batch 000/059 | Loss: 178.2183\n",
      "Epoch: 137/513 Train Loss: 213.9565\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 138/513 | Batch 000/059 | Loss: 230.0797\n",
      "Epoch: 138/513 Train Loss: 220.9786\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 139/513 | Batch 000/059 | Loss: 407.5513\n",
      "Epoch: 139/513 Train Loss: 220.3905\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 140/513 | Batch 000/059 | Loss: 221.1757\n",
      "Epoch: 140/513 Train Loss: 215.4465\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 141/513 | Batch 000/059 | Loss: 301.9449\n",
      "Epoch: 141/513 Train Loss: 214.7384\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 142/513 | Batch 000/059 | Loss: 237.5585\n",
      "Epoch: 142/513 Train Loss: 225.8506\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 143/513 | Batch 000/059 | Loss: 194.2118\n",
      "Epoch: 143/513 Train Loss: 216.9614\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 144/513 | Batch 000/059 | Loss: 254.5872\n",
      "Epoch: 144/513 Train Loss: 223.8024\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 145/513 | Batch 000/059 | Loss: 308.1342\n",
      "Epoch: 145/513 Train Loss: 216.2896\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 146/513 | Batch 000/059 | Loss: 128.1550\n",
      "Epoch: 146/513 Train Loss: 214.1097\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 147/513 | Batch 000/059 | Loss: 248.2581\n",
      "Epoch: 147/513 Train Loss: 214.7425\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 148/513 | Batch 000/059 | Loss: 242.8933\n",
      "Epoch: 148/513 Train Loss: 215.3521\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 149/513 | Batch 000/059 | Loss: 263.6202\n",
      "Epoch: 149/513 Train Loss: 230.5764\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 150/513 | Batch 000/059 | Loss: 341.9241\n",
      "Epoch: 150/513 Train Loss: 225.1426\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 151/513 | Batch 000/059 | Loss: 189.1205\n",
      "Epoch: 151/513 Train Loss: 216.2239\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 152/513 | Batch 000/059 | Loss: 196.0956\n",
      "Epoch: 152/513 Train Loss: 217.9458\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 153/513 | Batch 000/059 | Loss: 154.4318\n",
      "Epoch: 153/513 Train Loss: 217.6459\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 154/513 | Batch 000/059 | Loss: 376.5492\n",
      "Epoch: 154/513 Train Loss: 215.4341\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 155/513 | Batch 000/059 | Loss: 217.4175\n",
      "Epoch: 155/513 Train Loss: 217.4670\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 156/513 | Batch 000/059 | Loss: 255.0554\n",
      "Epoch: 156/513 Train Loss: 219.1938\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 157/513 | Batch 000/059 | Loss: 178.4069\n",
      "Epoch: 157/513 Train Loss: 213.7524\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 158/513 | Batch 000/059 | Loss: 209.4609\n",
      "Epoch: 158/513 Train Loss: 214.6071\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 159/513 | Batch 000/059 | Loss: 207.4743\n",
      "Epoch: 159/513 Train Loss: 213.7864\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 160/513 | Batch 000/059 | Loss: 272.8418\n",
      "Epoch: 160/513 Train Loss: 215.7578\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 161/513 | Batch 000/059 | Loss: 148.0092\n",
      "Epoch: 161/513 Train Loss: 214.8861\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 162/513 | Batch 000/059 | Loss: 170.5814\n",
      "Epoch: 162/513 Train Loss: 214.3350\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 163/513 | Batch 000/059 | Loss: 240.8657\n",
      "Epoch: 163/513 Train Loss: 229.6793\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 164/513 | Batch 000/059 | Loss: 299.3476\n",
      "Epoch: 164/513 Train Loss: 219.3336\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 165/513 | Batch 000/059 | Loss: 173.5242\n",
      "Epoch: 165/513 Train Loss: 212.6894\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 166/513 | Batch 000/059 | Loss: 431.7940\n",
      "Epoch: 166/513 Train Loss: 217.2262\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 167/513 | Batch 000/059 | Loss: 246.1595\n",
      "Epoch: 167/513 Train Loss: 215.4610\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 168/513 | Batch 000/059 | Loss: 263.0906\n",
      "Epoch: 168/513 Train Loss: 213.3674\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 169/513 | Batch 000/059 | Loss: 150.9180\n",
      "Epoch: 169/513 Train Loss: 213.3555\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 170/513 | Batch 000/059 | Loss: 250.9902\n",
      "Epoch: 170/513 Train Loss: 215.6123\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 171/513 | Batch 000/059 | Loss: 127.4207\n",
      "Epoch: 171/513 Train Loss: 227.1352\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 172/513 | Batch 000/059 | Loss: 197.2798\n",
      "Epoch: 172/513 Train Loss: 213.8329\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 173/513 | Batch 000/059 | Loss: 256.5600\n",
      "Epoch: 173/513 Train Loss: 213.8465\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 174/513 | Batch 000/059 | Loss: 235.1759\n",
      "Epoch: 174/513 Train Loss: 213.6053\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 175/513 | Batch 000/059 | Loss: 266.6150\n",
      "Epoch: 175/513 Train Loss: 213.5397\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 176/513 | Batch 000/059 | Loss: 216.8456\n",
      "Epoch: 176/513 Train Loss: 220.4416\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 177/513 | Batch 000/059 | Loss: 202.6544\n",
      "Epoch: 177/513 Train Loss: 213.3263\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 178/513 | Batch 000/059 | Loss: 170.9633\n",
      "Epoch: 178/513 Train Loss: 215.5726\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 179/513 | Batch 000/059 | Loss: 372.9302\n",
      "Epoch: 179/513 Train Loss: 215.7947\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 180/513 | Batch 000/059 | Loss: 333.9520\n",
      "Epoch: 180/513 Train Loss: 213.5830\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 181/513 | Batch 000/059 | Loss: 398.9269\n",
      "Epoch: 181/513 Train Loss: 213.5818\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 182/513 | Batch 000/059 | Loss: 140.5045\n",
      "Epoch: 182/513 Train Loss: 213.6067\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 183/513 | Batch 000/059 | Loss: 225.9692\n",
      "Epoch: 183/513 Train Loss: 213.1573\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 184/513 | Batch 000/059 | Loss: 263.0718\n",
      "Epoch: 184/513 Train Loss: 213.6352\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 185/513 | Batch 000/059 | Loss: 129.6055\n",
      "Epoch: 185/513 Train Loss: 214.3198\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 186/513 | Batch 000/059 | Loss: 214.3230\n",
      "Epoch: 186/513 Train Loss: 215.1131\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 187/513 | Batch 000/059 | Loss: 276.4883\n",
      "Epoch: 187/513 Train Loss: 214.8062\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 188/513 | Batch 000/059 | Loss: 226.5154\n",
      "Epoch: 188/513 Train Loss: 214.7872\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 189/513 | Batch 000/059 | Loss: 186.2141\n",
      "Epoch: 189/513 Train Loss: 212.7537\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 190/513 | Batch 000/059 | Loss: 215.4852\n",
      "Epoch: 190/513 Train Loss: 215.0090\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 191/513 | Batch 000/059 | Loss: 193.9572\n",
      "Epoch: 191/513 Train Loss: 214.2543\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 192/513 | Batch 000/059 | Loss: 169.9547\n",
      "Epoch: 192/513 Train Loss: 222.2990\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 193/513 | Batch 000/059 | Loss: 215.4552\n",
      "Epoch: 193/513 Train Loss: 235.4799\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 194/513 | Batch 000/059 | Loss: 286.5909\n",
      "Epoch: 194/513 Train Loss: 212.8841\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 195/513 | Batch 000/059 | Loss: 177.1140\n",
      "Epoch: 195/513 Train Loss: 221.3507\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 196/513 | Batch 000/059 | Loss: 164.5836\n",
      "Epoch: 196/513 Train Loss: 216.4506\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 197/513 | Batch 000/059 | Loss: 101.1790\n",
      "Epoch: 197/513 Train Loss: 212.9977\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 198/513 | Batch 000/059 | Loss: 135.8791\n",
      "Epoch: 198/513 Train Loss: 214.2187\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 199/513 | Batch 000/059 | Loss: 265.6407\n",
      "Epoch: 199/513 Train Loss: 217.3515\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 200/513 | Batch 000/059 | Loss: 178.0390\n",
      "Epoch: 200/513 Train Loss: 212.8744\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 201/513 | Batch 000/059 | Loss: 279.8641\n",
      "Epoch: 201/513 Train Loss: 218.4263\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 202/513 | Batch 000/059 | Loss: 145.9713\n",
      "Epoch: 202/513 Train Loss: 213.5040\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 203/513 | Batch 000/059 | Loss: 354.8246\n",
      "Epoch: 203/513 Train Loss: 214.4448\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 204/513 | Batch 000/059 | Loss: 161.9955\n",
      "Epoch: 204/513 Train Loss: 212.8278\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 205/513 | Batch 000/059 | Loss: 229.8572\n",
      "Epoch: 205/513 Train Loss: 213.5927\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 206/513 | Batch 000/059 | Loss: 160.2313\n",
      "Epoch: 206/513 Train Loss: 212.2613\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 207/513 | Batch 000/059 | Loss: 118.6692\n",
      "Epoch: 207/513 Train Loss: 225.8615\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 208/513 | Batch 000/059 | Loss: 178.3169\n",
      "Epoch: 208/513 Train Loss: 215.9221\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 209/513 | Batch 000/059 | Loss: 96.3054\n",
      "Epoch: 209/513 Train Loss: 230.2303\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 210/513 | Batch 000/059 | Loss: 153.2705\n",
      "Epoch: 210/513 Train Loss: 212.7014\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 211/513 | Batch 000/059 | Loss: 187.4904\n",
      "Epoch: 211/513 Train Loss: 212.4493\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 212/513 | Batch 000/059 | Loss: 328.3754\n",
      "Epoch: 212/513 Train Loss: 215.2656\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 213/513 | Batch 000/059 | Loss: 296.6455\n",
      "Epoch: 213/513 Train Loss: 218.0868\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 214/513 | Batch 000/059 | Loss: 227.0158\n",
      "Epoch: 214/513 Train Loss: 213.0282\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 215/513 | Batch 000/059 | Loss: 151.4016\n",
      "Epoch: 215/513 Train Loss: 212.6290\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 216/513 | Batch 000/059 | Loss: 168.0071\n",
      "Epoch: 216/513 Train Loss: 217.4300\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 217/513 | Batch 000/059 | Loss: 208.7242\n",
      "Epoch: 217/513 Train Loss: 212.6869\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 218/513 | Batch 000/059 | Loss: 383.0239\n",
      "Epoch: 218/513 Train Loss: 217.3166\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 219/513 | Batch 000/059 | Loss: 191.4108\n",
      "Epoch: 219/513 Train Loss: 215.0916\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 220/513 | Batch 000/059 | Loss: 217.4831\n",
      "Epoch: 220/513 Train Loss: 216.4556\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 221/513 | Batch 000/059 | Loss: 189.8406\n",
      "Epoch: 221/513 Train Loss: 212.8230\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 222/513 | Batch 000/059 | Loss: 241.6175\n",
      "Epoch: 222/513 Train Loss: 212.0776\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 223/513 | Batch 000/059 | Loss: 235.8301\n",
      "Epoch: 223/513 Train Loss: 215.6954\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 224/513 | Batch 000/059 | Loss: 304.0612\n",
      "Epoch: 224/513 Train Loss: 223.2810\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 225/513 | Batch 000/059 | Loss: 155.1662\n",
      "Epoch: 225/513 Train Loss: 249.4228\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 226/513 | Batch 000/059 | Loss: 195.3032\n",
      "Epoch: 226/513 Train Loss: 227.7300\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 227/513 | Batch 000/059 | Loss: 166.2322\n",
      "Epoch: 227/513 Train Loss: 212.9447\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 228/513 | Batch 000/059 | Loss: 346.9105\n",
      "Epoch: 228/513 Train Loss: 229.2036\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 229/513 | Batch 000/059 | Loss: 231.7984\n",
      "Epoch: 229/513 Train Loss: 212.2291\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 230/513 | Batch 000/059 | Loss: 231.5276\n",
      "Epoch: 230/513 Train Loss: 235.2040\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 231/513 | Batch 000/059 | Loss: 125.3791\n",
      "Epoch: 231/513 Train Loss: 213.5933\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 232/513 | Batch 000/059 | Loss: 281.6821\n",
      "Epoch: 232/513 Train Loss: 211.7979\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 233/513 | Batch 000/059 | Loss: 122.0303\n",
      "Epoch: 233/513 Train Loss: 213.5083\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 234/513 | Batch 000/059 | Loss: 221.8957\n",
      "Epoch: 234/513 Train Loss: 215.3590\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 235/513 | Batch 000/059 | Loss: 261.9048\n",
      "Epoch: 235/513 Train Loss: 213.6917\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 236/513 | Batch 000/059 | Loss: 184.4078\n",
      "Epoch: 236/513 Train Loss: 213.4387\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 237/513 | Batch 000/059 | Loss: 216.1759\n",
      "Epoch: 237/513 Train Loss: 255.7831\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 238/513 | Batch 000/059 | Loss: 249.1047\n",
      "Epoch: 238/513 Train Loss: 211.6157\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 239/513 | Batch 000/059 | Loss: 184.5783\n",
      "Epoch: 239/513 Train Loss: 216.2268\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 240/513 | Batch 000/059 | Loss: 162.8549\n",
      "Epoch: 240/513 Train Loss: 213.1459\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 241/513 | Batch 000/059 | Loss: 280.2730\n",
      "Epoch: 241/513 Train Loss: 212.6847\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 242/513 | Batch 000/059 | Loss: 213.2903\n",
      "Epoch: 242/513 Train Loss: 212.4035\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 243/513 | Batch 000/059 | Loss: 204.8964\n",
      "Epoch: 243/513 Train Loss: 213.1355\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 244/513 | Batch 000/059 | Loss: 428.1239\n",
      "Epoch: 244/513 Train Loss: 215.1549\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 245/513 | Batch 000/059 | Loss: 343.6930\n",
      "Epoch: 245/513 Train Loss: 215.2888\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 246/513 | Batch 000/059 | Loss: 316.2745\n",
      "Epoch: 246/513 Train Loss: 216.4983\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 247/513 | Batch 000/059 | Loss: 189.5475\n",
      "Epoch: 247/513 Train Loss: 213.2319\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 248/513 | Batch 000/059 | Loss: 151.4570\n",
      "Epoch: 248/513 Train Loss: 214.4325\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 249/513 | Batch 000/059 | Loss: 195.6049\n",
      "Epoch: 249/513 Train Loss: 225.9093\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 250/513 | Batch 000/059 | Loss: 198.1083\n",
      "Epoch: 250/513 Train Loss: 213.6850\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 251/513 | Batch 000/059 | Loss: 188.9751\n",
      "Epoch: 251/513 Train Loss: 220.6350\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 252/513 | Batch 000/059 | Loss: 214.5562\n",
      "Epoch: 252/513 Train Loss: 212.0835\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 253/513 | Batch 000/059 | Loss: 188.4348\n",
      "Epoch: 253/513 Train Loss: 212.3081\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 254/513 | Batch 000/059 | Loss: 274.4798\n",
      "Epoch: 254/513 Train Loss: 214.1152\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 255/513 | Batch 000/059 | Loss: 354.9068\n",
      "Epoch: 255/513 Train Loss: 214.4718\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 256/513 | Batch 000/059 | Loss: 224.0956\n",
      "Epoch: 256/513 Train Loss: 214.4496\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 257/513 | Batch 000/059 | Loss: 147.8663\n",
      "Epoch: 257/513 Train Loss: 213.2276\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 258/513 | Batch 000/059 | Loss: 252.4062\n",
      "Epoch: 258/513 Train Loss: 215.4688\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 259/513 | Batch 000/059 | Loss: 258.8641\n",
      "Epoch: 259/513 Train Loss: 215.5471\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 260/513 | Batch 000/059 | Loss: 302.1685\n",
      "Epoch: 260/513 Train Loss: 211.3527\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 261/513 | Batch 000/059 | Loss: 121.2136\n",
      "Epoch: 261/513 Train Loss: 212.1518\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 262/513 | Batch 000/059 | Loss: 137.1210\n",
      "Epoch: 262/513 Train Loss: 215.6419\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 263/513 | Batch 000/059 | Loss: 141.0742\n",
      "Epoch: 263/513 Train Loss: 215.3984\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 264/513 | Batch 000/059 | Loss: 152.6134\n",
      "Epoch: 264/513 Train Loss: 213.7867\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 265/513 | Batch 000/059 | Loss: 193.6658\n",
      "Epoch: 265/513 Train Loss: 211.4057\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 266/513 | Batch 000/059 | Loss: 231.0680\n",
      "Epoch: 266/513 Train Loss: 226.6253\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 267/513 | Batch 000/059 | Loss: 162.7083\n",
      "Epoch: 267/513 Train Loss: 215.8524\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 268/513 | Batch 000/059 | Loss: 133.9285\n",
      "Epoch: 268/513 Train Loss: 213.9848\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 269/513 | Batch 000/059 | Loss: 135.9368\n",
      "Epoch: 269/513 Train Loss: 214.7841\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 270/513 | Batch 000/059 | Loss: 194.7665\n",
      "Epoch: 270/513 Train Loss: 211.4664\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 271/513 | Batch 000/059 | Loss: 288.0953\n",
      "Epoch: 271/513 Train Loss: 216.1190\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 272/513 | Batch 000/059 | Loss: 146.2959\n",
      "Epoch: 272/513 Train Loss: 211.6524\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 273/513 | Batch 000/059 | Loss: 255.2060\n",
      "Epoch: 273/513 Train Loss: 211.5477\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 274/513 | Batch 000/059 | Loss: 274.8080\n",
      "Epoch: 274/513 Train Loss: 233.5399\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 275/513 | Batch 000/059 | Loss: 396.8148\n",
      "Epoch: 275/513 Train Loss: 214.4053\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 276/513 | Batch 000/059 | Loss: 167.3000\n",
      "Epoch: 276/513 Train Loss: 217.5226\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 277/513 | Batch 000/059 | Loss: 233.8176\n",
      "Epoch: 277/513 Train Loss: 225.4694\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 278/513 | Batch 000/059 | Loss: 154.2146\n",
      "Epoch: 278/513 Train Loss: 211.6433\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 279/513 | Batch 000/059 | Loss: 179.5983\n",
      "Epoch: 279/513 Train Loss: 219.7587\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 280/513 | Batch 000/059 | Loss: 350.1885\n",
      "Epoch: 280/513 Train Loss: 213.2169\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 281/513 | Batch 000/059 | Loss: 242.4501\n",
      "Epoch: 281/513 Train Loss: 219.3246\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 282/513 | Batch 000/059 | Loss: 172.0279\n",
      "Epoch: 282/513 Train Loss: 229.5292\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 283/513 | Batch 000/059 | Loss: 231.9470\n",
      "Epoch: 283/513 Train Loss: 219.8956\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 284/513 | Batch 000/059 | Loss: 211.4013\n",
      "Epoch: 284/513 Train Loss: 212.0183\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 285/513 | Batch 000/059 | Loss: 231.6011\n",
      "Epoch: 285/513 Train Loss: 211.5740\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 286/513 | Batch 000/059 | Loss: 382.2921\n",
      "Epoch: 286/513 Train Loss: 212.4576\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 287/513 | Batch 000/059 | Loss: 95.0421\n",
      "Epoch: 287/513 Train Loss: 211.6156\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 288/513 | Batch 000/059 | Loss: 231.7580\n",
      "Epoch: 288/513 Train Loss: 211.0151\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 289/513 | Batch 000/059 | Loss: 173.3425\n",
      "Epoch: 289/513 Train Loss: 214.3641\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 290/513 | Batch 000/059 | Loss: 240.8022\n",
      "Epoch: 290/513 Train Loss: 233.1098\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 291/513 | Batch 000/059 | Loss: 197.1095\n",
      "Epoch: 291/513 Train Loss: 211.6403\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 292/513 | Batch 000/059 | Loss: 259.2080\n",
      "Epoch: 292/513 Train Loss: 212.0225\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 293/513 | Batch 000/059 | Loss: 282.8628\n",
      "Epoch: 293/513 Train Loss: 217.3419\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 294/513 | Batch 000/059 | Loss: 196.8857\n",
      "Epoch: 294/513 Train Loss: 214.7841\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 295/513 | Batch 000/059 | Loss: 135.3622\n",
      "Epoch: 295/513 Train Loss: 226.9844\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 296/513 | Batch 000/059 | Loss: 457.9411\n",
      "Epoch: 296/513 Train Loss: 213.6418\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 297/513 | Batch 000/059 | Loss: 244.5980\n",
      "Epoch: 297/513 Train Loss: 211.3571\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 298/513 | Batch 000/059 | Loss: 211.3005\n",
      "Epoch: 298/513 Train Loss: 212.3116\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 299/513 | Batch 000/059 | Loss: 187.8660\n",
      "Epoch: 299/513 Train Loss: 212.2358\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 300/513 | Batch 000/059 | Loss: 209.3117\n",
      "Epoch: 300/513 Train Loss: 217.8756\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 301/513 | Batch 000/059 | Loss: 208.6185\n",
      "Epoch: 301/513 Train Loss: 211.5531\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 302/513 | Batch 000/059 | Loss: 275.6693\n",
      "Epoch: 302/513 Train Loss: 211.1400\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 303/513 | Batch 000/059 | Loss: 165.5447\n",
      "Epoch: 303/513 Train Loss: 211.8382\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 304/513 | Batch 000/059 | Loss: 208.5792\n",
      "Epoch: 304/513 Train Loss: 211.1061\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 305/513 | Batch 000/059 | Loss: 173.1897\n",
      "Epoch: 305/513 Train Loss: 261.1944\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 306/513 | Batch 000/059 | Loss: 280.9294\n",
      "Epoch: 306/513 Train Loss: 215.0673\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 307/513 | Batch 000/059 | Loss: 137.9351\n",
      "Epoch: 307/513 Train Loss: 213.2561\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 308/513 | Batch 000/059 | Loss: 175.6196\n",
      "Epoch: 308/513 Train Loss: 211.0066\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 309/513 | Batch 000/059 | Loss: 153.7616\n",
      "Epoch: 309/513 Train Loss: 212.0041\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 310/513 | Batch 000/059 | Loss: 203.3298\n",
      "Epoch: 310/513 Train Loss: 213.9058\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 311/513 | Batch 000/059 | Loss: 224.6596\n",
      "Epoch: 311/513 Train Loss: 210.9942\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 312/513 | Batch 000/059 | Loss: 245.6591\n",
      "Epoch: 312/513 Train Loss: 216.5334\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 313/513 | Batch 000/059 | Loss: 182.0882\n",
      "Epoch: 313/513 Train Loss: 211.8893\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 314/513 | Batch 000/059 | Loss: 152.1299\n",
      "Epoch: 314/513 Train Loss: 212.5513\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 315/513 | Batch 000/059 | Loss: 190.0490\n",
      "Epoch: 315/513 Train Loss: 222.8865\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 316/513 | Batch 000/059 | Loss: 164.3658\n",
      "Epoch: 316/513 Train Loss: 211.6224\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 317/513 | Batch 000/059 | Loss: 205.5880\n",
      "Epoch: 317/513 Train Loss: 211.4466\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 318/513 | Batch 000/059 | Loss: 209.4390\n",
      "Epoch: 318/513 Train Loss: 211.9533\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 319/513 | Batch 000/059 | Loss: 240.6856\n",
      "Epoch: 319/513 Train Loss: 212.4100\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 320/513 | Batch 000/059 | Loss: 104.9286\n",
      "Epoch: 320/513 Train Loss: 212.6809\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 321/513 | Batch 000/059 | Loss: 176.0073\n",
      "Epoch: 321/513 Train Loss: 219.9673\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 322/513 | Batch 000/059 | Loss: 196.1933\n",
      "Epoch: 322/513 Train Loss: 213.3712\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 323/513 | Batch 000/059 | Loss: 186.6417\n",
      "Epoch: 323/513 Train Loss: 221.6914\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 324/513 | Batch 000/059 | Loss: 172.1544\n",
      "Epoch: 324/513 Train Loss: 211.3249\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 325/513 | Batch 000/059 | Loss: 271.3104\n",
      "Epoch: 325/513 Train Loss: 217.3173\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 326/513 | Batch 000/059 | Loss: 282.1840\n",
      "Epoch: 326/513 Train Loss: 227.7273\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 327/513 | Batch 000/059 | Loss: 295.3658\n",
      "Epoch: 327/513 Train Loss: 213.1093\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 328/513 | Batch 000/059 | Loss: 255.6706\n",
      "Epoch: 328/513 Train Loss: 215.9858\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 329/513 | Batch 000/059 | Loss: 226.5219\n",
      "Epoch: 329/513 Train Loss: 212.0916\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 330/513 | Batch 000/059 | Loss: 272.0101\n",
      "Epoch: 330/513 Train Loss: 215.1718\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 331/513 | Batch 000/059 | Loss: 226.4780\n",
      "Epoch: 331/513 Train Loss: 211.4591\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 332/513 | Batch 000/059 | Loss: 261.8180\n",
      "Epoch: 332/513 Train Loss: 214.6830\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 333/513 | Batch 000/059 | Loss: 182.0661\n",
      "Epoch: 333/513 Train Loss: 222.8268\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 334/513 | Batch 000/059 | Loss: 109.0697\n",
      "Epoch: 334/513 Train Loss: 216.6822\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 335/513 | Batch 000/059 | Loss: 267.3326\n",
      "Epoch: 335/513 Train Loss: 211.7191\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 336/513 | Batch 000/059 | Loss: 202.5954\n",
      "Epoch: 336/513 Train Loss: 218.1222\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 337/513 | Batch 000/059 | Loss: 170.2818\n",
      "Epoch: 337/513 Train Loss: 213.6909\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 338/513 | Batch 000/059 | Loss: 216.1427\n",
      "Epoch: 338/513 Train Loss: 211.9165\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 339/513 | Batch 000/059 | Loss: 135.2839\n",
      "Epoch: 339/513 Train Loss: 214.1255\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 340/513 | Batch 000/059 | Loss: 315.9978\n",
      "Epoch: 340/513 Train Loss: 211.8184\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 341/513 | Batch 000/059 | Loss: 195.7054\n",
      "Epoch: 341/513 Train Loss: 212.6215\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 342/513 | Batch 000/059 | Loss: 168.0464\n",
      "Epoch: 342/513 Train Loss: 217.8522\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 343/513 | Batch 000/059 | Loss: 244.8768\n",
      "Epoch: 343/513 Train Loss: 228.7154\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 344/513 | Batch 000/059 | Loss: 195.1009\n",
      "Epoch: 344/513 Train Loss: 213.3238\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 345/513 | Batch 000/059 | Loss: 189.2788\n",
      "Epoch: 345/513 Train Loss: 211.3968\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 346/513 | Batch 000/059 | Loss: 179.8368\n",
      "Epoch: 346/513 Train Loss: 211.3724\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 347/513 | Batch 000/059 | Loss: 372.5526\n",
      "Epoch: 347/513 Train Loss: 211.4294\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 348/513 | Batch 000/059 | Loss: 175.5023\n",
      "Epoch: 348/513 Train Loss: 212.1889\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 349/513 | Batch 000/059 | Loss: 162.1506\n",
      "Epoch: 349/513 Train Loss: 213.1511\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 350/513 | Batch 000/059 | Loss: 320.6478\n",
      "Epoch: 350/513 Train Loss: 268.1676\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 351/513 | Batch 000/059 | Loss: 218.2228\n",
      "Epoch: 351/513 Train Loss: 211.2603\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 352/513 | Batch 000/059 | Loss: 140.3132\n",
      "Epoch: 352/513 Train Loss: 211.4307\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 353/513 | Batch 000/059 | Loss: 143.8935\n",
      "Epoch: 353/513 Train Loss: 210.4534\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 354/513 | Batch 000/059 | Loss: 222.9213\n",
      "Epoch: 354/513 Train Loss: 217.1239\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 355/513 | Batch 000/059 | Loss: 196.2576\n",
      "Epoch: 355/513 Train Loss: 211.5055\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 356/513 | Batch 000/059 | Loss: 180.2990\n",
      "Epoch: 356/513 Train Loss: 225.0057\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 357/513 | Batch 000/059 | Loss: 213.4066\n",
      "Epoch: 357/513 Train Loss: 213.1791\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 358/513 | Batch 000/059 | Loss: 193.3555\n",
      "Epoch: 358/513 Train Loss: 218.4253\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 359/513 | Batch 000/059 | Loss: 202.8945\n",
      "Epoch: 359/513 Train Loss: 210.8913\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 360/513 | Batch 000/059 | Loss: 385.8805\n",
      "Epoch: 360/513 Train Loss: 216.9150\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 361/513 | Batch 000/059 | Loss: 217.2942\n",
      "Epoch: 361/513 Train Loss: 210.6394\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 362/513 | Batch 000/059 | Loss: 142.8181\n",
      "Epoch: 362/513 Train Loss: 210.9059\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 363/513 | Batch 000/059 | Loss: 226.6690\n",
      "Epoch: 363/513 Train Loss: 212.0235\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 364/513 | Batch 000/059 | Loss: 217.9581\n",
      "Epoch: 364/513 Train Loss: 215.3518\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 365/513 | Batch 000/059 | Loss: 192.5790\n",
      "Epoch: 365/513 Train Loss: 211.8004\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 366/513 | Batch 000/059 | Loss: 259.2953\n",
      "Epoch: 366/513 Train Loss: 210.4196\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 367/513 | Batch 000/059 | Loss: 247.7854\n",
      "Epoch: 367/513 Train Loss: 210.5643\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 368/513 | Batch 000/059 | Loss: 207.2893\n",
      "Epoch: 368/513 Train Loss: 217.5012\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 369/513 | Batch 000/059 | Loss: 151.0364\n",
      "Epoch: 369/513 Train Loss: 217.0217\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 370/513 | Batch 000/059 | Loss: 245.7938\n",
      "Epoch: 370/513 Train Loss: 210.6705\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 371/513 | Batch 000/059 | Loss: 147.9121\n",
      "Epoch: 371/513 Train Loss: 214.1716\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 372/513 | Batch 000/059 | Loss: 356.0909\n",
      "Epoch: 372/513 Train Loss: 210.8111\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 373/513 | Batch 000/059 | Loss: 127.4367\n",
      "Epoch: 373/513 Train Loss: 211.0760\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 374/513 | Batch 000/059 | Loss: 213.4266\n",
      "Epoch: 374/513 Train Loss: 212.8047\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 375/513 | Batch 000/059 | Loss: 291.9183\n",
      "Epoch: 375/513 Train Loss: 211.8520\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 376/513 | Batch 000/059 | Loss: 291.0587\n",
      "Epoch: 376/513 Train Loss: 210.3682\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 377/513 | Batch 000/059 | Loss: 216.2085\n",
      "Epoch: 377/513 Train Loss: 222.4831\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 378/513 | Batch 000/059 | Loss: 204.7161\n",
      "Epoch: 378/513 Train Loss: 232.3028\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 379/513 | Batch 000/059 | Loss: 207.8633\n",
      "Epoch: 379/513 Train Loss: 211.1104\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 380/513 | Batch 000/059 | Loss: 213.0357\n",
      "Epoch: 380/513 Train Loss: 211.4023\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 381/513 | Batch 000/059 | Loss: 144.5633\n",
      "Epoch: 381/513 Train Loss: 210.0255\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 382/513 | Batch 000/059 | Loss: 301.2922\n",
      "Epoch: 382/513 Train Loss: 228.5098\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 383/513 | Batch 000/059 | Loss: 225.0264\n",
      "Epoch: 383/513 Train Loss: 215.5301\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 384/513 | Batch 000/059 | Loss: 152.4383\n",
      "Epoch: 384/513 Train Loss: 210.5760\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 385/513 | Batch 000/059 | Loss: 206.6475\n",
      "Epoch: 385/513 Train Loss: 211.9682\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 386/513 | Batch 000/059 | Loss: 274.0411\n",
      "Epoch: 386/513 Train Loss: 219.9760\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 387/513 | Batch 000/059 | Loss: 247.3457\n",
      "Epoch: 387/513 Train Loss: 212.4218\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 388/513 | Batch 000/059 | Loss: 262.6295\n",
      "Epoch: 388/513 Train Loss: 237.4463\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 389/513 | Batch 000/059 | Loss: 244.9644\n",
      "Epoch: 389/513 Train Loss: 213.1532\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 390/513 | Batch 000/059 | Loss: 189.7526\n",
      "Epoch: 390/513 Train Loss: 210.8402\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 391/513 | Batch 000/059 | Loss: 141.7250\n",
      "Epoch: 391/513 Train Loss: 225.9446\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 392/513 | Batch 000/059 | Loss: 218.1001\n",
      "Epoch: 392/513 Train Loss: 216.2473\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 393/513 | Batch 000/059 | Loss: 144.0929\n",
      "Epoch: 393/513 Train Loss: 214.4237\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 394/513 | Batch 000/059 | Loss: 226.2705\n",
      "Epoch: 394/513 Train Loss: 212.7767\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 395/513 | Batch 000/059 | Loss: 187.0464\n",
      "Epoch: 395/513 Train Loss: 211.0509\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 396/513 | Batch 000/059 | Loss: 153.0461\n",
      "Epoch: 396/513 Train Loss: 211.3985\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 397/513 | Batch 000/059 | Loss: 247.1605\n",
      "Epoch: 397/513 Train Loss: 212.5677\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 398/513 | Batch 000/059 | Loss: 298.3369\n",
      "Epoch: 398/513 Train Loss: 211.9936\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 399/513 | Batch 000/059 | Loss: 287.5302\n",
      "Epoch: 399/513 Train Loss: 213.9178\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 400/513 | Batch 000/059 | Loss: 166.4341\n",
      "Epoch: 400/513 Train Loss: 212.5272\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 401/513 | Batch 000/059 | Loss: 127.1766\n",
      "Epoch: 401/513 Train Loss: 211.5241\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 402/513 | Batch 000/059 | Loss: 180.2103\n",
      "Epoch: 402/513 Train Loss: 210.5508\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 403/513 | Batch 000/059 | Loss: 223.8984\n",
      "Epoch: 403/513 Train Loss: 211.3450\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 404/513 | Batch 000/059 | Loss: 224.8553\n",
      "Epoch: 404/513 Train Loss: 222.5292\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 405/513 | Batch 000/059 | Loss: 259.5134\n",
      "Epoch: 405/513 Train Loss: 212.1427\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 406/513 | Batch 000/059 | Loss: 222.4286\n",
      "Epoch: 406/513 Train Loss: 210.8587\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 407/513 | Batch 000/059 | Loss: 194.7742\n",
      "Epoch: 407/513 Train Loss: 210.9577\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 408/513 | Batch 000/059 | Loss: 155.9481\n",
      "Epoch: 408/513 Train Loss: 214.4064\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 409/513 | Batch 000/059 | Loss: 272.2052\n",
      "Epoch: 409/513 Train Loss: 220.7548\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 410/513 | Batch 000/059 | Loss: 144.8527\n",
      "Epoch: 410/513 Train Loss: 218.0169\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 411/513 | Batch 000/059 | Loss: 379.1886\n",
      "Epoch: 411/513 Train Loss: 217.9501\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 412/513 | Batch 000/059 | Loss: 366.6477\n",
      "Epoch: 412/513 Train Loss: 215.9849\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 413/513 | Batch 000/059 | Loss: 369.6142\n",
      "Epoch: 413/513 Train Loss: 210.3712\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 414/513 | Batch 000/059 | Loss: 323.5594\n",
      "Epoch: 414/513 Train Loss: 211.5723\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 415/513 | Batch 000/059 | Loss: 418.4206\n",
      "Epoch: 415/513 Train Loss: 211.3766\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 416/513 | Batch 000/059 | Loss: 361.8768\n",
      "Epoch: 416/513 Train Loss: 212.7702\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 417/513 | Batch 000/059 | Loss: 233.6158\n",
      "Epoch: 417/513 Train Loss: 211.8389\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 418/513 | Batch 000/059 | Loss: 193.7506\n",
      "Epoch: 418/513 Train Loss: 214.9377\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 419/513 | Batch 000/059 | Loss: 202.7017\n",
      "Epoch: 419/513 Train Loss: 217.4280\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 420/513 | Batch 000/059 | Loss: 234.1335\n",
      "Epoch: 420/513 Train Loss: 211.0543\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 421/513 | Batch 000/059 | Loss: 165.2237\n",
      "Epoch: 421/513 Train Loss: 211.1630\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 422/513 | Batch 000/059 | Loss: 140.3597\n",
      "Epoch: 422/513 Train Loss: 213.2811\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 423/513 | Batch 000/059 | Loss: 134.0108\n",
      "Epoch: 423/513 Train Loss: 212.4762\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 424/513 | Batch 000/059 | Loss: 339.6088\n",
      "Epoch: 424/513 Train Loss: 211.7085\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 425/513 | Batch 000/059 | Loss: 161.7833\n",
      "Epoch: 425/513 Train Loss: 218.7787\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 426/513 | Batch 000/059 | Loss: 256.7028\n",
      "Epoch: 426/513 Train Loss: 211.3281\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 427/513 | Batch 000/059 | Loss: 180.7535\n",
      "Epoch: 427/513 Train Loss: 216.5883\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 428/513 | Batch 000/059 | Loss: 240.1831\n",
      "Epoch: 428/513 Train Loss: 213.0816\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 429/513 | Batch 000/059 | Loss: 208.3178\n",
      "Epoch: 429/513 Train Loss: 213.2368\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 430/513 | Batch 000/059 | Loss: 200.4057\n",
      "Epoch: 430/513 Train Loss: 212.0828\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 431/513 | Batch 000/059 | Loss: 322.5844\n",
      "Epoch: 431/513 Train Loss: 210.6019\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 432/513 | Batch 000/059 | Loss: 153.6379\n",
      "Epoch: 432/513 Train Loss: 228.0139\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 433/513 | Batch 000/059 | Loss: 203.7454\n",
      "Epoch: 433/513 Train Loss: 213.5282\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 434/513 | Batch 000/059 | Loss: 267.8112\n",
      "Epoch: 434/513 Train Loss: 214.8985\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 435/513 | Batch 000/059 | Loss: 156.3815\n",
      "Epoch: 435/513 Train Loss: 210.4089\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 436/513 | Batch 000/059 | Loss: 195.9312\n",
      "Epoch: 436/513 Train Loss: 211.2563\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 437/513 | Batch 000/059 | Loss: 236.3998\n",
      "Epoch: 437/513 Train Loss: 211.6515\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 438/513 | Batch 000/059 | Loss: 178.8667\n",
      "Epoch: 438/513 Train Loss: 210.5199\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 439/513 | Batch 000/059 | Loss: 214.0625\n",
      "Epoch: 439/513 Train Loss: 215.3994\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 440/513 | Batch 000/059 | Loss: 262.2627\n",
      "Epoch: 440/513 Train Loss: 210.7189\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 441/513 | Batch 000/059 | Loss: 135.9731\n",
      "Epoch: 441/513 Train Loss: 223.9275\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 442/513 | Batch 000/059 | Loss: 382.9405\n",
      "Epoch: 442/513 Train Loss: 210.4979\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 443/513 | Batch 000/059 | Loss: 147.8338\n",
      "Epoch: 443/513 Train Loss: 210.8287\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 444/513 | Batch 000/059 | Loss: 100.5304\n",
      "Epoch: 444/513 Train Loss: 213.0922\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 445/513 | Batch 000/059 | Loss: 170.2333\n",
      "Epoch: 445/513 Train Loss: 223.4757\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 446/513 | Batch 000/059 | Loss: 185.0004\n",
      "Epoch: 446/513 Train Loss: 210.8628\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 447/513 | Batch 000/059 | Loss: 217.0471\n",
      "Epoch: 447/513 Train Loss: 210.9477\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 448/513 | Batch 000/059 | Loss: 245.4599\n",
      "Epoch: 448/513 Train Loss: 219.3884\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 449/513 | Batch 000/059 | Loss: 176.7414\n",
      "Epoch: 449/513 Train Loss: 211.1916\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 450/513 | Batch 000/059 | Loss: 108.7660\n",
      "Epoch: 450/513 Train Loss: 210.6664\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 451/513 | Batch 000/059 | Loss: 171.5205\n",
      "Epoch: 451/513 Train Loss: 214.8528\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 452/513 | Batch 000/059 | Loss: 124.0806\n",
      "Epoch: 452/513 Train Loss: 212.7810\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 453/513 | Batch 000/059 | Loss: 287.5577\n",
      "Epoch: 453/513 Train Loss: 210.3313\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 454/513 | Batch 000/059 | Loss: 332.3528\n",
      "Epoch: 454/513 Train Loss: 210.0365\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 455/513 | Batch 000/059 | Loss: 134.9301\n",
      "Epoch: 455/513 Train Loss: 211.2637\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 456/513 | Batch 000/059 | Loss: 219.4198\n",
      "Epoch: 456/513 Train Loss: 211.2366\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 457/513 | Batch 000/059 | Loss: 277.6774\n",
      "Epoch: 457/513 Train Loss: 210.4453\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 458/513 | Batch 000/059 | Loss: 278.3937\n",
      "Epoch: 458/513 Train Loss: 210.1374\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 459/513 | Batch 000/059 | Loss: 234.4958\n",
      "Epoch: 459/513 Train Loss: 213.0715\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 460/513 | Batch 000/059 | Loss: 135.4906\n",
      "Epoch: 460/513 Train Loss: 214.5376\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 461/513 | Batch 000/059 | Loss: 371.0992\n",
      "Epoch: 461/513 Train Loss: 210.7876\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 462/513 | Batch 000/059 | Loss: 199.0597\n",
      "Epoch: 462/513 Train Loss: 210.2559\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 463/513 | Batch 000/059 | Loss: 193.7850\n",
      "Epoch: 463/513 Train Loss: 211.6393\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 464/513 | Batch 000/059 | Loss: 174.5324\n",
      "Epoch: 464/513 Train Loss: 209.9350\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 465/513 | Batch 000/059 | Loss: 273.9407\n",
      "Epoch: 465/513 Train Loss: 214.8332\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 466/513 | Batch 000/059 | Loss: 475.6690\n",
      "Epoch: 466/513 Train Loss: 220.5376\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 467/513 | Batch 000/059 | Loss: 151.1714\n",
      "Epoch: 467/513 Train Loss: 212.0761\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 468/513 | Batch 000/059 | Loss: 124.1268\n",
      "Epoch: 468/513 Train Loss: 217.8607\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 469/513 | Batch 000/059 | Loss: 151.6580\n",
      "Epoch: 469/513 Train Loss: 209.5301\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 470/513 | Batch 000/059 | Loss: 170.7370\n",
      "Epoch: 470/513 Train Loss: 215.0245\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 471/513 | Batch 000/059 | Loss: 179.3607\n",
      "Epoch: 471/513 Train Loss: 232.6069\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 472/513 | Batch 000/059 | Loss: 221.9448\n",
      "Epoch: 472/513 Train Loss: 212.9372\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 473/513 | Batch 000/059 | Loss: 208.5452\n",
      "Epoch: 473/513 Train Loss: 219.6914\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 474/513 | Batch 000/059 | Loss: 226.8580\n",
      "Epoch: 474/513 Train Loss: 210.7063\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 475/513 | Batch 000/059 | Loss: 134.9734\n",
      "Epoch: 475/513 Train Loss: 219.0197\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 476/513 | Batch 000/059 | Loss: 174.6642\n",
      "Epoch: 476/513 Train Loss: 212.9627\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 477/513 | Batch 000/059 | Loss: 337.7772\n",
      "Epoch: 477/513 Train Loss: 229.5376\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 478/513 | Batch 000/059 | Loss: 280.4783\n",
      "Epoch: 478/513 Train Loss: 210.4161\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 479/513 | Batch 000/059 | Loss: 129.8178\n",
      "Epoch: 479/513 Train Loss: 211.4335\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 480/513 | Batch 000/059 | Loss: 321.7767\n",
      "Epoch: 480/513 Train Loss: 212.0553\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 481/513 | Batch 000/059 | Loss: 197.0513\n",
      "Epoch: 481/513 Train Loss: 218.0987\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 482/513 | Batch 000/059 | Loss: 163.9545\n",
      "Epoch: 482/513 Train Loss: 216.6332\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 483/513 | Batch 000/059 | Loss: 185.4522\n",
      "Epoch: 483/513 Train Loss: 212.2808\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 484/513 | Batch 000/059 | Loss: 150.4255\n",
      "Epoch: 484/513 Train Loss: 210.1162\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 485/513 | Batch 000/059 | Loss: 280.6340\n",
      "Epoch: 485/513 Train Loss: 215.9452\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 486/513 | Batch 000/059 | Loss: 190.7996\n",
      "Epoch: 486/513 Train Loss: 211.3470\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 487/513 | Batch 000/059 | Loss: 216.9233\n",
      "Epoch: 487/513 Train Loss: 212.3574\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 488/513 | Batch 000/059 | Loss: 179.4892\n",
      "Epoch: 488/513 Train Loss: 216.9282\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 489/513 | Batch 000/059 | Loss: 140.0796\n",
      "Epoch: 489/513 Train Loss: 224.6692\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 490/513 | Batch 000/059 | Loss: 204.3584\n",
      "Epoch: 490/513 Train Loss: 220.1995\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 491/513 | Batch 000/059 | Loss: 210.8457\n",
      "Epoch: 491/513 Train Loss: 221.5985\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 492/513 | Batch 000/059 | Loss: 165.4906\n",
      "Epoch: 492/513 Train Loss: 212.8065\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 493/513 | Batch 000/059 | Loss: 209.9752\n",
      "Epoch: 493/513 Train Loss: 214.4864\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 494/513 | Batch 000/059 | Loss: 138.6396\n",
      "Epoch: 494/513 Train Loss: 219.7581\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 495/513 | Batch 000/059 | Loss: 292.4430\n",
      "Epoch: 495/513 Train Loss: 212.7434\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 496/513 | Batch 000/059 | Loss: 293.3322\n",
      "Epoch: 496/513 Train Loss: 238.6779\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 497/513 | Batch 000/059 | Loss: 302.0185\n",
      "Epoch: 497/513 Train Loss: 210.7360\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 498/513 | Batch 000/059 | Loss: 206.9507\n",
      "Epoch: 498/513 Train Loss: 210.4290\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 499/513 | Batch 000/059 | Loss: 227.3053\n",
      "Epoch: 499/513 Train Loss: 210.7484\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 500/513 | Batch 000/059 | Loss: 118.0009\n",
      "Epoch: 500/513 Train Loss: 209.6731\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 501/513 | Batch 000/059 | Loss: 205.6731\n",
      "Epoch: 501/513 Train Loss: 211.3473\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 502/513 | Batch 000/059 | Loss: 211.3470\n",
      "Epoch: 502/513 Train Loss: 213.2491\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 503/513 | Batch 000/059 | Loss: 220.1788\n",
      "Epoch: 503/513 Train Loss: 211.1274\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 504/513 | Batch 000/059 | Loss: 152.7192\n",
      "Epoch: 504/513 Train Loss: 211.2213\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 505/513 | Batch 000/059 | Loss: 242.3686\n",
      "Epoch: 505/513 Train Loss: 209.8155\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 506/513 | Batch 000/059 | Loss: 140.6062\n",
      "Epoch: 506/513 Train Loss: 209.2577\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 507/513 | Batch 000/059 | Loss: 177.4151\n",
      "Epoch: 507/513 Train Loss: 218.7744\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 508/513 | Batch 000/059 | Loss: 291.5624\n",
      "Epoch: 508/513 Train Loss: 211.4604\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 509/513 | Batch 000/059 | Loss: 173.1986\n",
      "Epoch: 509/513 Train Loss: 212.2893\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 510/513 | Batch 000/059 | Loss: 155.5262\n",
      "Epoch: 510/513 Train Loss: 218.9032\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 511/513 | Batch 000/059 | Loss: 98.9351\n",
      "Epoch: 511/513 Train Loss: 210.7451\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 512/513 | Batch 000/059 | Loss: 272.7600\n",
      "Epoch: 512/513 Train Loss: 216.9941\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 513/513 | Batch 000/059 | Loss: 378.9355\n",
      "Epoch: 513/513 Train Loss: 210.8757\n",
      "Time elapsed: 1.16 min\n",
      "Total Training Time: 1.16 min\n",
      "Training Loss: 210.88\n",
      "Test Loss: 221.33\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "minibatch_cost = []\n",
    "epoch_cost = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(device)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        outputs = model(features)\n",
    "        \n",
    "        cost = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        minibatch_cost.append(cost)\n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 1000:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "       \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        cost = compute_loss(model, train_loader)\n",
    "        epoch_cost.append(cost)\n",
    "        print('Epoch: %03d/%03d Train Loss: %.4f' % (\n",
    "                epoch+1, num_epochs, cost))\n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "        \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "    print('Training Loss: %.2f' % compute_loss(model, train_loader))\n",
    "    print('Test Loss: %.2f' % compute_loss(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "Uz7gkvMPoRvz",
    "outputId": "9afe0680-e85d-4224-8d11-8427de12520d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the loss curve\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(len(epoch_cost)), epoch_cost)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross entropy loss\")\n",
    "plt.title(\"Loss of training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dn5NcDKsO9oQ"
   },
   "source": [
    "## 3. SGD Nesterov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3bB_CzfpmAt"
   },
   "source": [
    "### Model B: 1 Hidden Layer Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlnGzjNTpmAt"
   },
   "source": [
    "### Create model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-O1fO_AUpmAu",
    "outputId": "eeccd329-42f3-41b2-d67d-a3c80c87192e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "n_iters = 30000\n",
    "num_epochs = n_iters / (len(train_data) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "print(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "id": "21IRkQiBpmAv"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # create your dataloader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "id": "JVRFglJIpmAw"
   },
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, output_dim):\n",
    "        super(FNN, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear4 = nn.Linear(hidden_dim1, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.linear4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "id": "aRCbmPugpmAw"
   },
   "outputs": [],
   "source": [
    "input_dim = tensor_x.shape[1]\n",
    "output_dim = 1\n",
    "hidden_dim1 = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "id": "g6j4o3u7pmAw"
   },
   "outputs": [],
   "source": [
    "model = FNN(input_dim, hidden_dim1, output_dim)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jxC3-ylNpmAw",
    "outputId": "0ba3aad6-8462-42ba-d1a5-6ae1c716a331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 256]           2,816\n",
      "           Dropout-2               [-1, 1, 256]               0\n",
      "              ReLU-3               [-1, 1, 256]               0\n",
      "            Linear-4                 [-1, 1, 1]             257\n",
      "================================================================\n",
      "Total params: 3,073\n",
      "Trainable params: 3,073\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1,tensor_x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "id": "YylY4SH0pmAw"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 0.0001\n",
    "LAMBDA = 0.05\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=LAMBDA, momentum=0.6, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bg_1QhuPpmAx"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djMe-KLNpmAx",
    "outputId": "d2288e83-9c01-46fd-8311-99812f31d4ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/513 | Batch 000/059 | Loss: 7081.2129\n",
      "Epoch: 001/513 Train Loss: 350.1418\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 002/513 | Batch 000/059 | Loss: 439.4808\n",
      "Epoch: 002/513 Train Loss: 340.4605\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 003/513 | Batch 000/059 | Loss: 597.1635\n",
      "Epoch: 003/513 Train Loss: 335.0772\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 004/513 | Batch 000/059 | Loss: 448.6649\n",
      "Epoch: 004/513 Train Loss: 339.0919\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 005/513 | Batch 000/059 | Loss: 573.1984\n",
      "Epoch: 005/513 Train Loss: 329.0674\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 006/513 | Batch 000/059 | Loss: 578.1392\n",
      "Epoch: 006/513 Train Loss: 333.3672\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 007/513 | Batch 000/059 | Loss: 302.4783\n",
      "Epoch: 007/513 Train Loss: 320.3042\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 008/513 | Batch 000/059 | Loss: 265.5581\n",
      "Epoch: 008/513 Train Loss: 316.5804\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 009/513 | Batch 000/059 | Loss: 253.0677\n",
      "Epoch: 009/513 Train Loss: 321.8779\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 010/513 | Batch 000/059 | Loss: 257.2841\n",
      "Epoch: 010/513 Train Loss: 313.7048\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 011/513 | Batch 000/059 | Loss: 536.5891\n",
      "Epoch: 011/513 Train Loss: 306.9176\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 012/513 | Batch 000/059 | Loss: 288.1452\n",
      "Epoch: 012/513 Train Loss: 302.5251\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 013/513 | Batch 000/059 | Loss: 212.3074\n",
      "Epoch: 013/513 Train Loss: 301.8697\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 014/513 | Batch 000/059 | Loss: 352.5588\n",
      "Epoch: 014/513 Train Loss: 299.7301\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 015/513 | Batch 000/059 | Loss: 276.4960\n",
      "Epoch: 015/513 Train Loss: 290.7482\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 016/513 | Batch 000/059 | Loss: 267.6829\n",
      "Epoch: 016/513 Train Loss: 294.6144\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 017/513 | Batch 000/059 | Loss: 296.6356\n",
      "Epoch: 017/513 Train Loss: 283.5651\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 018/513 | Batch 000/059 | Loss: 179.9017\n",
      "Epoch: 018/513 Train Loss: 279.0808\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 019/513 | Batch 000/059 | Loss: 368.3684\n",
      "Epoch: 019/513 Train Loss: 275.5030\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 020/513 | Batch 000/059 | Loss: 175.8335\n",
      "Epoch: 020/513 Train Loss: 273.0802\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 021/513 | Batch 000/059 | Loss: 160.7767\n",
      "Epoch: 021/513 Train Loss: 284.5248\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 022/513 | Batch 000/059 | Loss: 153.6468\n",
      "Epoch: 022/513 Train Loss: 264.3092\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 023/513 | Batch 000/059 | Loss: 271.3464\n",
      "Epoch: 023/513 Train Loss: 260.3536\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 024/513 | Batch 000/059 | Loss: 250.7177\n",
      "Epoch: 024/513 Train Loss: 259.3527\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 025/513 | Batch 000/059 | Loss: 235.1965\n",
      "Epoch: 025/513 Train Loss: 254.7828\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 026/513 | Batch 000/059 | Loss: 199.2975\n",
      "Epoch: 026/513 Train Loss: 252.8122\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 027/513 | Batch 000/059 | Loss: 351.8355\n",
      "Epoch: 027/513 Train Loss: 252.4685\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 028/513 | Batch 000/059 | Loss: 184.9487\n",
      "Epoch: 028/513 Train Loss: 248.2022\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 029/513 | Batch 000/059 | Loss: 242.6444\n",
      "Epoch: 029/513 Train Loss: 243.5350\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 030/513 | Batch 000/059 | Loss: 182.1844\n",
      "Epoch: 030/513 Train Loss: 258.2957\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 031/513 | Batch 000/059 | Loss: 311.1302\n",
      "Epoch: 031/513 Train Loss: 240.3491\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 032/513 | Batch 000/059 | Loss: 180.3238\n",
      "Epoch: 032/513 Train Loss: 238.8604\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 033/513 | Batch 000/059 | Loss: 212.8706\n",
      "Epoch: 033/513 Train Loss: 258.5271\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 034/513 | Batch 000/059 | Loss: 140.7785\n",
      "Epoch: 034/513 Train Loss: 241.7713\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 035/513 | Batch 000/059 | Loss: 206.1590\n",
      "Epoch: 035/513 Train Loss: 241.1676\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 036/513 | Batch 000/059 | Loss: 323.2202\n",
      "Epoch: 036/513 Train Loss: 239.0799\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 037/513 | Batch 000/059 | Loss: 236.5077\n",
      "Epoch: 037/513 Train Loss: 230.4047\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 038/513 | Batch 000/059 | Loss: 347.8155\n",
      "Epoch: 038/513 Train Loss: 230.0851\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 039/513 | Batch 000/059 | Loss: 329.6752\n",
      "Epoch: 039/513 Train Loss: 229.2020\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 040/513 | Batch 000/059 | Loss: 311.6015\n",
      "Epoch: 040/513 Train Loss: 234.6746\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 041/513 | Batch 000/059 | Loss: 235.9211\n",
      "Epoch: 041/513 Train Loss: 227.8836\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 042/513 | Batch 000/059 | Loss: 224.2254\n",
      "Epoch: 042/513 Train Loss: 226.4887\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 043/513 | Batch 000/059 | Loss: 188.7729\n",
      "Epoch: 043/513 Train Loss: 225.3794\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 044/513 | Batch 000/059 | Loss: 243.1846\n",
      "Epoch: 044/513 Train Loss: 232.4851\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 045/513 | Batch 000/059 | Loss: 253.7054\n",
      "Epoch: 045/513 Train Loss: 225.7967\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 046/513 | Batch 000/059 | Loss: 324.4210\n",
      "Epoch: 046/513 Train Loss: 225.4819\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 047/513 | Batch 000/059 | Loss: 182.1810\n",
      "Epoch: 047/513 Train Loss: 232.6215\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 048/513 | Batch 000/059 | Loss: 255.7309\n",
      "Epoch: 048/513 Train Loss: 246.4648\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 049/513 | Batch 000/059 | Loss: 354.7309\n",
      "Epoch: 049/513 Train Loss: 243.9680\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 050/513 | Batch 000/059 | Loss: 252.0382\n",
      "Epoch: 050/513 Train Loss: 224.7440\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 051/513 | Batch 000/059 | Loss: 156.9505\n",
      "Epoch: 051/513 Train Loss: 230.5563\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 052/513 | Batch 000/059 | Loss: 210.8498\n",
      "Epoch: 052/513 Train Loss: 225.8963\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 053/513 | Batch 000/059 | Loss: 143.9644\n",
      "Epoch: 053/513 Train Loss: 233.4898\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 054/513 | Batch 000/059 | Loss: 254.5771\n",
      "Epoch: 054/513 Train Loss: 222.5365\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 055/513 | Batch 000/059 | Loss: 202.5804\n",
      "Epoch: 055/513 Train Loss: 223.9643\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 056/513 | Batch 000/059 | Loss: 317.4529\n",
      "Epoch: 056/513 Train Loss: 221.1563\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 057/513 | Batch 000/059 | Loss: 288.7376\n",
      "Epoch: 057/513 Train Loss: 220.7979\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 058/513 | Batch 000/059 | Loss: 178.4553\n",
      "Epoch: 058/513 Train Loss: 236.3298\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 059/513 | Batch 000/059 | Loss: 181.5442\n",
      "Epoch: 059/513 Train Loss: 223.3335\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 060/513 | Batch 000/059 | Loss: 186.2276\n",
      "Epoch: 060/513 Train Loss: 219.0813\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 061/513 | Batch 000/059 | Loss: 352.2267\n",
      "Epoch: 061/513 Train Loss: 220.3434\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 062/513 | Batch 000/059 | Loss: 403.8883\n",
      "Epoch: 062/513 Train Loss: 223.2909\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 063/513 | Batch 000/059 | Loss: 297.4248\n",
      "Epoch: 063/513 Train Loss: 220.4208\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 064/513 | Batch 000/059 | Loss: 213.2330\n",
      "Epoch: 064/513 Train Loss: 220.5804\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 065/513 | Batch 000/059 | Loss: 378.9011\n",
      "Epoch: 065/513 Train Loss: 221.1737\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 066/513 | Batch 000/059 | Loss: 143.2434\n",
      "Epoch: 066/513 Train Loss: 229.2174\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 067/513 | Batch 000/059 | Loss: 137.5913\n",
      "Epoch: 067/513 Train Loss: 221.2201\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 068/513 | Batch 000/059 | Loss: 182.5342\n",
      "Epoch: 068/513 Train Loss: 220.8967\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 069/513 | Batch 000/059 | Loss: 123.0810\n",
      "Epoch: 069/513 Train Loss: 220.5418\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 070/513 | Batch 000/059 | Loss: 427.0071\n",
      "Epoch: 070/513 Train Loss: 241.5507\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 071/513 | Batch 000/059 | Loss: 303.7318\n",
      "Epoch: 071/513 Train Loss: 217.6502\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 072/513 | Batch 000/059 | Loss: 238.2985\n",
      "Epoch: 072/513 Train Loss: 220.6863\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 073/513 | Batch 000/059 | Loss: 227.0663\n",
      "Epoch: 073/513 Train Loss: 219.4364\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 074/513 | Batch 000/059 | Loss: 184.4069\n",
      "Epoch: 074/513 Train Loss: 223.2833\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 075/513 | Batch 000/059 | Loss: 234.7880\n",
      "Epoch: 075/513 Train Loss: 219.3963\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 076/513 | Batch 000/059 | Loss: 204.9003\n",
      "Epoch: 076/513 Train Loss: 222.4943\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 077/513 | Batch 000/059 | Loss: 313.4560\n",
      "Epoch: 077/513 Train Loss: 218.2342\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 078/513 | Batch 000/059 | Loss: 178.0955\n",
      "Epoch: 078/513 Train Loss: 218.8863\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 079/513 | Batch 000/059 | Loss: 252.4790\n",
      "Epoch: 079/513 Train Loss: 218.4296\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 080/513 | Batch 000/059 | Loss: 240.8864\n",
      "Epoch: 080/513 Train Loss: 218.5526\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 081/513 | Batch 000/059 | Loss: 246.5255\n",
      "Epoch: 081/513 Train Loss: 217.8100\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 082/513 | Batch 000/059 | Loss: 210.5134\n",
      "Epoch: 082/513 Train Loss: 220.0877\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 083/513 | Batch 000/059 | Loss: 225.3658\n",
      "Epoch: 083/513 Train Loss: 223.1811\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 084/513 | Batch 000/059 | Loss: 240.5517\n",
      "Epoch: 084/513 Train Loss: 216.6180\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 085/513 | Batch 000/059 | Loss: 167.1302\n",
      "Epoch: 085/513 Train Loss: 220.9156\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 086/513 | Batch 000/059 | Loss: 136.7139\n",
      "Epoch: 086/513 Train Loss: 215.7765\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 087/513 | Batch 000/059 | Loss: 220.7748\n",
      "Epoch: 087/513 Train Loss: 216.4597\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 088/513 | Batch 000/059 | Loss: 196.0440\n",
      "Epoch: 088/513 Train Loss: 219.3308\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 089/513 | Batch 000/059 | Loss: 209.0637\n",
      "Epoch: 089/513 Train Loss: 217.9037\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 090/513 | Batch 000/059 | Loss: 230.4200\n",
      "Epoch: 090/513 Train Loss: 223.1416\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 091/513 | Batch 000/059 | Loss: 156.6405\n",
      "Epoch: 091/513 Train Loss: 217.4562\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 092/513 | Batch 000/059 | Loss: 220.6158\n",
      "Epoch: 092/513 Train Loss: 244.1536\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 093/513 | Batch 000/059 | Loss: 280.5762\n",
      "Epoch: 093/513 Train Loss: 224.5267\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 094/513 | Batch 000/059 | Loss: 315.5999\n",
      "Epoch: 094/513 Train Loss: 221.7965\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 095/513 | Batch 000/059 | Loss: 144.7757\n",
      "Epoch: 095/513 Train Loss: 227.0688\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 096/513 | Batch 000/059 | Loss: 274.0036\n",
      "Epoch: 096/513 Train Loss: 220.5977\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 097/513 | Batch 000/059 | Loss: 184.4523\n",
      "Epoch: 097/513 Train Loss: 215.3359\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 098/513 | Batch 000/059 | Loss: 252.1004\n",
      "Epoch: 098/513 Train Loss: 217.0886\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 099/513 | Batch 000/059 | Loss: 202.7098\n",
      "Epoch: 099/513 Train Loss: 217.0222\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 100/513 | Batch 000/059 | Loss: 317.9149\n",
      "Epoch: 100/513 Train Loss: 215.6176\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 101/513 | Batch 000/059 | Loss: 209.1397\n",
      "Epoch: 101/513 Train Loss: 225.3195\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 102/513 | Batch 000/059 | Loss: 194.7854\n",
      "Epoch: 102/513 Train Loss: 217.2548\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 103/513 | Batch 000/059 | Loss: 272.3983\n",
      "Epoch: 103/513 Train Loss: 214.7203\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 104/513 | Batch 000/059 | Loss: 145.1780\n",
      "Epoch: 104/513 Train Loss: 216.6936\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 105/513 | Batch 000/059 | Loss: 227.7094\n",
      "Epoch: 105/513 Train Loss: 217.4796\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 106/513 | Batch 000/059 | Loss: 318.6205\n",
      "Epoch: 106/513 Train Loss: 222.8054\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 107/513 | Batch 000/059 | Loss: 248.4295\n",
      "Epoch: 107/513 Train Loss: 222.9549\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 108/513 | Batch 000/059 | Loss: 224.6518\n",
      "Epoch: 108/513 Train Loss: 220.2486\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 109/513 | Batch 000/059 | Loss: 253.8152\n",
      "Epoch: 109/513 Train Loss: 215.2364\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 110/513 | Batch 000/059 | Loss: 207.3224\n",
      "Epoch: 110/513 Train Loss: 216.0127\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 111/513 | Batch 000/059 | Loss: 253.2200\n",
      "Epoch: 111/513 Train Loss: 214.9670\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 112/513 | Batch 000/059 | Loss: 282.6691\n",
      "Epoch: 112/513 Train Loss: 216.8887\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 113/513 | Batch 000/059 | Loss: 199.1516\n",
      "Epoch: 113/513 Train Loss: 214.6633\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 114/513 | Batch 000/059 | Loss: 241.0272\n",
      "Epoch: 114/513 Train Loss: 215.1300\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 115/513 | Batch 000/059 | Loss: 206.9455\n",
      "Epoch: 115/513 Train Loss: 216.8237\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 116/513 | Batch 000/059 | Loss: 267.8434\n",
      "Epoch: 116/513 Train Loss: 214.4991\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 117/513 | Batch 000/059 | Loss: 174.5650\n",
      "Epoch: 117/513 Train Loss: 235.1943\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 118/513 | Batch 000/059 | Loss: 352.1341\n",
      "Epoch: 118/513 Train Loss: 215.6778\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 119/513 | Batch 000/059 | Loss: 300.6297\n",
      "Epoch: 119/513 Train Loss: 222.7582\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 120/513 | Batch 000/059 | Loss: 152.3992\n",
      "Epoch: 120/513 Train Loss: 215.5372\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 121/513 | Batch 000/059 | Loss: 250.7345\n",
      "Epoch: 121/513 Train Loss: 226.5351\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 122/513 | Batch 000/059 | Loss: 389.0578\n",
      "Epoch: 122/513 Train Loss: 225.9877\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 123/513 | Batch 000/059 | Loss: 239.3862\n",
      "Epoch: 123/513 Train Loss: 215.4132\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 124/513 | Batch 000/059 | Loss: 188.3721\n",
      "Epoch: 124/513 Train Loss: 236.6052\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 125/513 | Batch 000/059 | Loss: 176.4177\n",
      "Epoch: 125/513 Train Loss: 219.9318\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 126/513 | Batch 000/059 | Loss: 195.6310\n",
      "Epoch: 126/513 Train Loss: 216.4156\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 127/513 | Batch 000/059 | Loss: 135.3496\n",
      "Epoch: 127/513 Train Loss: 216.1173\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 128/513 | Batch 000/059 | Loss: 176.7426\n",
      "Epoch: 128/513 Train Loss: 215.8632\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 129/513 | Batch 000/059 | Loss: 256.5007\n",
      "Epoch: 129/513 Train Loss: 218.1727\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 130/513 | Batch 000/059 | Loss: 326.7136\n",
      "Epoch: 130/513 Train Loss: 224.6392\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 131/513 | Batch 000/059 | Loss: 241.7786\n",
      "Epoch: 131/513 Train Loss: 213.7873\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 132/513 | Batch 000/059 | Loss: 355.4545\n",
      "Epoch: 132/513 Train Loss: 219.0845\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 133/513 | Batch 000/059 | Loss: 172.9382\n",
      "Epoch: 133/513 Train Loss: 217.6515\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 134/513 | Batch 000/059 | Loss: 255.4847\n",
      "Epoch: 134/513 Train Loss: 213.6834\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 135/513 | Batch 000/059 | Loss: 259.4152\n",
      "Epoch: 135/513 Train Loss: 215.4414\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 136/513 | Batch 000/059 | Loss: 146.9310\n",
      "Epoch: 136/513 Train Loss: 219.2856\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 137/513 | Batch 000/059 | Loss: 319.5933\n",
      "Epoch: 137/513 Train Loss: 223.0662\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 138/513 | Batch 000/059 | Loss: 271.7882\n",
      "Epoch: 138/513 Train Loss: 214.0272\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 139/513 | Batch 000/059 | Loss: 194.4039\n",
      "Epoch: 139/513 Train Loss: 216.8530\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 140/513 | Batch 000/059 | Loss: 170.1851\n",
      "Epoch: 140/513 Train Loss: 222.3066\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 141/513 | Batch 000/059 | Loss: 181.3030\n",
      "Epoch: 141/513 Train Loss: 219.0793\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 142/513 | Batch 000/059 | Loss: 257.9382\n",
      "Epoch: 142/513 Train Loss: 214.1370\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 143/513 | Batch 000/059 | Loss: 281.3861\n",
      "Epoch: 143/513 Train Loss: 221.7082\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 144/513 | Batch 000/059 | Loss: 284.9181\n",
      "Epoch: 144/513 Train Loss: 213.1665\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 145/513 | Batch 000/059 | Loss: 122.9052\n",
      "Epoch: 145/513 Train Loss: 237.2597\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 146/513 | Batch 000/059 | Loss: 132.9300\n",
      "Epoch: 146/513 Train Loss: 214.0444\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 147/513 | Batch 000/059 | Loss: 217.0398\n",
      "Epoch: 147/513 Train Loss: 227.3995\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 148/513 | Batch 000/059 | Loss: 222.1819\n",
      "Epoch: 148/513 Train Loss: 228.0235\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 149/513 | Batch 000/059 | Loss: 289.0098\n",
      "Epoch: 149/513 Train Loss: 212.7890\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 150/513 | Batch 000/059 | Loss: 243.3389\n",
      "Epoch: 150/513 Train Loss: 229.1239\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 151/513 | Batch 000/059 | Loss: 239.6911\n",
      "Epoch: 151/513 Train Loss: 216.9559\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 152/513 | Batch 000/059 | Loss: 301.4810\n",
      "Epoch: 152/513 Train Loss: 219.8303\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 153/513 | Batch 000/059 | Loss: 165.3488\n",
      "Epoch: 153/513 Train Loss: 213.3457\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 154/513 | Batch 000/059 | Loss: 247.3393\n",
      "Epoch: 154/513 Train Loss: 215.8303\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 155/513 | Batch 000/059 | Loss: 243.2471\n",
      "Epoch: 155/513 Train Loss: 214.9547\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 156/513 | Batch 000/059 | Loss: 183.1355\n",
      "Epoch: 156/513 Train Loss: 222.3464\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 157/513 | Batch 000/059 | Loss: 316.6768\n",
      "Epoch: 157/513 Train Loss: 215.2639\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 158/513 | Batch 000/059 | Loss: 167.6114\n",
      "Epoch: 158/513 Train Loss: 213.5951\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 159/513 | Batch 000/059 | Loss: 243.5889\n",
      "Epoch: 159/513 Train Loss: 238.3424\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 160/513 | Batch 000/059 | Loss: 203.3852\n",
      "Epoch: 160/513 Train Loss: 216.3385\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 161/513 | Batch 000/059 | Loss: 268.1804\n",
      "Epoch: 161/513 Train Loss: 219.1225\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 162/513 | Batch 000/059 | Loss: 178.9660\n",
      "Epoch: 162/513 Train Loss: 213.6255\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 163/513 | Batch 000/059 | Loss: 188.9580\n",
      "Epoch: 163/513 Train Loss: 224.9660\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 164/513 | Batch 000/059 | Loss: 157.9582\n",
      "Epoch: 164/513 Train Loss: 213.0771\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 165/513 | Batch 000/059 | Loss: 317.9037\n",
      "Epoch: 165/513 Train Loss: 216.7193\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 166/513 | Batch 000/059 | Loss: 361.9663\n",
      "Epoch: 166/513 Train Loss: 220.9275\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 167/513 | Batch 000/059 | Loss: 216.5662\n",
      "Epoch: 167/513 Train Loss: 219.9835\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 168/513 | Batch 000/059 | Loss: 281.5085\n",
      "Epoch: 168/513 Train Loss: 213.3108\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 169/513 | Batch 000/059 | Loss: 283.4749\n",
      "Epoch: 169/513 Train Loss: 213.1857\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 170/513 | Batch 000/059 | Loss: 102.8279\n",
      "Epoch: 170/513 Train Loss: 212.6871\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 171/513 | Batch 000/059 | Loss: 245.8639\n",
      "Epoch: 171/513 Train Loss: 223.9468\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 172/513 | Batch 000/059 | Loss: 264.2741\n",
      "Epoch: 172/513 Train Loss: 215.5879\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 173/513 | Batch 000/059 | Loss: 324.8572\n",
      "Epoch: 173/513 Train Loss: 215.8541\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 174/513 | Batch 000/059 | Loss: 249.1961\n",
      "Epoch: 174/513 Train Loss: 247.6991\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 175/513 | Batch 000/059 | Loss: 125.9895\n",
      "Epoch: 175/513 Train Loss: 216.5694\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 176/513 | Batch 000/059 | Loss: 225.7478\n",
      "Epoch: 176/513 Train Loss: 213.3990\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 177/513 | Batch 000/059 | Loss: 181.2168\n",
      "Epoch: 177/513 Train Loss: 236.0724\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 178/513 | Batch 000/059 | Loss: 231.0025\n",
      "Epoch: 178/513 Train Loss: 216.3065\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 179/513 | Batch 000/059 | Loss: 227.3234\n",
      "Epoch: 179/513 Train Loss: 212.0528\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 180/513 | Batch 000/059 | Loss: 187.8850\n",
      "Epoch: 180/513 Train Loss: 221.8734\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 181/513 | Batch 000/059 | Loss: 196.3481\n",
      "Epoch: 181/513 Train Loss: 212.1060\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 182/513 | Batch 000/059 | Loss: 167.4855\n",
      "Epoch: 182/513 Train Loss: 218.2488\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 183/513 | Batch 000/059 | Loss: 304.5898\n",
      "Epoch: 183/513 Train Loss: 213.2616\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 184/513 | Batch 000/059 | Loss: 147.4646\n",
      "Epoch: 184/513 Train Loss: 231.3948\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 185/513 | Batch 000/059 | Loss: 209.3052\n",
      "Epoch: 185/513 Train Loss: 215.7860\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 186/513 | Batch 000/059 | Loss: 295.8527\n",
      "Epoch: 186/513 Train Loss: 212.0481\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 187/513 | Batch 000/059 | Loss: 161.2333\n",
      "Epoch: 187/513 Train Loss: 213.9603\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 188/513 | Batch 000/059 | Loss: 151.6662\n",
      "Epoch: 188/513 Train Loss: 212.1750\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 189/513 | Batch 000/059 | Loss: 233.8100\n",
      "Epoch: 189/513 Train Loss: 212.9535\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 190/513 | Batch 000/059 | Loss: 215.2128\n",
      "Epoch: 190/513 Train Loss: 216.7941\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 191/513 | Batch 000/059 | Loss: 282.7379\n",
      "Epoch: 191/513 Train Loss: 215.1638\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 192/513 | Batch 000/059 | Loss: 141.2386\n",
      "Epoch: 192/513 Train Loss: 215.8736\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 193/513 | Batch 000/059 | Loss: 237.8707\n",
      "Epoch: 193/513 Train Loss: 214.9084\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 194/513 | Batch 000/059 | Loss: 247.3107\n",
      "Epoch: 194/513 Train Loss: 212.1374\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 195/513 | Batch 000/059 | Loss: 276.7801\n",
      "Epoch: 195/513 Train Loss: 214.8407\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 196/513 | Batch 000/059 | Loss: 201.3002\n",
      "Epoch: 196/513 Train Loss: 213.5982\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 197/513 | Batch 000/059 | Loss: 234.4479\n",
      "Epoch: 197/513 Train Loss: 215.7919\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 198/513 | Batch 000/059 | Loss: 292.6542\n",
      "Epoch: 198/513 Train Loss: 215.0871\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 199/513 | Batch 000/059 | Loss: 187.3934\n",
      "Epoch: 199/513 Train Loss: 229.8772\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 200/513 | Batch 000/059 | Loss: 175.7211\n",
      "Epoch: 200/513 Train Loss: 213.0712\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 201/513 | Batch 000/059 | Loss: 139.9397\n",
      "Epoch: 201/513 Train Loss: 212.9403\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 202/513 | Batch 000/059 | Loss: 288.4685\n",
      "Epoch: 202/513 Train Loss: 218.5190\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 203/513 | Batch 000/059 | Loss: 258.5193\n",
      "Epoch: 203/513 Train Loss: 220.0942\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 204/513 | Batch 000/059 | Loss: 168.1430\n",
      "Epoch: 204/513 Train Loss: 216.3879\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 205/513 | Batch 000/059 | Loss: 169.6039\n",
      "Epoch: 205/513 Train Loss: 215.3830\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 206/513 | Batch 000/059 | Loss: 163.4541\n",
      "Epoch: 206/513 Train Loss: 224.9553\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 207/513 | Batch 000/059 | Loss: 154.3082\n",
      "Epoch: 207/513 Train Loss: 211.5671\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 208/513 | Batch 000/059 | Loss: 125.2805\n",
      "Epoch: 208/513 Train Loss: 223.3024\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 209/513 | Batch 000/059 | Loss: 142.8839\n",
      "Epoch: 209/513 Train Loss: 214.6231\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 210/513 | Batch 000/059 | Loss: 254.3533\n",
      "Epoch: 210/513 Train Loss: 215.5764\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 211/513 | Batch 000/059 | Loss: 307.2494\n",
      "Epoch: 211/513 Train Loss: 213.3509\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 212/513 | Batch 000/059 | Loss: 230.9207\n",
      "Epoch: 212/513 Train Loss: 214.4319\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 213/513 | Batch 000/059 | Loss: 283.6383\n",
      "Epoch: 213/513 Train Loss: 213.2782\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 214/513 | Batch 000/059 | Loss: 272.2628\n",
      "Epoch: 214/513 Train Loss: 211.8241\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 215/513 | Batch 000/059 | Loss: 196.1737\n",
      "Epoch: 215/513 Train Loss: 212.0632\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 216/513 | Batch 000/059 | Loss: 175.7785\n",
      "Epoch: 216/513 Train Loss: 222.2101\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 217/513 | Batch 000/059 | Loss: 200.7636\n",
      "Epoch: 217/513 Train Loss: 220.5307\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 218/513 | Batch 000/059 | Loss: 215.0135\n",
      "Epoch: 218/513 Train Loss: 222.9471\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 219/513 | Batch 000/059 | Loss: 119.6148\n",
      "Epoch: 219/513 Train Loss: 214.3209\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 220/513 | Batch 000/059 | Loss: 261.5797\n",
      "Epoch: 220/513 Train Loss: 213.2902\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 221/513 | Batch 000/059 | Loss: 160.8043\n",
      "Epoch: 221/513 Train Loss: 213.0616\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 222/513 | Batch 000/059 | Loss: 325.9198\n",
      "Epoch: 222/513 Train Loss: 213.7045\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 223/513 | Batch 000/059 | Loss: 258.5598\n",
      "Epoch: 223/513 Train Loss: 214.6174\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 224/513 | Batch 000/059 | Loss: 212.6717\n",
      "Epoch: 224/513 Train Loss: 213.4735\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 225/513 | Batch 000/059 | Loss: 107.0411\n",
      "Epoch: 225/513 Train Loss: 217.8796\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 226/513 | Batch 000/059 | Loss: 151.1215\n",
      "Epoch: 226/513 Train Loss: 212.1326\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 227/513 | Batch 000/059 | Loss: 155.8332\n",
      "Epoch: 227/513 Train Loss: 212.5966\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 228/513 | Batch 000/059 | Loss: 159.8203\n",
      "Epoch: 228/513 Train Loss: 215.8536\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 229/513 | Batch 000/059 | Loss: 269.4433\n",
      "Epoch: 229/513 Train Loss: 222.3682\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 230/513 | Batch 000/059 | Loss: 177.9516\n",
      "Epoch: 230/513 Train Loss: 211.7532\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 231/513 | Batch 000/059 | Loss: 196.7934\n",
      "Epoch: 231/513 Train Loss: 224.0517\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 232/513 | Batch 000/059 | Loss: 255.9531\n",
      "Epoch: 232/513 Train Loss: 215.6189\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 233/513 | Batch 000/059 | Loss: 295.4514\n",
      "Epoch: 233/513 Train Loss: 214.0880\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 234/513 | Batch 000/059 | Loss: 122.6607\n",
      "Epoch: 234/513 Train Loss: 214.4401\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 235/513 | Batch 000/059 | Loss: 186.6125\n",
      "Epoch: 235/513 Train Loss: 215.9313\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 236/513 | Batch 000/059 | Loss: 267.9158\n",
      "Epoch: 236/513 Train Loss: 226.0569\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 237/513 | Batch 000/059 | Loss: 145.0730\n",
      "Epoch: 237/513 Train Loss: 228.1990\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 238/513 | Batch 000/059 | Loss: 213.7735\n",
      "Epoch: 238/513 Train Loss: 233.3100\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 239/513 | Batch 000/059 | Loss: 264.8308\n",
      "Epoch: 239/513 Train Loss: 217.8747\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 240/513 | Batch 000/059 | Loss: 253.0099\n",
      "Epoch: 240/513 Train Loss: 222.0050\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 241/513 | Batch 000/059 | Loss: 217.4267\n",
      "Epoch: 241/513 Train Loss: 213.3409\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 242/513 | Batch 000/059 | Loss: 199.8964\n",
      "Epoch: 242/513 Train Loss: 214.7419\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 243/513 | Batch 000/059 | Loss: 162.7035\n",
      "Epoch: 243/513 Train Loss: 214.1235\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 244/513 | Batch 000/059 | Loss: 166.6013\n",
      "Epoch: 244/513 Train Loss: 214.4495\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 245/513 | Batch 000/059 | Loss: 172.7629\n",
      "Epoch: 245/513 Train Loss: 213.6828\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 246/513 | Batch 000/059 | Loss: 320.0794\n",
      "Epoch: 246/513 Train Loss: 212.4748\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 247/513 | Batch 000/059 | Loss: 362.9290\n",
      "Epoch: 247/513 Train Loss: 211.4299\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 248/513 | Batch 000/059 | Loss: 112.3141\n",
      "Epoch: 248/513 Train Loss: 214.2807\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 249/513 | Batch 000/059 | Loss: 237.6979\n",
      "Epoch: 249/513 Train Loss: 216.8255\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 250/513 | Batch 000/059 | Loss: 156.8107\n",
      "Epoch: 250/513 Train Loss: 214.8963\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 251/513 | Batch 000/059 | Loss: 133.0054\n",
      "Epoch: 251/513 Train Loss: 213.0671\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 252/513 | Batch 000/059 | Loss: 190.7760\n",
      "Epoch: 252/513 Train Loss: 230.5915\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 253/513 | Batch 000/059 | Loss: 337.9544\n",
      "Epoch: 253/513 Train Loss: 212.1020\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 254/513 | Batch 000/059 | Loss: 241.5735\n",
      "Epoch: 254/513 Train Loss: 211.8001\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 255/513 | Batch 000/059 | Loss: 165.1185\n",
      "Epoch: 255/513 Train Loss: 213.3231\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 256/513 | Batch 000/059 | Loss: 153.9127\n",
      "Epoch: 256/513 Train Loss: 217.0108\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 257/513 | Batch 000/059 | Loss: 223.2657\n",
      "Epoch: 257/513 Train Loss: 213.5770\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 258/513 | Batch 000/059 | Loss: 272.9625\n",
      "Epoch: 258/513 Train Loss: 211.9021\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 259/513 | Batch 000/059 | Loss: 207.2076\n",
      "Epoch: 259/513 Train Loss: 212.7900\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 260/513 | Batch 000/059 | Loss: 322.3058\n",
      "Epoch: 260/513 Train Loss: 223.6204\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 261/513 | Batch 000/059 | Loss: 226.9323\n",
      "Epoch: 261/513 Train Loss: 218.8349\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 262/513 | Batch 000/059 | Loss: 339.1214\n",
      "Epoch: 262/513 Train Loss: 216.3624\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 263/513 | Batch 000/059 | Loss: 228.6852\n",
      "Epoch: 263/513 Train Loss: 211.7259\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 264/513 | Batch 000/059 | Loss: 266.0519\n",
      "Epoch: 264/513 Train Loss: 211.2348\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 265/513 | Batch 000/059 | Loss: 149.7139\n",
      "Epoch: 265/513 Train Loss: 216.4171\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 266/513 | Batch 000/059 | Loss: 407.2904\n",
      "Epoch: 266/513 Train Loss: 212.4707\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 267/513 | Batch 000/059 | Loss: 173.4008\n",
      "Epoch: 267/513 Train Loss: 219.0924\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 268/513 | Batch 000/059 | Loss: 136.2348\n",
      "Epoch: 268/513 Train Loss: 217.6667\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 269/513 | Batch 000/059 | Loss: 242.6832\n",
      "Epoch: 269/513 Train Loss: 212.8970\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 270/513 | Batch 000/059 | Loss: 116.8470\n",
      "Epoch: 270/513 Train Loss: 212.0387\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 271/513 | Batch 000/059 | Loss: 291.5245\n",
      "Epoch: 271/513 Train Loss: 216.1794\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 272/513 | Batch 000/059 | Loss: 337.6332\n",
      "Epoch: 272/513 Train Loss: 213.9343\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 273/513 | Batch 000/059 | Loss: 294.3588\n",
      "Epoch: 273/513 Train Loss: 212.5423\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 274/513 | Batch 000/059 | Loss: 149.0932\n",
      "Epoch: 274/513 Train Loss: 215.3809\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 275/513 | Batch 000/059 | Loss: 243.2833\n",
      "Epoch: 275/513 Train Loss: 211.4430\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 276/513 | Batch 000/059 | Loss: 162.6734\n",
      "Epoch: 276/513 Train Loss: 212.3387\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 277/513 | Batch 000/059 | Loss: 272.4391\n",
      "Epoch: 277/513 Train Loss: 214.4628\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 278/513 | Batch 000/059 | Loss: 137.7694\n",
      "Epoch: 278/513 Train Loss: 212.3340\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 279/513 | Batch 000/059 | Loss: 283.4476\n",
      "Epoch: 279/513 Train Loss: 215.3719\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 280/513 | Batch 000/059 | Loss: 184.0868\n",
      "Epoch: 280/513 Train Loss: 212.9514\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 281/513 | Batch 000/059 | Loss: 278.3007\n",
      "Epoch: 281/513 Train Loss: 212.8150\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 282/513 | Batch 000/059 | Loss: 208.9637\n",
      "Epoch: 282/513 Train Loss: 211.2972\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 283/513 | Batch 000/059 | Loss: 185.2520\n",
      "Epoch: 283/513 Train Loss: 225.1536\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 284/513 | Batch 000/059 | Loss: 284.4428\n",
      "Epoch: 284/513 Train Loss: 212.5011\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 285/513 | Batch 000/059 | Loss: 203.6402\n",
      "Epoch: 285/513 Train Loss: 219.0208\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 286/513 | Batch 000/059 | Loss: 206.7483\n",
      "Epoch: 286/513 Train Loss: 222.6477\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 287/513 | Batch 000/059 | Loss: 334.8533\n",
      "Epoch: 287/513 Train Loss: 214.8698\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 288/513 | Batch 000/059 | Loss: 269.8143\n",
      "Epoch: 288/513 Train Loss: 219.3771\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 289/513 | Batch 000/059 | Loss: 273.9978\n",
      "Epoch: 289/513 Train Loss: 212.5052\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 290/513 | Batch 000/059 | Loss: 291.3621\n",
      "Epoch: 290/513 Train Loss: 213.0266\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 291/513 | Batch 000/059 | Loss: 103.7538\n",
      "Epoch: 291/513 Train Loss: 211.1420\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 292/513 | Batch 000/059 | Loss: 337.2526\n",
      "Epoch: 292/513 Train Loss: 211.0039\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 293/513 | Batch 000/059 | Loss: 230.5918\n",
      "Epoch: 293/513 Train Loss: 212.3808\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 294/513 | Batch 000/059 | Loss: 223.7182\n",
      "Epoch: 294/513 Train Loss: 211.0784\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 295/513 | Batch 000/059 | Loss: 270.5211\n",
      "Epoch: 295/513 Train Loss: 210.7909\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 296/513 | Batch 000/059 | Loss: 356.0850\n",
      "Epoch: 296/513 Train Loss: 213.4397\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 297/513 | Batch 000/059 | Loss: 200.9337\n",
      "Epoch: 297/513 Train Loss: 226.5430\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 298/513 | Batch 000/059 | Loss: 226.0425\n",
      "Epoch: 298/513 Train Loss: 214.8477\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 299/513 | Batch 000/059 | Loss: 196.6551\n",
      "Epoch: 299/513 Train Loss: 220.1989\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 300/513 | Batch 000/059 | Loss: 270.3143\n",
      "Epoch: 300/513 Train Loss: 212.1379\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 301/513 | Batch 000/059 | Loss: 215.2989\n",
      "Epoch: 301/513 Train Loss: 212.9099\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 302/513 | Batch 000/059 | Loss: 161.0605\n",
      "Epoch: 302/513 Train Loss: 223.0103\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 303/513 | Batch 000/059 | Loss: 213.8667\n",
      "Epoch: 303/513 Train Loss: 216.5487\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 304/513 | Batch 000/059 | Loss: 273.9724\n",
      "Epoch: 304/513 Train Loss: 211.1441\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 305/513 | Batch 000/059 | Loss: 137.3871\n",
      "Epoch: 305/513 Train Loss: 212.6800\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 306/513 | Batch 000/059 | Loss: 236.2995\n",
      "Epoch: 306/513 Train Loss: 215.6586\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 307/513 | Batch 000/059 | Loss: 144.7462\n",
      "Epoch: 307/513 Train Loss: 211.5567\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 308/513 | Batch 000/059 | Loss: 308.7779\n",
      "Epoch: 308/513 Train Loss: 211.1800\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 309/513 | Batch 000/059 | Loss: 174.3004\n",
      "Epoch: 309/513 Train Loss: 211.0341\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 310/513 | Batch 000/059 | Loss: 148.5210\n",
      "Epoch: 310/513 Train Loss: 211.5035\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 311/513 | Batch 000/059 | Loss: 138.5489\n",
      "Epoch: 311/513 Train Loss: 210.9474\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 312/513 | Batch 000/059 | Loss: 220.6484\n",
      "Epoch: 312/513 Train Loss: 218.7927\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 313/513 | Batch 000/059 | Loss: 542.6156\n",
      "Epoch: 313/513 Train Loss: 212.8016\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 314/513 | Batch 000/059 | Loss: 116.2255\n",
      "Epoch: 314/513 Train Loss: 211.7874\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 315/513 | Batch 000/059 | Loss: 235.0334\n",
      "Epoch: 315/513 Train Loss: 214.0821\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 316/513 | Batch 000/059 | Loss: 130.2569\n",
      "Epoch: 316/513 Train Loss: 222.5920\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 317/513 | Batch 000/059 | Loss: 232.0980\n",
      "Epoch: 317/513 Train Loss: 218.0726\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 318/513 | Batch 000/059 | Loss: 164.8669\n",
      "Epoch: 318/513 Train Loss: 214.6439\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 319/513 | Batch 000/059 | Loss: 139.7668\n",
      "Epoch: 319/513 Train Loss: 210.7377\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 320/513 | Batch 000/059 | Loss: 182.1159\n",
      "Epoch: 320/513 Train Loss: 213.0758\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 321/513 | Batch 000/059 | Loss: 159.6454\n",
      "Epoch: 321/513 Train Loss: 210.6203\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 322/513 | Batch 000/059 | Loss: 213.0756\n",
      "Epoch: 322/513 Train Loss: 209.8444\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 323/513 | Batch 000/059 | Loss: 248.5219\n",
      "Epoch: 323/513 Train Loss: 211.8884\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 324/513 | Batch 000/059 | Loss: 195.4986\n",
      "Epoch: 324/513 Train Loss: 241.7906\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 325/513 | Batch 000/059 | Loss: 193.2384\n",
      "Epoch: 325/513 Train Loss: 212.6075\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 326/513 | Batch 000/059 | Loss: 180.2056\n",
      "Epoch: 326/513 Train Loss: 213.2517\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 327/513 | Batch 000/059 | Loss: 138.1757\n",
      "Epoch: 327/513 Train Loss: 210.7446\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 328/513 | Batch 000/059 | Loss: 217.2349\n",
      "Epoch: 328/513 Train Loss: 213.5912\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 329/513 | Batch 000/059 | Loss: 281.4863\n",
      "Epoch: 329/513 Train Loss: 210.8444\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 330/513 | Batch 000/059 | Loss: 175.1005\n",
      "Epoch: 330/513 Train Loss: 211.8888\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 331/513 | Batch 000/059 | Loss: 240.0508\n",
      "Epoch: 331/513 Train Loss: 211.5111\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 332/513 | Batch 000/059 | Loss: 241.7921\n",
      "Epoch: 332/513 Train Loss: 220.7345\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 333/513 | Batch 000/059 | Loss: 381.9888\n",
      "Epoch: 333/513 Train Loss: 214.2693\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 334/513 | Batch 000/059 | Loss: 278.8047\n",
      "Epoch: 334/513 Train Loss: 211.9766\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 335/513 | Batch 000/059 | Loss: 114.3872\n",
      "Epoch: 335/513 Train Loss: 216.2517\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 336/513 | Batch 000/059 | Loss: 188.8023\n",
      "Epoch: 336/513 Train Loss: 211.2834\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 337/513 | Batch 000/059 | Loss: 224.0809\n",
      "Epoch: 337/513 Train Loss: 211.0547\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 338/513 | Batch 000/059 | Loss: 264.7979\n",
      "Epoch: 338/513 Train Loss: 213.7484\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 339/513 | Batch 000/059 | Loss: 245.3251\n",
      "Epoch: 339/513 Train Loss: 218.5239\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 340/513 | Batch 000/059 | Loss: 236.7012\n",
      "Epoch: 340/513 Train Loss: 213.8546\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 341/513 | Batch 000/059 | Loss: 207.6401\n",
      "Epoch: 341/513 Train Loss: 214.6793\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 342/513 | Batch 000/059 | Loss: 224.8827\n",
      "Epoch: 342/513 Train Loss: 216.9822\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 343/513 | Batch 000/059 | Loss: 192.7880\n",
      "Epoch: 343/513 Train Loss: 213.0346\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 344/513 | Batch 000/059 | Loss: 205.6631\n",
      "Epoch: 344/513 Train Loss: 211.7382\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 345/513 | Batch 000/059 | Loss: 240.8836\n",
      "Epoch: 345/513 Train Loss: 211.9648\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 346/513 | Batch 000/059 | Loss: 230.6786\n",
      "Epoch: 346/513 Train Loss: 215.4514\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 347/513 | Batch 000/059 | Loss: 108.9278\n",
      "Epoch: 347/513 Train Loss: 210.2878\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 348/513 | Batch 000/059 | Loss: 242.2221\n",
      "Epoch: 348/513 Train Loss: 223.0074\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 349/513 | Batch 000/059 | Loss: 261.3104\n",
      "Epoch: 349/513 Train Loss: 212.4826\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 350/513 | Batch 000/059 | Loss: 171.4677\n",
      "Epoch: 350/513 Train Loss: 211.2549\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 351/513 | Batch 000/059 | Loss: 219.3937\n",
      "Epoch: 351/513 Train Loss: 210.7123\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 352/513 | Batch 000/059 | Loss: 263.6212\n",
      "Epoch: 352/513 Train Loss: 213.1484\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 353/513 | Batch 000/059 | Loss: 165.9210\n",
      "Epoch: 353/513 Train Loss: 216.7347\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 354/513 | Batch 000/059 | Loss: 204.6179\n",
      "Epoch: 354/513 Train Loss: 217.3399\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 355/513 | Batch 000/059 | Loss: 222.4232\n",
      "Epoch: 355/513 Train Loss: 211.0663\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 356/513 | Batch 000/059 | Loss: 128.2291\n",
      "Epoch: 356/513 Train Loss: 210.6738\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 357/513 | Batch 000/059 | Loss: 198.2549\n",
      "Epoch: 357/513 Train Loss: 211.7131\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 358/513 | Batch 000/059 | Loss: 157.4856\n",
      "Epoch: 358/513 Train Loss: 210.0095\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 359/513 | Batch 000/059 | Loss: 206.4068\n",
      "Epoch: 359/513 Train Loss: 213.1720\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 360/513 | Batch 000/059 | Loss: 140.5869\n",
      "Epoch: 360/513 Train Loss: 212.2505\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 361/513 | Batch 000/059 | Loss: 219.1717\n",
      "Epoch: 361/513 Train Loss: 212.0474\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 362/513 | Batch 000/059 | Loss: 135.5642\n",
      "Epoch: 362/513 Train Loss: 212.6716\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 363/513 | Batch 000/059 | Loss: 159.0513\n",
      "Epoch: 363/513 Train Loss: 212.4247\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 364/513 | Batch 000/059 | Loss: 345.5992\n",
      "Epoch: 364/513 Train Loss: 215.4143\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 365/513 | Batch 000/059 | Loss: 162.6600\n",
      "Epoch: 365/513 Train Loss: 213.1122\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 366/513 | Batch 000/059 | Loss: 377.2897\n",
      "Epoch: 366/513 Train Loss: 210.9351\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 367/513 | Batch 000/059 | Loss: 198.2863\n",
      "Epoch: 367/513 Train Loss: 212.3095\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 368/513 | Batch 000/059 | Loss: 221.5771\n",
      "Epoch: 368/513 Train Loss: 214.0685\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 369/513 | Batch 000/059 | Loss: 252.9010\n",
      "Epoch: 369/513 Train Loss: 210.8846\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 370/513 | Batch 000/059 | Loss: 291.0035\n",
      "Epoch: 370/513 Train Loss: 220.4224\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 371/513 | Batch 000/059 | Loss: 128.1331\n",
      "Epoch: 371/513 Train Loss: 212.7560\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 372/513 | Batch 000/059 | Loss: 263.0457\n",
      "Epoch: 372/513 Train Loss: 211.0631\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 373/513 | Batch 000/059 | Loss: 251.7177\n",
      "Epoch: 373/513 Train Loss: 222.7778\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 374/513 | Batch 000/059 | Loss: 228.5409\n",
      "Epoch: 374/513 Train Loss: 211.3481\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 375/513 | Batch 000/059 | Loss: 244.8730\n",
      "Epoch: 375/513 Train Loss: 209.8934\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 376/513 | Batch 000/059 | Loss: 179.5645\n",
      "Epoch: 376/513 Train Loss: 211.6633\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 377/513 | Batch 000/059 | Loss: 295.9077\n",
      "Epoch: 377/513 Train Loss: 210.5620\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 378/513 | Batch 000/059 | Loss: 162.1398\n",
      "Epoch: 378/513 Train Loss: 210.8286\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 379/513 | Batch 000/059 | Loss: 216.6002\n",
      "Epoch: 379/513 Train Loss: 211.4139\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 380/513 | Batch 000/059 | Loss: 171.9356\n",
      "Epoch: 380/513 Train Loss: 209.9792\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 381/513 | Batch 000/059 | Loss: 187.5908\n",
      "Epoch: 381/513 Train Loss: 214.8448\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 382/513 | Batch 000/059 | Loss: 238.3403\n",
      "Epoch: 382/513 Train Loss: 231.3350\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 383/513 | Batch 000/059 | Loss: 290.6841\n",
      "Epoch: 383/513 Train Loss: 215.8059\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 384/513 | Batch 000/059 | Loss: 263.2548\n",
      "Epoch: 384/513 Train Loss: 216.4341\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 385/513 | Batch 000/059 | Loss: 198.7405\n",
      "Epoch: 385/513 Train Loss: 212.2962\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 386/513 | Batch 000/059 | Loss: 256.5354\n",
      "Epoch: 386/513 Train Loss: 210.2409\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 387/513 | Batch 000/059 | Loss: 251.7744\n",
      "Epoch: 387/513 Train Loss: 210.0845\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 388/513 | Batch 000/059 | Loss: 105.3705\n",
      "Epoch: 388/513 Train Loss: 214.3618\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 389/513 | Batch 000/059 | Loss: 257.0409\n",
      "Epoch: 389/513 Train Loss: 211.5451\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 390/513 | Batch 000/059 | Loss: 164.7837\n",
      "Epoch: 390/513 Train Loss: 210.8945\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 391/513 | Batch 000/059 | Loss: 191.4641\n",
      "Epoch: 391/513 Train Loss: 211.5528\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 392/513 | Batch 000/059 | Loss: 112.0472\n",
      "Epoch: 392/513 Train Loss: 210.6038\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 393/513 | Batch 000/059 | Loss: 122.9948\n",
      "Epoch: 393/513 Train Loss: 211.7320\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 394/513 | Batch 000/059 | Loss: 306.4380\n",
      "Epoch: 394/513 Train Loss: 210.1428\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 395/513 | Batch 000/059 | Loss: 194.5121\n",
      "Epoch: 395/513 Train Loss: 210.6318\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 396/513 | Batch 000/059 | Loss: 166.2310\n",
      "Epoch: 396/513 Train Loss: 216.6539\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 397/513 | Batch 000/059 | Loss: 211.2343\n",
      "Epoch: 397/513 Train Loss: 236.0631\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 398/513 | Batch 000/059 | Loss: 245.5139\n",
      "Epoch: 398/513 Train Loss: 212.0922\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 399/513 | Batch 000/059 | Loss: 224.8353\n",
      "Epoch: 399/513 Train Loss: 215.6320\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 400/513 | Batch 000/059 | Loss: 174.5544\n",
      "Epoch: 400/513 Train Loss: 211.0059\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 401/513 | Batch 000/059 | Loss: 234.2640\n",
      "Epoch: 401/513 Train Loss: 213.0514\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 402/513 | Batch 000/059 | Loss: 85.9971\n",
      "Epoch: 402/513 Train Loss: 210.8058\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 403/513 | Batch 000/059 | Loss: 260.5905\n",
      "Epoch: 403/513 Train Loss: 210.3102\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 404/513 | Batch 000/059 | Loss: 169.1304\n",
      "Epoch: 404/513 Train Loss: 216.3388\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 405/513 | Batch 000/059 | Loss: 257.9770\n",
      "Epoch: 405/513 Train Loss: 219.5651\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 406/513 | Batch 000/059 | Loss: 333.5075\n",
      "Epoch: 406/513 Train Loss: 209.6780\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 407/513 | Batch 000/059 | Loss: 309.8538\n",
      "Epoch: 407/513 Train Loss: 218.3207\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 408/513 | Batch 000/059 | Loss: 192.2755\n",
      "Epoch: 408/513 Train Loss: 219.5284\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 409/513 | Batch 000/059 | Loss: 175.1505\n",
      "Epoch: 409/513 Train Loss: 211.7989\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 410/513 | Batch 000/059 | Loss: 326.6045\n",
      "Epoch: 410/513 Train Loss: 209.6616\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 411/513 | Batch 000/059 | Loss: 246.0068\n",
      "Epoch: 411/513 Train Loss: 210.4731\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 412/513 | Batch 000/059 | Loss: 214.8275\n",
      "Epoch: 412/513 Train Loss: 212.3835\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 413/513 | Batch 000/059 | Loss: 269.8816\n",
      "Epoch: 413/513 Train Loss: 218.2428\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 414/513 | Batch 000/059 | Loss: 230.8326\n",
      "Epoch: 414/513 Train Loss: 215.1247\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 415/513 | Batch 000/059 | Loss: 116.6506\n",
      "Epoch: 415/513 Train Loss: 209.7045\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 416/513 | Batch 000/059 | Loss: 207.1869\n",
      "Epoch: 416/513 Train Loss: 213.0702\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 417/513 | Batch 000/059 | Loss: 179.9795\n",
      "Epoch: 417/513 Train Loss: 212.7339\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 418/513 | Batch 000/059 | Loss: 207.4410\n",
      "Epoch: 418/513 Train Loss: 214.4320\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 419/513 | Batch 000/059 | Loss: 296.8301\n",
      "Epoch: 419/513 Train Loss: 215.4034\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 420/513 | Batch 000/059 | Loss: 194.3074\n",
      "Epoch: 420/513 Train Loss: 213.0771\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 421/513 | Batch 000/059 | Loss: 291.6101\n",
      "Epoch: 421/513 Train Loss: 210.1040\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 422/513 | Batch 000/059 | Loss: 317.9547\n",
      "Epoch: 422/513 Train Loss: 210.9574\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 423/513 | Batch 000/059 | Loss: 230.0929\n",
      "Epoch: 423/513 Train Loss: 211.0132\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 424/513 | Batch 000/059 | Loss: 177.1921\n",
      "Epoch: 424/513 Train Loss: 210.2907\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 425/513 | Batch 000/059 | Loss: 424.3597\n",
      "Epoch: 425/513 Train Loss: 219.0251\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 426/513 | Batch 000/059 | Loss: 197.2216\n",
      "Epoch: 426/513 Train Loss: 210.6005\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 427/513 | Batch 000/059 | Loss: 208.3026\n",
      "Epoch: 427/513 Train Loss: 210.0617\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 428/513 | Batch 000/059 | Loss: 287.1057\n",
      "Epoch: 428/513 Train Loss: 209.3440\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 429/513 | Batch 000/059 | Loss: 230.6335\n",
      "Epoch: 429/513 Train Loss: 209.9650\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 430/513 | Batch 000/059 | Loss: 272.7708\n",
      "Epoch: 430/513 Train Loss: 212.7704\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 431/513 | Batch 000/059 | Loss: 90.7294\n",
      "Epoch: 431/513 Train Loss: 210.2075\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 432/513 | Batch 000/059 | Loss: 138.6195\n",
      "Epoch: 432/513 Train Loss: 209.9854\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 433/513 | Batch 000/059 | Loss: 245.2157\n",
      "Epoch: 433/513 Train Loss: 211.0106\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 434/513 | Batch 000/059 | Loss: 277.3206\n",
      "Epoch: 434/513 Train Loss: 213.9801\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 435/513 | Batch 000/059 | Loss: 189.1849\n",
      "Epoch: 435/513 Train Loss: 211.2684\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 436/513 | Batch 000/059 | Loss: 394.7490\n",
      "Epoch: 436/513 Train Loss: 213.8837\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 437/513 | Batch 000/059 | Loss: 243.1994\n",
      "Epoch: 437/513 Train Loss: 210.2587\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 438/513 | Batch 000/059 | Loss: 244.8464\n",
      "Epoch: 438/513 Train Loss: 215.2972\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 439/513 | Batch 000/059 | Loss: 262.3334\n",
      "Epoch: 439/513 Train Loss: 211.0580\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 440/513 | Batch 000/059 | Loss: 219.4128\n",
      "Epoch: 440/513 Train Loss: 209.1760\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 441/513 | Batch 000/059 | Loss: 265.7478\n",
      "Epoch: 441/513 Train Loss: 210.0717\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 442/513 | Batch 000/059 | Loss: 187.3491\n",
      "Epoch: 442/513 Train Loss: 210.8910\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 443/513 | Batch 000/059 | Loss: 192.1082\n",
      "Epoch: 443/513 Train Loss: 210.7502\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 444/513 | Batch 000/059 | Loss: 252.5930\n",
      "Epoch: 444/513 Train Loss: 211.1125\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 445/513 | Batch 000/059 | Loss: 201.8280\n",
      "Epoch: 445/513 Train Loss: 212.8275\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 446/513 | Batch 000/059 | Loss: 212.2251\n",
      "Epoch: 446/513 Train Loss: 212.1563\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 447/513 | Batch 000/059 | Loss: 295.3357\n",
      "Epoch: 447/513 Train Loss: 210.0942\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 448/513 | Batch 000/059 | Loss: 162.8888\n",
      "Epoch: 448/513 Train Loss: 220.5221\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 449/513 | Batch 000/059 | Loss: 260.0752\n",
      "Epoch: 449/513 Train Loss: 211.1021\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 450/513 | Batch 000/059 | Loss: 168.3015\n",
      "Epoch: 450/513 Train Loss: 209.1321\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 451/513 | Batch 000/059 | Loss: 153.3331\n",
      "Epoch: 451/513 Train Loss: 210.5601\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 452/513 | Batch 000/059 | Loss: 228.5307\n",
      "Epoch: 452/513 Train Loss: 210.8729\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 453/513 | Batch 000/059 | Loss: 238.9950\n",
      "Epoch: 453/513 Train Loss: 209.4955\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 454/513 | Batch 000/059 | Loss: 181.0480\n",
      "Epoch: 454/513 Train Loss: 213.7046\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 455/513 | Batch 000/059 | Loss: 363.6577\n",
      "Epoch: 455/513 Train Loss: 214.7296\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 456/513 | Batch 000/059 | Loss: 230.7267\n",
      "Epoch: 456/513 Train Loss: 216.0976\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 457/513 | Batch 000/059 | Loss: 248.0568\n",
      "Epoch: 457/513 Train Loss: 210.3893\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 458/513 | Batch 000/059 | Loss: 311.9119\n",
      "Epoch: 458/513 Train Loss: 209.5145\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 459/513 | Batch 000/059 | Loss: 303.3504\n",
      "Epoch: 459/513 Train Loss: 214.8163\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 460/513 | Batch 000/059 | Loss: 123.0092\n",
      "Epoch: 460/513 Train Loss: 214.8329\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 461/513 | Batch 000/059 | Loss: 368.8091\n",
      "Epoch: 461/513 Train Loss: 210.2008\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 462/513 | Batch 000/059 | Loss: 146.8924\n",
      "Epoch: 462/513 Train Loss: 210.9734\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 463/513 | Batch 000/059 | Loss: 209.4090\n",
      "Epoch: 463/513 Train Loss: 215.7124\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 464/513 | Batch 000/059 | Loss: 169.4089\n",
      "Epoch: 464/513 Train Loss: 218.1597\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 465/513 | Batch 000/059 | Loss: 189.2175\n",
      "Epoch: 465/513 Train Loss: 227.1954\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 466/513 | Batch 000/059 | Loss: 101.0751\n",
      "Epoch: 466/513 Train Loss: 210.8260\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 467/513 | Batch 000/059 | Loss: 124.5778\n",
      "Epoch: 467/513 Train Loss: 211.1388\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 468/513 | Batch 000/059 | Loss: 291.8126\n",
      "Epoch: 468/513 Train Loss: 210.9099\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 469/513 | Batch 000/059 | Loss: 384.5797\n",
      "Epoch: 469/513 Train Loss: 224.4854\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 470/513 | Batch 000/059 | Loss: 259.7925\n",
      "Epoch: 470/513 Train Loss: 210.2389\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 471/513 | Batch 000/059 | Loss: 142.8568\n",
      "Epoch: 471/513 Train Loss: 220.4832\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 472/513 | Batch 000/059 | Loss: 195.9190\n",
      "Epoch: 472/513 Train Loss: 211.5690\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 473/513 | Batch 000/059 | Loss: 181.2758\n",
      "Epoch: 473/513 Train Loss: 209.9560\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 474/513 | Batch 000/059 | Loss: 137.9262\n",
      "Epoch: 474/513 Train Loss: 212.5481\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 475/513 | Batch 000/059 | Loss: 396.2311\n",
      "Epoch: 475/513 Train Loss: 211.0112\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 476/513 | Batch 000/059 | Loss: 163.6260\n",
      "Epoch: 476/513 Train Loss: 210.1082\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 477/513 | Batch 000/059 | Loss: 267.0150\n",
      "Epoch: 477/513 Train Loss: 213.3284\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 478/513 | Batch 000/059 | Loss: 181.0750\n",
      "Epoch: 478/513 Train Loss: 210.4175\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 479/513 | Batch 000/059 | Loss: 178.6033\n",
      "Epoch: 479/513 Train Loss: 211.9338\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 480/513 | Batch 000/059 | Loss: 233.9231\n",
      "Epoch: 480/513 Train Loss: 210.4732\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 481/513 | Batch 000/059 | Loss: 232.9232\n",
      "Epoch: 481/513 Train Loss: 212.4520\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 482/513 | Batch 000/059 | Loss: 350.9485\n",
      "Epoch: 482/513 Train Loss: 210.4788\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 483/513 | Batch 000/059 | Loss: 115.8985\n",
      "Epoch: 483/513 Train Loss: 210.2348\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 484/513 | Batch 000/059 | Loss: 368.3478\n",
      "Epoch: 484/513 Train Loss: 209.7499\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 485/513 | Batch 000/059 | Loss: 116.3502\n",
      "Epoch: 485/513 Train Loss: 209.4937\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 486/513 | Batch 000/059 | Loss: 293.5349\n",
      "Epoch: 486/513 Train Loss: 209.3720\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 487/513 | Batch 000/059 | Loss: 256.7178\n",
      "Epoch: 487/513 Train Loss: 215.6820\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 488/513 | Batch 000/059 | Loss: 221.8440\n",
      "Epoch: 488/513 Train Loss: 209.7066\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 489/513 | Batch 000/059 | Loss: 156.2834\n",
      "Epoch: 489/513 Train Loss: 210.9287\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 490/513 | Batch 000/059 | Loss: 300.8305\n",
      "Epoch: 490/513 Train Loss: 210.4656\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 491/513 | Batch 000/059 | Loss: 154.9333\n",
      "Epoch: 491/513 Train Loss: 211.8295\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 492/513 | Batch 000/059 | Loss: 338.2863\n",
      "Epoch: 492/513 Train Loss: 211.8841\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 493/513 | Batch 000/059 | Loss: 321.7035\n",
      "Epoch: 493/513 Train Loss: 212.7097\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 494/513 | Batch 000/059 | Loss: 192.7335\n",
      "Epoch: 494/513 Train Loss: 211.4203\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 495/513 | Batch 000/059 | Loss: 156.3987\n",
      "Epoch: 495/513 Train Loss: 209.8269\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 496/513 | Batch 000/059 | Loss: 192.1049\n",
      "Epoch: 496/513 Train Loss: 210.3826\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 497/513 | Batch 000/059 | Loss: 201.9060\n",
      "Epoch: 497/513 Train Loss: 214.3308\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 498/513 | Batch 000/059 | Loss: 250.7077\n",
      "Epoch: 498/513 Train Loss: 212.3320\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 499/513 | Batch 000/059 | Loss: 107.6112\n",
      "Epoch: 499/513 Train Loss: 211.3149\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 500/513 | Batch 000/059 | Loss: 196.0590\n",
      "Epoch: 500/513 Train Loss: 210.3665\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 501/513 | Batch 000/059 | Loss: 222.8293\n",
      "Epoch: 501/513 Train Loss: 226.0245\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 502/513 | Batch 000/059 | Loss: 235.3882\n",
      "Epoch: 502/513 Train Loss: 212.4927\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 503/513 | Batch 000/059 | Loss: 244.2212\n",
      "Epoch: 503/513 Train Loss: 219.3970\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 504/513 | Batch 000/059 | Loss: 91.7460\n",
      "Epoch: 504/513 Train Loss: 210.5161\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 505/513 | Batch 000/059 | Loss: 200.1858\n",
      "Epoch: 505/513 Train Loss: 217.2804\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 506/513 | Batch 000/059 | Loss: 125.9984\n",
      "Epoch: 506/513 Train Loss: 210.5354\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 507/513 | Batch 000/059 | Loss: 234.5815\n",
      "Epoch: 507/513 Train Loss: 211.3758\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 508/513 | Batch 000/059 | Loss: 191.5641\n",
      "Epoch: 508/513 Train Loss: 219.2939\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 509/513 | Batch 000/059 | Loss: 176.5854\n",
      "Epoch: 509/513 Train Loss: 214.0765\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 510/513 | Batch 000/059 | Loss: 145.7691\n",
      "Epoch: 510/513 Train Loss: 216.1484\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 511/513 | Batch 000/059 | Loss: 333.7073\n",
      "Epoch: 511/513 Train Loss: 214.6087\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 512/513 | Batch 000/059 | Loss: 244.1459\n",
      "Epoch: 512/513 Train Loss: 210.9456\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 513/513 | Batch 000/059 | Loss: 220.4811\n",
      "Epoch: 513/513 Train Loss: 215.3042\n",
      "Time elapsed: 1.17 min\n",
      "Total Training Time: 1.17 min\n",
      "Training Loss: 215.30\n",
      "Test Loss: 228.44\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "minibatch_cost = []\n",
    "epoch_cost = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(device)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        outputs = model(features)\n",
    "        \n",
    "        cost = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        minibatch_cost.append(cost)\n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 1000:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "       \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        cost = compute_loss(model, train_loader)\n",
    "        epoch_cost.append(cost)\n",
    "        print('Epoch: %03d/%03d Train Loss: %.4f' % (\n",
    "                epoch+1, num_epochs, cost))\n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "        \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "    print('Training Loss: %.2f' % compute_loss(model, train_loader))\n",
    "    print('Test Loss: %.2f' % compute_loss(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "bziYXC8wpmAx",
    "outputId": "58a11135-fe10-48d2-b065-153707180267"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the loss curve\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(len(epoch_cost)), epoch_cost)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross entropy loss\")\n",
    "plt.title(\"Loss of training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbF138UBO9oU"
   },
   "source": [
    "## 4. Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdCGZbITp-Mb"
   },
   "source": [
    "### Model B: 1 Hidden Layer Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03R0J1RNp-Mb"
   },
   "source": [
    "### Create model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rbmkU4wnp-Mb",
    "outputId": "6398f355-964c-418f-e4cd-ec2c0e10abf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "n_iters = 30000\n",
    "num_epochs = n_iters / (len(train_data) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "print(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "id": "_uKooGeUp-Mb"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # create your dataloader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "id": "KDREFEPnp-Mc"
   },
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, output_dim):\n",
    "        super(FNN, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear4 = nn.Linear(hidden_dim1, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.linear4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "id": "6buHUoURp-Mc"
   },
   "outputs": [],
   "source": [
    "input_dim = tensor_x.shape[1]\n",
    "output_dim = 1\n",
    "hidden_dim1 = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "id": "jLseDEaxp-Mc"
   },
   "outputs": [],
   "source": [
    "model = FNN(input_dim, hidden_dim1, output_dim)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6s_tUHiop-Mc",
    "outputId": "d610648c-a79e-4e70-dab5-a3377211af58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 256]           2,816\n",
      "           Dropout-2               [-1, 1, 256]               0\n",
      "              ReLU-3               [-1, 1, 256]               0\n",
      "            Linear-4                 [-1, 1, 1]             257\n",
      "================================================================\n",
      "Total params: 3,073\n",
      "Trainable params: 3,073\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1,tensor_x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "id": "X2NgyokHp-Mc"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 0.0001\n",
    "LAMBDA = 0.05\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=LAMBDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2owrxlaip-Mc"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RbSJrysWp-Mc",
    "outputId": "d19bb7c3-76dd-4f78-f66f-2d8b33b59e65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/513 | Batch 000/059 | Loss: 6399.1191\n",
      "Epoch: 001/513 Train Loss: 6847.7333\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 002/513 | Batch 000/059 | Loss: 6948.9365\n",
      "Epoch: 002/513 Train Loss: 6557.6546\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 003/513 | Batch 000/059 | Loss: 6540.7295\n",
      "Epoch: 003/513 Train Loss: 6238.4655\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 004/513 | Batch 000/059 | Loss: 7252.2598\n",
      "Epoch: 004/513 Train Loss: 5877.2511\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 005/513 | Batch 000/059 | Loss: 5177.0259\n",
      "Epoch: 005/513 Train Loss: 5478.4537\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 006/513 | Batch 000/059 | Loss: 4723.2324\n",
      "Epoch: 006/513 Train Loss: 5038.7408\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 007/513 | Batch 000/059 | Loss: 5267.1650\n",
      "Epoch: 007/513 Train Loss: 4565.6131\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 008/513 | Batch 000/059 | Loss: 5271.6851\n",
      "Epoch: 008/513 Train Loss: 4066.6525\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 009/513 | Batch 000/059 | Loss: 4287.1353\n",
      "Epoch: 009/513 Train Loss: 3557.6034\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 010/513 | Batch 000/059 | Loss: 4332.2783\n",
      "Epoch: 010/513 Train Loss: 3049.8015\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 011/513 | Batch 000/059 | Loss: 4043.5564\n",
      "Epoch: 011/513 Train Loss: 2560.4809\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 012/513 | Batch 000/059 | Loss: 2418.1094\n",
      "Epoch: 012/513 Train Loss: 2108.4815\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 013/513 | Batch 000/059 | Loss: 1915.2037\n",
      "Epoch: 013/513 Train Loss: 1707.6863\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 014/513 | Batch 000/059 | Loss: 1823.9633\n",
      "Epoch: 014/513 Train Loss: 1366.6370\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 015/513 | Batch 000/059 | Loss: 1252.8038\n",
      "Epoch: 015/513 Train Loss: 1086.5766\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 016/513 | Batch 000/059 | Loss: 1224.6211\n",
      "Epoch: 016/513 Train Loss: 867.8730\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 017/513 | Batch 000/059 | Loss: 892.2080\n",
      "Epoch: 017/513 Train Loss: 704.6342\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 018/513 | Batch 000/059 | Loss: 1033.2081\n",
      "Epoch: 018/513 Train Loss: 590.2691\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 019/513 | Batch 000/059 | Loss: 421.2669\n",
      "Epoch: 019/513 Train Loss: 512.3748\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 020/513 | Batch 000/059 | Loss: 463.3580\n",
      "Epoch: 020/513 Train Loss: 462.0432\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 021/513 | Batch 000/059 | Loss: 626.5422\n",
      "Epoch: 021/513 Train Loss: 429.6927\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 022/513 | Batch 000/059 | Loss: 338.5481\n",
      "Epoch: 022/513 Train Loss: 409.5052\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 023/513 | Batch 000/059 | Loss: 391.0208\n",
      "Epoch: 023/513 Train Loss: 396.8907\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 024/513 | Batch 000/059 | Loss: 448.4853\n",
      "Epoch: 024/513 Train Loss: 388.4691\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 025/513 | Batch 000/059 | Loss: 352.3777\n",
      "Epoch: 025/513 Train Loss: 382.7699\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 026/513 | Batch 000/059 | Loss: 374.4994\n",
      "Epoch: 026/513 Train Loss: 378.3494\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 027/513 | Batch 000/059 | Loss: 435.4738\n",
      "Epoch: 027/513 Train Loss: 374.6767\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 028/513 | Batch 000/059 | Loss: 341.0440\n",
      "Epoch: 028/513 Train Loss: 371.8209\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 029/513 | Batch 000/059 | Loss: 239.7644\n",
      "Epoch: 029/513 Train Loss: 369.3761\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 030/513 | Batch 000/059 | Loss: 193.2921\n",
      "Epoch: 030/513 Train Loss: 367.2822\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 031/513 | Batch 000/059 | Loss: 321.9207\n",
      "Epoch: 031/513 Train Loss: 365.4965\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 032/513 | Batch 000/059 | Loss: 373.4371\n",
      "Epoch: 032/513 Train Loss: 363.8348\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 033/513 | Batch 000/059 | Loss: 371.9816\n",
      "Epoch: 033/513 Train Loss: 362.3798\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 034/513 | Batch 000/059 | Loss: 364.4001\n",
      "Epoch: 034/513 Train Loss: 361.0835\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 035/513 | Batch 000/059 | Loss: 196.4547\n",
      "Epoch: 035/513 Train Loss: 360.0236\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 036/513 | Batch 000/059 | Loss: 207.2627\n",
      "Epoch: 036/513 Train Loss: 358.9962\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 037/513 | Batch 000/059 | Loss: 248.3998\n",
      "Epoch: 037/513 Train Loss: 358.0829\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 038/513 | Batch 000/059 | Loss: 391.8416\n",
      "Epoch: 038/513 Train Loss: 357.0672\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 039/513 | Batch 000/059 | Loss: 360.4218\n",
      "Epoch: 039/513 Train Loss: 356.2519\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 040/513 | Batch 000/059 | Loss: 345.5845\n",
      "Epoch: 040/513 Train Loss: 355.4519\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 041/513 | Batch 000/059 | Loss: 388.6172\n",
      "Epoch: 041/513 Train Loss: 354.8054\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 042/513 | Batch 000/059 | Loss: 405.6666\n",
      "Epoch: 042/513 Train Loss: 354.1757\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 043/513 | Batch 000/059 | Loss: 338.8636\n",
      "Epoch: 043/513 Train Loss: 353.5635\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 044/513 | Batch 000/059 | Loss: 548.9626\n",
      "Epoch: 044/513 Train Loss: 352.8821\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 045/513 | Batch 000/059 | Loss: 466.4807\n",
      "Epoch: 045/513 Train Loss: 352.3650\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 046/513 | Batch 000/059 | Loss: 332.5879\n",
      "Epoch: 046/513 Train Loss: 351.7066\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 047/513 | Batch 000/059 | Loss: 358.3210\n",
      "Epoch: 047/513 Train Loss: 351.1928\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 048/513 | Batch 000/059 | Loss: 260.5021\n",
      "Epoch: 048/513 Train Loss: 350.6338\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 049/513 | Batch 000/059 | Loss: 243.8631\n",
      "Epoch: 049/513 Train Loss: 350.1534\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 050/513 | Batch 000/059 | Loss: 528.8855\n",
      "Epoch: 050/513 Train Loss: 349.6378\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 051/513 | Batch 000/059 | Loss: 240.9148\n",
      "Epoch: 051/513 Train Loss: 349.1394\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 052/513 | Batch 000/059 | Loss: 572.6635\n",
      "Epoch: 052/513 Train Loss: 348.6866\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 053/513 | Batch 000/059 | Loss: 737.5444\n",
      "Epoch: 053/513 Train Loss: 348.1549\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 054/513 | Batch 000/059 | Loss: 319.4813\n",
      "Epoch: 054/513 Train Loss: 347.6628\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 055/513 | Batch 000/059 | Loss: 217.5627\n",
      "Epoch: 055/513 Train Loss: 347.2295\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 056/513 | Batch 000/059 | Loss: 268.5519\n",
      "Epoch: 056/513 Train Loss: 346.7157\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 057/513 | Batch 000/059 | Loss: 349.0386\n",
      "Epoch: 057/513 Train Loss: 346.1969\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 058/513 | Batch 000/059 | Loss: 244.8694\n",
      "Epoch: 058/513 Train Loss: 345.7215\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 059/513 | Batch 000/059 | Loss: 393.2707\n",
      "Epoch: 059/513 Train Loss: 345.1870\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 060/513 | Batch 000/059 | Loss: 374.6368\n",
      "Epoch: 060/513 Train Loss: 344.6871\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 061/513 | Batch 000/059 | Loss: 439.2688\n",
      "Epoch: 061/513 Train Loss: 344.2844\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 062/513 | Batch 000/059 | Loss: 156.2315\n",
      "Epoch: 062/513 Train Loss: 343.7239\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 063/513 | Batch 000/059 | Loss: 278.2040\n",
      "Epoch: 063/513 Train Loss: 343.2659\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 064/513 | Batch 000/059 | Loss: 632.6144\n",
      "Epoch: 064/513 Train Loss: 342.7769\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 065/513 | Batch 000/059 | Loss: 302.6820\n",
      "Epoch: 065/513 Train Loss: 342.4732\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 066/513 | Batch 000/059 | Loss: 368.5124\n",
      "Epoch: 066/513 Train Loss: 341.8736\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 067/513 | Batch 000/059 | Loss: 288.6277\n",
      "Epoch: 067/513 Train Loss: 341.3070\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 068/513 | Batch 000/059 | Loss: 372.8566\n",
      "Epoch: 068/513 Train Loss: 340.8540\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 069/513 | Batch 000/059 | Loss: 434.1416\n",
      "Epoch: 069/513 Train Loss: 340.3985\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 070/513 | Batch 000/059 | Loss: 263.5870\n",
      "Epoch: 070/513 Train Loss: 339.9165\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 071/513 | Batch 000/059 | Loss: 326.6939\n",
      "Epoch: 071/513 Train Loss: 339.4836\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 072/513 | Batch 000/059 | Loss: 546.7451\n",
      "Epoch: 072/513 Train Loss: 339.0071\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 073/513 | Batch 000/059 | Loss: 325.2827\n",
      "Epoch: 073/513 Train Loss: 338.5719\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 074/513 | Batch 000/059 | Loss: 532.4871\n",
      "Epoch: 074/513 Train Loss: 338.1570\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 075/513 | Batch 000/059 | Loss: 330.7515\n",
      "Epoch: 075/513 Train Loss: 337.6942\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 076/513 | Batch 000/059 | Loss: 420.4532\n",
      "Epoch: 076/513 Train Loss: 337.2811\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 077/513 | Batch 000/059 | Loss: 320.5672\n",
      "Epoch: 077/513 Train Loss: 336.7836\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 078/513 | Batch 000/059 | Loss: 353.3839\n",
      "Epoch: 078/513 Train Loss: 336.3516\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 079/513 | Batch 000/059 | Loss: 230.9892\n",
      "Epoch: 079/513 Train Loss: 335.9155\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 080/513 | Batch 000/059 | Loss: 243.6129\n",
      "Epoch: 080/513 Train Loss: 335.4658\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 081/513 | Batch 000/059 | Loss: 289.5325\n",
      "Epoch: 081/513 Train Loss: 335.1167\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 082/513 | Batch 000/059 | Loss: 390.6400\n",
      "Epoch: 082/513 Train Loss: 334.6242\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 083/513 | Batch 000/059 | Loss: 356.3367\n",
      "Epoch: 083/513 Train Loss: 334.1411\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 084/513 | Batch 000/059 | Loss: 219.8866\n",
      "Epoch: 084/513 Train Loss: 333.8497\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 085/513 | Batch 000/059 | Loss: 312.0216\n",
      "Epoch: 085/513 Train Loss: 333.2996\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 086/513 | Batch 000/059 | Loss: 357.5993\n",
      "Epoch: 086/513 Train Loss: 332.8577\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 087/513 | Batch 000/059 | Loss: 238.0421\n",
      "Epoch: 087/513 Train Loss: 332.4859\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 088/513 | Batch 000/059 | Loss: 336.9375\n",
      "Epoch: 088/513 Train Loss: 332.0702\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 089/513 | Batch 000/059 | Loss: 215.5730\n",
      "Epoch: 089/513 Train Loss: 331.6678\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 090/513 | Batch 000/059 | Loss: 389.0926\n",
      "Epoch: 090/513 Train Loss: 331.3390\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 091/513 | Batch 000/059 | Loss: 316.0985\n",
      "Epoch: 091/513 Train Loss: 330.9106\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 092/513 | Batch 000/059 | Loss: 303.2293\n",
      "Epoch: 092/513 Train Loss: 330.3325\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 093/513 | Batch 000/059 | Loss: 181.7773\n",
      "Epoch: 093/513 Train Loss: 329.8818\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 094/513 | Batch 000/059 | Loss: 249.0125\n",
      "Epoch: 094/513 Train Loss: 329.4861\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 095/513 | Batch 000/059 | Loss: 373.2183\n",
      "Epoch: 095/513 Train Loss: 329.1937\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 096/513 | Batch 000/059 | Loss: 263.6031\n",
      "Epoch: 096/513 Train Loss: 328.7345\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 097/513 | Batch 000/059 | Loss: 388.5685\n",
      "Epoch: 097/513 Train Loss: 328.2233\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 098/513 | Batch 000/059 | Loss: 416.8831\n",
      "Epoch: 098/513 Train Loss: 327.8146\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 099/513 | Batch 000/059 | Loss: 358.1481\n",
      "Epoch: 099/513 Train Loss: 327.4485\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 100/513 | Batch 000/059 | Loss: 389.9194\n",
      "Epoch: 100/513 Train Loss: 326.9768\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 101/513 | Batch 000/059 | Loss: 236.8023\n",
      "Epoch: 101/513 Train Loss: 326.7144\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 102/513 | Batch 000/059 | Loss: 475.5875\n",
      "Epoch: 102/513 Train Loss: 326.1644\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 103/513 | Batch 000/059 | Loss: 220.7014\n",
      "Epoch: 103/513 Train Loss: 325.8129\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 104/513 | Batch 000/059 | Loss: 207.6661\n",
      "Epoch: 104/513 Train Loss: 325.5157\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 105/513 | Batch 000/059 | Loss: 331.1412\n",
      "Epoch: 105/513 Train Loss: 324.9558\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 106/513 | Batch 000/059 | Loss: 159.0176\n",
      "Epoch: 106/513 Train Loss: 324.5271\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 107/513 | Batch 000/059 | Loss: 356.6061\n",
      "Epoch: 107/513 Train Loss: 324.0933\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 108/513 | Batch 000/059 | Loss: 181.5417\n",
      "Epoch: 108/513 Train Loss: 323.6691\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 109/513 | Batch 000/059 | Loss: 169.7501\n",
      "Epoch: 109/513 Train Loss: 323.2802\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 110/513 | Batch 000/059 | Loss: 315.8079\n",
      "Epoch: 110/513 Train Loss: 322.9371\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 111/513 | Batch 000/059 | Loss: 320.3210\n",
      "Epoch: 111/513 Train Loss: 322.4209\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 112/513 | Batch 000/059 | Loss: 283.7456\n",
      "Epoch: 112/513 Train Loss: 322.0034\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 113/513 | Batch 000/059 | Loss: 401.4944\n",
      "Epoch: 113/513 Train Loss: 321.5940\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 114/513 | Batch 000/059 | Loss: 268.8025\n",
      "Epoch: 114/513 Train Loss: 321.4048\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 115/513 | Batch 000/059 | Loss: 255.7430\n",
      "Epoch: 115/513 Train Loss: 320.7348\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 116/513 | Batch 000/059 | Loss: 127.5390\n",
      "Epoch: 116/513 Train Loss: 320.3311\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 117/513 | Batch 000/059 | Loss: 459.5171\n",
      "Epoch: 117/513 Train Loss: 319.9130\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 118/513 | Batch 000/059 | Loss: 292.1961\n",
      "Epoch: 118/513 Train Loss: 319.6028\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 119/513 | Batch 000/059 | Loss: 177.4848\n",
      "Epoch: 119/513 Train Loss: 319.1193\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 120/513 | Batch 000/059 | Loss: 335.3858\n",
      "Epoch: 120/513 Train Loss: 318.6507\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 121/513 | Batch 000/059 | Loss: 311.7680\n",
      "Epoch: 121/513 Train Loss: 318.2650\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 122/513 | Batch 000/059 | Loss: 619.0694\n",
      "Epoch: 122/513 Train Loss: 317.9620\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 123/513 | Batch 000/059 | Loss: 259.4510\n",
      "Epoch: 123/513 Train Loss: 317.7140\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 124/513 | Batch 000/059 | Loss: 373.6763\n",
      "Epoch: 124/513 Train Loss: 317.0340\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 125/513 | Batch 000/059 | Loss: 158.1548\n",
      "Epoch: 125/513 Train Loss: 316.5983\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 126/513 | Batch 000/059 | Loss: 206.5358\n",
      "Epoch: 126/513 Train Loss: 316.3592\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 127/513 | Batch 000/059 | Loss: 343.5468\n",
      "Epoch: 127/513 Train Loss: 315.8068\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 128/513 | Batch 000/059 | Loss: 144.4015\n",
      "Epoch: 128/513 Train Loss: 315.3504\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 129/513 | Batch 000/059 | Loss: 310.2191\n",
      "Epoch: 129/513 Train Loss: 314.9399\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 130/513 | Batch 000/059 | Loss: 184.6502\n",
      "Epoch: 130/513 Train Loss: 314.5224\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 131/513 | Batch 000/059 | Loss: 379.7708\n",
      "Epoch: 131/513 Train Loss: 314.1743\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 132/513 | Batch 000/059 | Loss: 436.7921\n",
      "Epoch: 132/513 Train Loss: 313.6484\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 133/513 | Batch 000/059 | Loss: 235.7667\n",
      "Epoch: 133/513 Train Loss: 313.2040\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 134/513 | Batch 000/059 | Loss: 323.0066\n",
      "Epoch: 134/513 Train Loss: 312.8987\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 135/513 | Batch 000/059 | Loss: 791.8401\n",
      "Epoch: 135/513 Train Loss: 312.4249\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 136/513 | Batch 000/059 | Loss: 302.5382\n",
      "Epoch: 136/513 Train Loss: 312.0078\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 137/513 | Batch 000/059 | Loss: 237.4705\n",
      "Epoch: 137/513 Train Loss: 311.6508\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 138/513 | Batch 000/059 | Loss: 221.6772\n",
      "Epoch: 138/513 Train Loss: 311.1988\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 139/513 | Batch 000/059 | Loss: 403.8494\n",
      "Epoch: 139/513 Train Loss: 310.8907\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 140/513 | Batch 000/059 | Loss: 193.2377\n",
      "Epoch: 140/513 Train Loss: 310.4855\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 141/513 | Batch 000/059 | Loss: 572.2999\n",
      "Epoch: 141/513 Train Loss: 309.9896\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 142/513 | Batch 000/059 | Loss: 192.2253\n",
      "Epoch: 142/513 Train Loss: 309.5643\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 143/513 | Batch 000/059 | Loss: 196.5840\n",
      "Epoch: 143/513 Train Loss: 309.1192\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 144/513 | Batch 000/059 | Loss: 214.1080\n",
      "Epoch: 144/513 Train Loss: 308.7317\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 145/513 | Batch 000/059 | Loss: 356.0883\n",
      "Epoch: 145/513 Train Loss: 308.3121\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 146/513 | Batch 000/059 | Loss: 546.1437\n",
      "Epoch: 146/513 Train Loss: 307.9068\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 147/513 | Batch 000/059 | Loss: 331.4814\n",
      "Epoch: 147/513 Train Loss: 307.5255\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 148/513 | Batch 000/059 | Loss: 224.6938\n",
      "Epoch: 148/513 Train Loss: 307.0813\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 149/513 | Batch 000/059 | Loss: 550.8293\n",
      "Epoch: 149/513 Train Loss: 307.0013\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 150/513 | Batch 000/059 | Loss: 579.1909\n",
      "Epoch: 150/513 Train Loss: 306.3547\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 151/513 | Batch 000/059 | Loss: 368.8212\n",
      "Epoch: 151/513 Train Loss: 305.9349\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 152/513 | Batch 000/059 | Loss: 208.3143\n",
      "Epoch: 152/513 Train Loss: 305.6378\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 153/513 | Batch 000/059 | Loss: 333.5580\n",
      "Epoch: 153/513 Train Loss: 305.0904\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 154/513 | Batch 000/059 | Loss: 236.9447\n",
      "Epoch: 154/513 Train Loss: 304.6901\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 155/513 | Batch 000/059 | Loss: 341.7590\n",
      "Epoch: 155/513 Train Loss: 304.2877\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 156/513 | Batch 000/059 | Loss: 196.6985\n",
      "Epoch: 156/513 Train Loss: 303.9160\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 157/513 | Batch 000/059 | Loss: 376.9168\n",
      "Epoch: 157/513 Train Loss: 303.5555\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 158/513 | Batch 000/059 | Loss: 417.3493\n",
      "Epoch: 158/513 Train Loss: 303.0919\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 159/513 | Batch 000/059 | Loss: 494.9294\n",
      "Epoch: 159/513 Train Loss: 302.8975\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 160/513 | Batch 000/059 | Loss: 300.4476\n",
      "Epoch: 160/513 Train Loss: 302.2639\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 161/513 | Batch 000/059 | Loss: 243.2352\n",
      "Epoch: 161/513 Train Loss: 301.7923\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 162/513 | Batch 000/059 | Loss: 438.3463\n",
      "Epoch: 162/513 Train Loss: 301.4078\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 163/513 | Batch 000/059 | Loss: 224.5308\n",
      "Epoch: 163/513 Train Loss: 300.9924\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 164/513 | Batch 000/059 | Loss: 188.6058\n",
      "Epoch: 164/513 Train Loss: 300.5354\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 165/513 | Batch 000/059 | Loss: 547.1816\n",
      "Epoch: 165/513 Train Loss: 300.2002\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 166/513 | Batch 000/059 | Loss: 347.1454\n",
      "Epoch: 166/513 Train Loss: 299.6897\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 167/513 | Batch 000/059 | Loss: 166.6056\n",
      "Epoch: 167/513 Train Loss: 299.3582\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 168/513 | Batch 000/059 | Loss: 280.3314\n",
      "Epoch: 168/513 Train Loss: 298.8836\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 169/513 | Batch 000/059 | Loss: 291.6502\n",
      "Epoch: 169/513 Train Loss: 298.4554\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 170/513 | Batch 000/059 | Loss: 359.3265\n",
      "Epoch: 170/513 Train Loss: 298.2428\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 171/513 | Batch 000/059 | Loss: 187.7306\n",
      "Epoch: 171/513 Train Loss: 297.7092\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 172/513 | Batch 000/059 | Loss: 237.7136\n",
      "Epoch: 172/513 Train Loss: 297.2628\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 173/513 | Batch 000/059 | Loss: 322.0545\n",
      "Epoch: 173/513 Train Loss: 296.9086\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 174/513 | Batch 000/059 | Loss: 315.3640\n",
      "Epoch: 174/513 Train Loss: 296.4498\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 175/513 | Batch 000/059 | Loss: 399.4150\n",
      "Epoch: 175/513 Train Loss: 296.0500\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 176/513 | Batch 000/059 | Loss: 259.1182\n",
      "Epoch: 176/513 Train Loss: 295.6596\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 177/513 | Batch 000/059 | Loss: 297.3499\n",
      "Epoch: 177/513 Train Loss: 295.2524\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 178/513 | Batch 000/059 | Loss: 163.7365\n",
      "Epoch: 178/513 Train Loss: 294.8999\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 179/513 | Batch 000/059 | Loss: 347.7807\n",
      "Epoch: 179/513 Train Loss: 294.5355\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 180/513 | Batch 000/059 | Loss: 157.0527\n",
      "Epoch: 180/513 Train Loss: 294.0258\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 181/513 | Batch 000/059 | Loss: 353.5327\n",
      "Epoch: 181/513 Train Loss: 293.6307\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 182/513 | Batch 000/059 | Loss: 596.7085\n",
      "Epoch: 182/513 Train Loss: 293.3335\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 183/513 | Batch 000/059 | Loss: 259.9348\n",
      "Epoch: 183/513 Train Loss: 292.9740\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 184/513 | Batch 000/059 | Loss: 182.5986\n",
      "Epoch: 184/513 Train Loss: 292.6578\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 185/513 | Batch 000/059 | Loss: 643.2441\n",
      "Epoch: 185/513 Train Loss: 292.1258\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 186/513 | Batch 000/059 | Loss: 397.0354\n",
      "Epoch: 186/513 Train Loss: 292.1649\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 187/513 | Batch 000/059 | Loss: 280.3157\n",
      "Epoch: 187/513 Train Loss: 291.2591\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 188/513 | Batch 000/059 | Loss: 400.1977\n",
      "Epoch: 188/513 Train Loss: 290.8963\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 189/513 | Batch 000/059 | Loss: 490.6037\n",
      "Epoch: 189/513 Train Loss: 290.5151\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 190/513 | Batch 000/059 | Loss: 247.9674\n",
      "Epoch: 190/513 Train Loss: 290.2074\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 191/513 | Batch 000/059 | Loss: 282.8710\n",
      "Epoch: 191/513 Train Loss: 289.7441\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 192/513 | Batch 000/059 | Loss: 284.8016\n",
      "Epoch: 192/513 Train Loss: 289.3179\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 193/513 | Batch 000/059 | Loss: 300.3961\n",
      "Epoch: 193/513 Train Loss: 288.9774\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 194/513 | Batch 000/059 | Loss: 233.2250\n",
      "Epoch: 194/513 Train Loss: 288.6250\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 195/513 | Batch 000/059 | Loss: 419.8974\n",
      "Epoch: 195/513 Train Loss: 288.3167\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 196/513 | Batch 000/059 | Loss: 238.2669\n",
      "Epoch: 196/513 Train Loss: 287.8752\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 197/513 | Batch 000/059 | Loss: 341.9138\n",
      "Epoch: 197/513 Train Loss: 287.3819\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 198/513 | Batch 000/059 | Loss: 292.7131\n",
      "Epoch: 198/513 Train Loss: 286.9809\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 199/513 | Batch 000/059 | Loss: 241.0000\n",
      "Epoch: 199/513 Train Loss: 286.6096\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 200/513 | Batch 000/059 | Loss: 239.8155\n",
      "Epoch: 200/513 Train Loss: 286.2073\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 201/513 | Batch 000/059 | Loss: 219.2866\n",
      "Epoch: 201/513 Train Loss: 285.7919\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 202/513 | Batch 000/059 | Loss: 395.5729\n",
      "Epoch: 202/513 Train Loss: 285.5352\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 203/513 | Batch 000/059 | Loss: 265.7357\n",
      "Epoch: 203/513 Train Loss: 285.0886\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 204/513 | Batch 000/059 | Loss: 444.0223\n",
      "Epoch: 204/513 Train Loss: 284.6114\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 205/513 | Batch 000/059 | Loss: 344.6371\n",
      "Epoch: 205/513 Train Loss: 284.2748\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 206/513 | Batch 000/059 | Loss: 295.1747\n",
      "Epoch: 206/513 Train Loss: 283.8290\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 207/513 | Batch 000/059 | Loss: 232.1175\n",
      "Epoch: 207/513 Train Loss: 283.4989\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 208/513 | Batch 000/059 | Loss: 350.8365\n",
      "Epoch: 208/513 Train Loss: 283.0802\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 209/513 | Batch 000/059 | Loss: 214.8384\n",
      "Epoch: 209/513 Train Loss: 282.8295\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 210/513 | Batch 000/059 | Loss: 212.0714\n",
      "Epoch: 210/513 Train Loss: 282.2009\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 211/513 | Batch 000/059 | Loss: 180.4109\n",
      "Epoch: 211/513 Train Loss: 281.8928\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 212/513 | Batch 000/059 | Loss: 231.9150\n",
      "Epoch: 212/513 Train Loss: 281.5233\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 213/513 | Batch 000/059 | Loss: 278.8276\n",
      "Epoch: 213/513 Train Loss: 281.0957\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 214/513 | Batch 000/059 | Loss: 417.6602\n",
      "Epoch: 214/513 Train Loss: 280.7710\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 215/513 | Batch 000/059 | Loss: 298.4112\n",
      "Epoch: 215/513 Train Loss: 280.4804\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 216/513 | Batch 000/059 | Loss: 323.3302\n",
      "Epoch: 216/513 Train Loss: 279.9729\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 217/513 | Batch 000/059 | Loss: 268.3228\n",
      "Epoch: 217/513 Train Loss: 279.5835\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 218/513 | Batch 000/059 | Loss: 503.2356\n",
      "Epoch: 218/513 Train Loss: 279.1406\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 219/513 | Batch 000/059 | Loss: 221.8087\n",
      "Epoch: 219/513 Train Loss: 278.7636\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 220/513 | Batch 000/059 | Loss: 362.9724\n",
      "Epoch: 220/513 Train Loss: 278.3953\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 221/513 | Batch 000/059 | Loss: 167.0707\n",
      "Epoch: 221/513 Train Loss: 278.0350\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 222/513 | Batch 000/059 | Loss: 352.4864\n",
      "Epoch: 222/513 Train Loss: 277.6253\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 223/513 | Batch 000/059 | Loss: 291.4164\n",
      "Epoch: 223/513 Train Loss: 277.2705\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 224/513 | Batch 000/059 | Loss: 233.4103\n",
      "Epoch: 224/513 Train Loss: 276.9406\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 225/513 | Batch 000/059 | Loss: 204.7183\n",
      "Epoch: 225/513 Train Loss: 276.6187\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 226/513 | Batch 000/059 | Loss: 204.5502\n",
      "Epoch: 226/513 Train Loss: 276.2980\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 227/513 | Batch 000/059 | Loss: 310.9633\n",
      "Epoch: 227/513 Train Loss: 275.8418\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 228/513 | Batch 000/059 | Loss: 213.9722\n",
      "Epoch: 228/513 Train Loss: 275.4689\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 229/513 | Batch 000/059 | Loss: 202.1264\n",
      "Epoch: 229/513 Train Loss: 275.1208\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 230/513 | Batch 000/059 | Loss: 396.0437\n",
      "Epoch: 230/513 Train Loss: 274.6922\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 231/513 | Batch 000/059 | Loss: 257.4285\n",
      "Epoch: 231/513 Train Loss: 274.4684\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 232/513 | Batch 000/059 | Loss: 353.0709\n",
      "Epoch: 232/513 Train Loss: 274.0062\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 233/513 | Batch 000/059 | Loss: 247.9881\n",
      "Epoch: 233/513 Train Loss: 273.6199\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 234/513 | Batch 000/059 | Loss: 208.7541\n",
      "Epoch: 234/513 Train Loss: 273.2692\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 235/513 | Batch 000/059 | Loss: 237.1300\n",
      "Epoch: 235/513 Train Loss: 272.8762\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 236/513 | Batch 000/059 | Loss: 354.2346\n",
      "Epoch: 236/513 Train Loss: 272.5194\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 237/513 | Batch 000/059 | Loss: 435.0164\n",
      "Epoch: 237/513 Train Loss: 272.1551\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 238/513 | Batch 000/059 | Loss: 432.0378\n",
      "Epoch: 238/513 Train Loss: 271.8058\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 239/513 | Batch 000/059 | Loss: 267.1699\n",
      "Epoch: 239/513 Train Loss: 271.4253\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 240/513 | Batch 000/059 | Loss: 310.4404\n",
      "Epoch: 240/513 Train Loss: 271.0903\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 241/513 | Batch 000/059 | Loss: 353.3718\n",
      "Epoch: 241/513 Train Loss: 271.0079\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 242/513 | Batch 000/059 | Loss: 209.7202\n",
      "Epoch: 242/513 Train Loss: 270.4369\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 243/513 | Batch 000/059 | Loss: 146.5636\n",
      "Epoch: 243/513 Train Loss: 270.0872\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 244/513 | Batch 000/059 | Loss: 267.8007\n",
      "Epoch: 244/513 Train Loss: 269.8778\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 245/513 | Batch 000/059 | Loss: 310.8938\n",
      "Epoch: 245/513 Train Loss: 269.5181\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 246/513 | Batch 000/059 | Loss: 328.0797\n",
      "Epoch: 246/513 Train Loss: 269.0905\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 247/513 | Batch 000/059 | Loss: 216.3455\n",
      "Epoch: 247/513 Train Loss: 268.7537\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 248/513 | Batch 000/059 | Loss: 409.0266\n",
      "Epoch: 248/513 Train Loss: 268.4253\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 249/513 | Batch 000/059 | Loss: 206.1968\n",
      "Epoch: 249/513 Train Loss: 268.1369\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 250/513 | Batch 000/059 | Loss: 270.0285\n",
      "Epoch: 250/513 Train Loss: 267.7702\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 251/513 | Batch 000/059 | Loss: 289.7661\n",
      "Epoch: 251/513 Train Loss: 267.4516\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 252/513 | Batch 000/059 | Loss: 264.4571\n",
      "Epoch: 252/513 Train Loss: 267.0922\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 253/513 | Batch 000/059 | Loss: 215.2231\n",
      "Epoch: 253/513 Train Loss: 266.7598\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 254/513 | Batch 000/059 | Loss: 193.1126\n",
      "Epoch: 254/513 Train Loss: 266.7179\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 255/513 | Batch 000/059 | Loss: 446.2503\n",
      "Epoch: 255/513 Train Loss: 266.1502\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 256/513 | Batch 000/059 | Loss: 338.1034\n",
      "Epoch: 256/513 Train Loss: 265.8520\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 257/513 | Batch 000/059 | Loss: 184.4940\n",
      "Epoch: 257/513 Train Loss: 265.5087\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 258/513 | Batch 000/059 | Loss: 290.4183\n",
      "Epoch: 258/513 Train Loss: 265.1235\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 259/513 | Batch 000/059 | Loss: 448.5172\n",
      "Epoch: 259/513 Train Loss: 264.8342\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 260/513 | Batch 000/059 | Loss: 301.5492\n",
      "Epoch: 260/513 Train Loss: 264.5944\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 261/513 | Batch 000/059 | Loss: 208.8864\n",
      "Epoch: 261/513 Train Loss: 264.2490\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 262/513 | Batch 000/059 | Loss: 161.9069\n",
      "Epoch: 262/513 Train Loss: 263.9066\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 263/513 | Batch 000/059 | Loss: 385.3440\n",
      "Epoch: 263/513 Train Loss: 263.6582\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 264/513 | Batch 000/059 | Loss: 189.9517\n",
      "Epoch: 264/513 Train Loss: 263.3782\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 265/513 | Batch 000/059 | Loss: 241.6702\n",
      "Epoch: 265/513 Train Loss: 262.9087\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 266/513 | Batch 000/059 | Loss: 356.0047\n",
      "Epoch: 266/513 Train Loss: 263.2248\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 267/513 | Batch 000/059 | Loss: 319.0414\n",
      "Epoch: 267/513 Train Loss: 262.3066\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 268/513 | Batch 000/059 | Loss: 324.0104\n",
      "Epoch: 268/513 Train Loss: 262.0076\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 269/513 | Batch 000/059 | Loss: 149.8924\n",
      "Epoch: 269/513 Train Loss: 261.6747\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 270/513 | Batch 000/059 | Loss: 278.8586\n",
      "Epoch: 270/513 Train Loss: 261.4086\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 271/513 | Batch 000/059 | Loss: 465.5751\n",
      "Epoch: 271/513 Train Loss: 261.0157\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 272/513 | Batch 000/059 | Loss: 290.9970\n",
      "Epoch: 272/513 Train Loss: 260.7913\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 273/513 | Batch 000/059 | Loss: 231.5594\n",
      "Epoch: 273/513 Train Loss: 260.4421\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 274/513 | Batch 000/059 | Loss: 264.9388\n",
      "Epoch: 274/513 Train Loss: 260.1309\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 275/513 | Batch 000/059 | Loss: 406.2501\n",
      "Epoch: 275/513 Train Loss: 259.9603\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 276/513 | Batch 000/059 | Loss: 270.8313\n",
      "Epoch: 276/513 Train Loss: 259.5246\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 277/513 | Batch 000/059 | Loss: 298.6158\n",
      "Epoch: 277/513 Train Loss: 259.2270\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 278/513 | Batch 000/059 | Loss: 271.2476\n",
      "Epoch: 278/513 Train Loss: 258.9284\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 279/513 | Batch 000/059 | Loss: 266.0446\n",
      "Epoch: 279/513 Train Loss: 258.8973\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 280/513 | Batch 000/059 | Loss: 340.6780\n",
      "Epoch: 280/513 Train Loss: 258.4001\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 281/513 | Batch 000/059 | Loss: 242.5952\n",
      "Epoch: 281/513 Train Loss: 258.1106\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 282/513 | Batch 000/059 | Loss: 257.0411\n",
      "Epoch: 282/513 Train Loss: 257.8096\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 283/513 | Batch 000/059 | Loss: 303.4666\n",
      "Epoch: 283/513 Train Loss: 257.5584\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 284/513 | Batch 000/059 | Loss: 236.7995\n",
      "Epoch: 284/513 Train Loss: 257.2775\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 285/513 | Batch 000/059 | Loss: 312.0814\n",
      "Epoch: 285/513 Train Loss: 256.9217\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 286/513 | Batch 000/059 | Loss: 384.9746\n",
      "Epoch: 286/513 Train Loss: 256.6321\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 287/513 | Batch 000/059 | Loss: 275.7893\n",
      "Epoch: 287/513 Train Loss: 256.3913\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 288/513 | Batch 000/059 | Loss: 346.3039\n",
      "Epoch: 288/513 Train Loss: 256.1431\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 289/513 | Batch 000/059 | Loss: 183.2256\n",
      "Epoch: 289/513 Train Loss: 255.9937\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 290/513 | Batch 000/059 | Loss: 205.6318\n",
      "Epoch: 290/513 Train Loss: 255.6414\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 291/513 | Batch 000/059 | Loss: 381.3016\n",
      "Epoch: 291/513 Train Loss: 255.5150\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 292/513 | Batch 000/059 | Loss: 172.3608\n",
      "Epoch: 292/513 Train Loss: 255.1575\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 293/513 | Batch 000/059 | Loss: 282.7369\n",
      "Epoch: 293/513 Train Loss: 254.8792\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 294/513 | Batch 000/059 | Loss: 146.4235\n",
      "Epoch: 294/513 Train Loss: 254.6462\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 295/513 | Batch 000/059 | Loss: 296.1747\n",
      "Epoch: 295/513 Train Loss: 254.3286\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 296/513 | Batch 000/059 | Loss: 180.8191\n",
      "Epoch: 296/513 Train Loss: 254.1741\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 297/513 | Batch 000/059 | Loss: 197.2828\n",
      "Epoch: 297/513 Train Loss: 253.7764\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 298/513 | Batch 000/059 | Loss: 121.2290\n",
      "Epoch: 298/513 Train Loss: 253.6653\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 299/513 | Batch 000/059 | Loss: 511.2135\n",
      "Epoch: 299/513 Train Loss: 253.3692\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 300/513 | Batch 000/059 | Loss: 335.5610\n",
      "Epoch: 300/513 Train Loss: 253.2628\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 301/513 | Batch 000/059 | Loss: 435.4931\n",
      "Epoch: 301/513 Train Loss: 252.8391\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 302/513 | Batch 000/059 | Loss: 276.1871\n",
      "Epoch: 302/513 Train Loss: 252.7457\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 303/513 | Batch 000/059 | Loss: 288.7949\n",
      "Epoch: 303/513 Train Loss: 252.3930\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 304/513 | Batch 000/059 | Loss: 168.5647\n",
      "Epoch: 304/513 Train Loss: 252.1138\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 305/513 | Batch 000/059 | Loss: 244.2853\n",
      "Epoch: 305/513 Train Loss: 251.8759\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 306/513 | Batch 000/059 | Loss: 286.5259\n",
      "Epoch: 306/513 Train Loss: 251.9423\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 307/513 | Batch 000/059 | Loss: 389.6082\n",
      "Epoch: 307/513 Train Loss: 251.3928\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 308/513 | Batch 000/059 | Loss: 226.6345\n",
      "Epoch: 308/513 Train Loss: 251.1693\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 309/513 | Batch 000/059 | Loss: 311.8092\n",
      "Epoch: 309/513 Train Loss: 250.9116\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 310/513 | Batch 000/059 | Loss: 148.1782\n",
      "Epoch: 310/513 Train Loss: 250.6752\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 311/513 | Batch 000/059 | Loss: 233.4670\n",
      "Epoch: 311/513 Train Loss: 250.5813\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 312/513 | Batch 000/059 | Loss: 392.8784\n",
      "Epoch: 312/513 Train Loss: 250.2245\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 313/513 | Batch 000/059 | Loss: 279.1880\n",
      "Epoch: 313/513 Train Loss: 250.1207\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 314/513 | Batch 000/059 | Loss: 386.7504\n",
      "Epoch: 314/513 Train Loss: 249.8554\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 315/513 | Batch 000/059 | Loss: 352.5610\n",
      "Epoch: 315/513 Train Loss: 249.6715\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 316/513 | Batch 000/059 | Loss: 334.5517\n",
      "Epoch: 316/513 Train Loss: 249.4033\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 317/513 | Batch 000/059 | Loss: 159.7003\n",
      "Epoch: 317/513 Train Loss: 249.1476\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 318/513 | Batch 000/059 | Loss: 165.3919\n",
      "Epoch: 318/513 Train Loss: 248.9776\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 319/513 | Batch 000/059 | Loss: 413.7901\n",
      "Epoch: 319/513 Train Loss: 248.7619\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 320/513 | Batch 000/059 | Loss: 308.5376\n",
      "Epoch: 320/513 Train Loss: 248.4538\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 321/513 | Batch 000/059 | Loss: 169.5762\n",
      "Epoch: 321/513 Train Loss: 248.3787\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 322/513 | Batch 000/059 | Loss: 170.2185\n",
      "Epoch: 322/513 Train Loss: 248.0593\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 323/513 | Batch 000/059 | Loss: 240.2433\n",
      "Epoch: 323/513 Train Loss: 247.8529\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 324/513 | Batch 000/059 | Loss: 192.7690\n",
      "Epoch: 324/513 Train Loss: 247.8676\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 325/513 | Batch 000/059 | Loss: 402.3669\n",
      "Epoch: 325/513 Train Loss: 247.4129\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 326/513 | Batch 000/059 | Loss: 284.0121\n",
      "Epoch: 326/513 Train Loss: 247.2545\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 327/513 | Batch 000/059 | Loss: 384.0933\n",
      "Epoch: 327/513 Train Loss: 247.1128\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 328/513 | Batch 000/059 | Loss: 246.4418\n",
      "Epoch: 328/513 Train Loss: 246.8173\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 329/513 | Batch 000/059 | Loss: 206.5049\n",
      "Epoch: 329/513 Train Loss: 246.6269\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 330/513 | Batch 000/059 | Loss: 150.0363\n",
      "Epoch: 330/513 Train Loss: 246.4859\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 331/513 | Batch 000/059 | Loss: 186.5233\n",
      "Epoch: 331/513 Train Loss: 246.2887\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 332/513 | Batch 000/059 | Loss: 393.0020\n",
      "Epoch: 332/513 Train Loss: 246.2255\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 333/513 | Batch 000/059 | Loss: 129.1020\n",
      "Epoch: 333/513 Train Loss: 245.9285\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 334/513 | Batch 000/059 | Loss: 334.7278\n",
      "Epoch: 334/513 Train Loss: 246.2421\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 335/513 | Batch 000/059 | Loss: 346.8209\n",
      "Epoch: 335/513 Train Loss: 245.7410\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 336/513 | Batch 000/059 | Loss: 238.7749\n",
      "Epoch: 336/513 Train Loss: 245.3625\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 337/513 | Batch 000/059 | Loss: 195.7945\n",
      "Epoch: 337/513 Train Loss: 245.4050\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 338/513 | Batch 000/059 | Loss: 342.4905\n",
      "Epoch: 338/513 Train Loss: 245.0375\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 339/513 | Batch 000/059 | Loss: 245.4027\n",
      "Epoch: 339/513 Train Loss: 245.0823\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 340/513 | Batch 000/059 | Loss: 379.9197\n",
      "Epoch: 340/513 Train Loss: 244.6691\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 341/513 | Batch 000/059 | Loss: 351.0967\n",
      "Epoch: 341/513 Train Loss: 244.5174\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 342/513 | Batch 000/059 | Loss: 276.1533\n",
      "Epoch: 342/513 Train Loss: 244.3368\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 343/513 | Batch 000/059 | Loss: 160.4797\n",
      "Epoch: 343/513 Train Loss: 244.0903\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 344/513 | Batch 000/059 | Loss: 197.2448\n",
      "Epoch: 344/513 Train Loss: 243.9587\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 345/513 | Batch 000/059 | Loss: 210.7034\n",
      "Epoch: 345/513 Train Loss: 243.8047\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 346/513 | Batch 000/059 | Loss: 180.3700\n",
      "Epoch: 346/513 Train Loss: 243.6199\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 347/513 | Batch 000/059 | Loss: 357.9602\n",
      "Epoch: 347/513 Train Loss: 243.4402\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 348/513 | Batch 000/059 | Loss: 248.2705\n",
      "Epoch: 348/513 Train Loss: 243.2826\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 349/513 | Batch 000/059 | Loss: 226.7213\n",
      "Epoch: 349/513 Train Loss: 243.1334\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 350/513 | Batch 000/059 | Loss: 240.5823\n",
      "Epoch: 350/513 Train Loss: 242.9585\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 351/513 | Batch 000/059 | Loss: 186.2468\n",
      "Epoch: 351/513 Train Loss: 242.7734\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 352/513 | Batch 000/059 | Loss: 332.7289\n",
      "Epoch: 352/513 Train Loss: 242.8357\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 353/513 | Batch 000/059 | Loss: 130.7933\n",
      "Epoch: 353/513 Train Loss: 242.4585\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 354/513 | Batch 000/059 | Loss: 236.9411\n",
      "Epoch: 354/513 Train Loss: 242.3859\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 355/513 | Batch 000/059 | Loss: 267.5779\n",
      "Epoch: 355/513 Train Loss: 242.1733\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 356/513 | Batch 000/059 | Loss: 230.6454\n",
      "Epoch: 356/513 Train Loss: 242.0732\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 357/513 | Batch 000/059 | Loss: 334.3065\n",
      "Epoch: 357/513 Train Loss: 241.9227\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 358/513 | Batch 000/059 | Loss: 524.3906\n",
      "Epoch: 358/513 Train Loss: 241.7893\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 359/513 | Batch 000/059 | Loss: 315.8537\n",
      "Epoch: 359/513 Train Loss: 241.6444\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 360/513 | Batch 000/059 | Loss: 279.8331\n",
      "Epoch: 360/513 Train Loss: 241.5005\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 361/513 | Batch 000/059 | Loss: 144.8847\n",
      "Epoch: 361/513 Train Loss: 241.5633\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 362/513 | Batch 000/059 | Loss: 157.8903\n",
      "Epoch: 362/513 Train Loss: 241.2209\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 363/513 | Batch 000/059 | Loss: 247.6333\n",
      "Epoch: 363/513 Train Loss: 241.1068\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 364/513 | Batch 000/059 | Loss: 318.5375\n",
      "Epoch: 364/513 Train Loss: 241.0414\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 365/513 | Batch 000/059 | Loss: 256.9771\n",
      "Epoch: 365/513 Train Loss: 240.9256\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 366/513 | Batch 000/059 | Loss: 216.0723\n",
      "Epoch: 366/513 Train Loss: 240.7400\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 367/513 | Batch 000/059 | Loss: 244.7946\n",
      "Epoch: 367/513 Train Loss: 240.8521\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 368/513 | Batch 000/059 | Loss: 348.2770\n",
      "Epoch: 368/513 Train Loss: 240.5391\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 369/513 | Batch 000/059 | Loss: 308.4679\n",
      "Epoch: 369/513 Train Loss: 240.5083\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 370/513 | Batch 000/059 | Loss: 167.4114\n",
      "Epoch: 370/513 Train Loss: 240.3103\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 371/513 | Batch 000/059 | Loss: 184.0392\n",
      "Epoch: 371/513 Train Loss: 240.6387\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 372/513 | Batch 000/059 | Loss: 352.7830\n",
      "Epoch: 372/513 Train Loss: 240.1028\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 373/513 | Batch 000/059 | Loss: 194.4665\n",
      "Epoch: 373/513 Train Loss: 240.0485\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 374/513 | Batch 000/059 | Loss: 158.0811\n",
      "Epoch: 374/513 Train Loss: 240.0492\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 375/513 | Batch 000/059 | Loss: 276.4776\n",
      "Epoch: 375/513 Train Loss: 239.7005\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 376/513 | Batch 000/059 | Loss: 186.7439\n",
      "Epoch: 376/513 Train Loss: 240.0490\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 377/513 | Batch 000/059 | Loss: 360.1526\n",
      "Epoch: 377/513 Train Loss: 239.6686\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 378/513 | Batch 000/059 | Loss: 233.2101\n",
      "Epoch: 378/513 Train Loss: 239.4718\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 379/513 | Batch 000/059 | Loss: 380.6219\n",
      "Epoch: 379/513 Train Loss: 239.1915\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 380/513 | Batch 000/059 | Loss: 351.0684\n",
      "Epoch: 380/513 Train Loss: 239.0718\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 381/513 | Batch 000/059 | Loss: 161.0005\n",
      "Epoch: 381/513 Train Loss: 238.9866\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 382/513 | Batch 000/059 | Loss: 362.7568\n",
      "Epoch: 382/513 Train Loss: 238.8532\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 383/513 | Batch 000/059 | Loss: 187.1495\n",
      "Epoch: 383/513 Train Loss: 238.7694\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 384/513 | Batch 000/059 | Loss: 304.1668\n",
      "Epoch: 384/513 Train Loss: 238.6491\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 385/513 | Batch 000/059 | Loss: 145.7821\n",
      "Epoch: 385/513 Train Loss: 238.5298\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 386/513 | Batch 000/059 | Loss: 270.4738\n",
      "Epoch: 386/513 Train Loss: 238.4069\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 387/513 | Batch 000/059 | Loss: 176.1594\n",
      "Epoch: 387/513 Train Loss: 238.6074\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 388/513 | Batch 000/059 | Loss: 119.1432\n",
      "Epoch: 388/513 Train Loss: 238.4863\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 389/513 | Batch 000/059 | Loss: 312.6961\n",
      "Epoch: 389/513 Train Loss: 238.1840\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 390/513 | Batch 000/059 | Loss: 202.3166\n",
      "Epoch: 390/513 Train Loss: 238.0113\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 391/513 | Batch 000/059 | Loss: 177.9282\n",
      "Epoch: 391/513 Train Loss: 237.9509\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 392/513 | Batch 000/059 | Loss: 207.7726\n",
      "Epoch: 392/513 Train Loss: 237.8643\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 393/513 | Batch 000/059 | Loss: 355.7642\n",
      "Epoch: 393/513 Train Loss: 237.7148\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 394/513 | Batch 000/059 | Loss: 407.7015\n",
      "Epoch: 394/513 Train Loss: 237.6943\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 395/513 | Batch 000/059 | Loss: 182.1961\n",
      "Epoch: 395/513 Train Loss: 237.6384\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 396/513 | Batch 000/059 | Loss: 213.2792\n",
      "Epoch: 396/513 Train Loss: 237.4337\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 397/513 | Batch 000/059 | Loss: 266.8256\n",
      "Epoch: 397/513 Train Loss: 237.3782\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 398/513 | Batch 000/059 | Loss: 536.6347\n",
      "Epoch: 398/513 Train Loss: 237.2483\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 399/513 | Batch 000/059 | Loss: 188.8534\n",
      "Epoch: 399/513 Train Loss: 237.1372\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 400/513 | Batch 000/059 | Loss: 265.7282\n",
      "Epoch: 400/513 Train Loss: 237.0471\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 401/513 | Batch 000/059 | Loss: 155.6618\n",
      "Epoch: 401/513 Train Loss: 237.1359\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 402/513 | Batch 000/059 | Loss: 166.9182\n",
      "Epoch: 402/513 Train Loss: 236.8673\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 403/513 | Batch 000/059 | Loss: 248.5137\n",
      "Epoch: 403/513 Train Loss: 236.8541\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 404/513 | Batch 000/059 | Loss: 190.9714\n",
      "Epoch: 404/513 Train Loss: 236.6689\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 405/513 | Batch 000/059 | Loss: 238.0650\n",
      "Epoch: 405/513 Train Loss: 236.5919\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 406/513 | Batch 000/059 | Loss: 308.4088\n",
      "Epoch: 406/513 Train Loss: 236.6211\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 407/513 | Batch 000/059 | Loss: 235.2442\n",
      "Epoch: 407/513 Train Loss: 236.4451\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 408/513 | Batch 000/059 | Loss: 220.8200\n",
      "Epoch: 408/513 Train Loss: 236.3930\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 409/513 | Batch 000/059 | Loss: 154.8324\n",
      "Epoch: 409/513 Train Loss: 236.3478\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 410/513 | Batch 000/059 | Loss: 183.8723\n",
      "Epoch: 410/513 Train Loss: 236.3368\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 411/513 | Batch 000/059 | Loss: 185.3667\n",
      "Epoch: 411/513 Train Loss: 236.2854\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 412/513 | Batch 000/059 | Loss: 223.7437\n",
      "Epoch: 412/513 Train Loss: 236.1518\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 413/513 | Batch 000/059 | Loss: 397.3998\n",
      "Epoch: 413/513 Train Loss: 236.1094\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 414/513 | Batch 000/059 | Loss: 295.6369\n",
      "Epoch: 414/513 Train Loss: 236.6942\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 415/513 | Batch 000/059 | Loss: 184.6849\n",
      "Epoch: 415/513 Train Loss: 235.9051\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 416/513 | Batch 000/059 | Loss: 355.2801\n",
      "Epoch: 416/513 Train Loss: 235.8956\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 417/513 | Batch 000/059 | Loss: 274.0392\n",
      "Epoch: 417/513 Train Loss: 235.7659\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 418/513 | Batch 000/059 | Loss: 352.1002\n",
      "Epoch: 418/513 Train Loss: 235.7205\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 419/513 | Batch 000/059 | Loss: 293.8297\n",
      "Epoch: 419/513 Train Loss: 235.8006\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 420/513 | Batch 000/059 | Loss: 459.6416\n",
      "Epoch: 420/513 Train Loss: 235.5096\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 421/513 | Batch 000/059 | Loss: 202.7437\n",
      "Epoch: 421/513 Train Loss: 235.5906\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 422/513 | Batch 000/059 | Loss: 148.1509\n",
      "Epoch: 422/513 Train Loss: 235.3456\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 423/513 | Batch 000/059 | Loss: 184.1960\n",
      "Epoch: 423/513 Train Loss: 235.8856\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 424/513 | Batch 000/059 | Loss: 326.7946\n",
      "Epoch: 424/513 Train Loss: 235.2081\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 425/513 | Batch 000/059 | Loss: 566.9052\n",
      "Epoch: 425/513 Train Loss: 235.1560\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 426/513 | Batch 000/059 | Loss: 306.9755\n",
      "Epoch: 426/513 Train Loss: 235.0363\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 427/513 | Batch 000/059 | Loss: 188.1088\n",
      "Epoch: 427/513 Train Loss: 234.9676\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 428/513 | Batch 000/059 | Loss: 321.7424\n",
      "Epoch: 428/513 Train Loss: 235.1651\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 429/513 | Batch 000/059 | Loss: 216.4978\n",
      "Epoch: 429/513 Train Loss: 234.8399\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 430/513 | Batch 000/059 | Loss: 220.5306\n",
      "Epoch: 430/513 Train Loss: 235.8100\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 431/513 | Batch 000/059 | Loss: 417.6011\n",
      "Epoch: 431/513 Train Loss: 234.7576\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 432/513 | Batch 000/059 | Loss: 438.7427\n",
      "Epoch: 432/513 Train Loss: 234.7692\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 433/513 | Batch 000/059 | Loss: 360.5292\n",
      "Epoch: 433/513 Train Loss: 234.6771\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 434/513 | Batch 000/059 | Loss: 160.2593\n",
      "Epoch: 434/513 Train Loss: 234.6282\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 435/513 | Batch 000/059 | Loss: 245.8683\n",
      "Epoch: 435/513 Train Loss: 234.7012\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 436/513 | Batch 000/059 | Loss: 227.6044\n",
      "Epoch: 436/513 Train Loss: 234.5212\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 437/513 | Batch 000/059 | Loss: 251.4619\n",
      "Epoch: 437/513 Train Loss: 234.4257\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 438/513 | Batch 000/059 | Loss: 317.8139\n",
      "Epoch: 438/513 Train Loss: 234.3960\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 439/513 | Batch 000/059 | Loss: 277.0240\n",
      "Epoch: 439/513 Train Loss: 234.6504\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 440/513 | Batch 000/059 | Loss: 266.4928\n",
      "Epoch: 440/513 Train Loss: 234.3802\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 441/513 | Batch 000/059 | Loss: 215.0006\n",
      "Epoch: 441/513 Train Loss: 234.1302\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 442/513 | Batch 000/059 | Loss: 225.4384\n",
      "Epoch: 442/513 Train Loss: 234.1305\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 443/513 | Batch 000/059 | Loss: 274.6564\n",
      "Epoch: 443/513 Train Loss: 234.0453\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 444/513 | Batch 000/059 | Loss: 212.3215\n",
      "Epoch: 444/513 Train Loss: 233.9459\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 445/513 | Batch 000/059 | Loss: 250.7498\n",
      "Epoch: 445/513 Train Loss: 233.8494\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 446/513 | Batch 000/059 | Loss: 286.7883\n",
      "Epoch: 446/513 Train Loss: 233.7976\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 447/513 | Batch 000/059 | Loss: 227.7314\n",
      "Epoch: 447/513 Train Loss: 234.0881\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 448/513 | Batch 000/059 | Loss: 216.7898\n",
      "Epoch: 448/513 Train Loss: 233.7484\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 449/513 | Batch 000/059 | Loss: 113.3976\n",
      "Epoch: 449/513 Train Loss: 233.7104\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 450/513 | Batch 000/059 | Loss: 196.4726\n",
      "Epoch: 450/513 Train Loss: 233.7065\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 451/513 | Batch 000/059 | Loss: 206.1860\n",
      "Epoch: 451/513 Train Loss: 233.9707\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 452/513 | Batch 000/059 | Loss: 177.7067\n",
      "Epoch: 452/513 Train Loss: 233.5468\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 453/513 | Batch 000/059 | Loss: 351.5838\n",
      "Epoch: 453/513 Train Loss: 233.5057\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 454/513 | Batch 000/059 | Loss: 317.5837\n",
      "Epoch: 454/513 Train Loss: 233.9282\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 455/513 | Batch 000/059 | Loss: 307.1879\n",
      "Epoch: 455/513 Train Loss: 233.4477\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 456/513 | Batch 000/059 | Loss: 280.8407\n",
      "Epoch: 456/513 Train Loss: 233.4272\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 457/513 | Batch 000/059 | Loss: 212.1309\n",
      "Epoch: 457/513 Train Loss: 233.3586\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 458/513 | Batch 000/059 | Loss: 243.8837\n",
      "Epoch: 458/513 Train Loss: 233.3180\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 459/513 | Batch 000/059 | Loss: 323.9244\n",
      "Epoch: 459/513 Train Loss: 233.4361\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 460/513 | Batch 000/059 | Loss: 249.8829\n",
      "Epoch: 460/513 Train Loss: 233.2532\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 461/513 | Batch 000/059 | Loss: 193.7498\n",
      "Epoch: 461/513 Train Loss: 233.2207\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 462/513 | Batch 000/059 | Loss: 262.7313\n",
      "Epoch: 462/513 Train Loss: 233.1962\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 463/513 | Batch 000/059 | Loss: 253.7204\n",
      "Epoch: 463/513 Train Loss: 233.1802\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 464/513 | Batch 000/059 | Loss: 212.8916\n",
      "Epoch: 464/513 Train Loss: 233.0738\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 465/513 | Batch 000/059 | Loss: 296.5238\n",
      "Epoch: 465/513 Train Loss: 233.1476\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 466/513 | Batch 000/059 | Loss: 234.0147\n",
      "Epoch: 466/513 Train Loss: 232.9397\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 467/513 | Batch 000/059 | Loss: 174.1605\n",
      "Epoch: 467/513 Train Loss: 232.8808\n",
      "Time elapsed: 1.19 min\n",
      "Epoch: 468/513 | Batch 000/059 | Loss: 220.2812\n",
      "Epoch: 468/513 Train Loss: 233.0227\n",
      "Time elapsed: 1.19 min\n",
      "Epoch: 469/513 | Batch 000/059 | Loss: 176.2569\n",
      "Epoch: 469/513 Train Loss: 232.9770\n",
      "Time elapsed: 1.19 min\n",
      "Epoch: 470/513 | Batch 000/059 | Loss: 156.0317\n",
      "Epoch: 470/513 Train Loss: 232.8595\n",
      "Time elapsed: 1.19 min\n",
      "Epoch: 471/513 | Batch 000/059 | Loss: 214.3127\n",
      "Epoch: 471/513 Train Loss: 232.8569\n",
      "Time elapsed: 1.20 min\n",
      "Epoch: 472/513 | Batch 000/059 | Loss: 310.5403\n",
      "Epoch: 472/513 Train Loss: 232.7998\n",
      "Time elapsed: 1.20 min\n",
      "Epoch: 473/513 | Batch 000/059 | Loss: 133.8240\n",
      "Epoch: 473/513 Train Loss: 232.9395\n",
      "Time elapsed: 1.20 min\n",
      "Epoch: 474/513 | Batch 000/059 | Loss: 191.2323\n",
      "Epoch: 474/513 Train Loss: 232.6992\n",
      "Time elapsed: 1.20 min\n",
      "Epoch: 475/513 | Batch 000/059 | Loss: 177.6115\n",
      "Epoch: 475/513 Train Loss: 232.6560\n",
      "Time elapsed: 1.21 min\n",
      "Epoch: 476/513 | Batch 000/059 | Loss: 177.0031\n",
      "Epoch: 476/513 Train Loss: 232.7318\n",
      "Time elapsed: 1.21 min\n",
      "Epoch: 477/513 | Batch 000/059 | Loss: 270.3681\n",
      "Epoch: 477/513 Train Loss: 232.5695\n",
      "Time elapsed: 1.21 min\n",
      "Epoch: 478/513 | Batch 000/059 | Loss: 168.5526\n",
      "Epoch: 478/513 Train Loss: 232.5859\n",
      "Time elapsed: 1.21 min\n",
      "Epoch: 479/513 | Batch 000/059 | Loss: 153.4580\n",
      "Epoch: 479/513 Train Loss: 232.5344\n",
      "Time elapsed: 1.22 min\n",
      "Epoch: 480/513 | Batch 000/059 | Loss: 221.6508\n",
      "Epoch: 480/513 Train Loss: 232.4665\n",
      "Time elapsed: 1.22 min\n",
      "Epoch: 481/513 | Batch 000/059 | Loss: 180.8707\n",
      "Epoch: 481/513 Train Loss: 232.3909\n",
      "Time elapsed: 1.22 min\n",
      "Epoch: 482/513 | Batch 000/059 | Loss: 344.5224\n",
      "Epoch: 482/513 Train Loss: 232.7475\n",
      "Time elapsed: 1.22 min\n",
      "Epoch: 483/513 | Batch 000/059 | Loss: 245.6867\n",
      "Epoch: 483/513 Train Loss: 232.4329\n",
      "Time elapsed: 1.23 min\n",
      "Epoch: 484/513 | Batch 000/059 | Loss: 296.4449\n",
      "Epoch: 484/513 Train Loss: 232.2673\n",
      "Time elapsed: 1.23 min\n",
      "Epoch: 485/513 | Batch 000/059 | Loss: 302.2253\n",
      "Epoch: 485/513 Train Loss: 232.2245\n",
      "Time elapsed: 1.23 min\n",
      "Epoch: 486/513 | Batch 000/059 | Loss: 201.7533\n",
      "Epoch: 486/513 Train Loss: 232.2479\n",
      "Time elapsed: 1.23 min\n",
      "Epoch: 487/513 | Batch 000/059 | Loss: 296.6131\n",
      "Epoch: 487/513 Train Loss: 232.1621\n",
      "Time elapsed: 1.24 min\n",
      "Epoch: 488/513 | Batch 000/059 | Loss: 229.9007\n",
      "Epoch: 488/513 Train Loss: 232.1413\n",
      "Time elapsed: 1.24 min\n",
      "Epoch: 489/513 | Batch 000/059 | Loss: 468.6114\n",
      "Epoch: 489/513 Train Loss: 232.0818\n",
      "Time elapsed: 1.24 min\n",
      "Epoch: 490/513 | Batch 000/059 | Loss: 297.5064\n",
      "Epoch: 490/513 Train Loss: 232.0543\n",
      "Time elapsed: 1.24 min\n",
      "Epoch: 491/513 | Batch 000/059 | Loss: 362.9543\n",
      "Epoch: 491/513 Train Loss: 231.9879\n",
      "Time elapsed: 1.25 min\n",
      "Epoch: 492/513 | Batch 000/059 | Loss: 366.7579\n",
      "Epoch: 492/513 Train Loss: 232.0339\n",
      "Time elapsed: 1.25 min\n",
      "Epoch: 493/513 | Batch 000/059 | Loss: 189.8261\n",
      "Epoch: 493/513 Train Loss: 231.9351\n",
      "Time elapsed: 1.25 min\n",
      "Epoch: 494/513 | Batch 000/059 | Loss: 188.3677\n",
      "Epoch: 494/513 Train Loss: 231.8770\n",
      "Time elapsed: 1.25 min\n",
      "Epoch: 495/513 | Batch 000/059 | Loss: 207.6954\n",
      "Epoch: 495/513 Train Loss: 232.1053\n",
      "Time elapsed: 1.26 min\n",
      "Epoch: 496/513 | Batch 000/059 | Loss: 218.3994\n",
      "Epoch: 496/513 Train Loss: 231.7658\n",
      "Time elapsed: 1.26 min\n",
      "Epoch: 497/513 | Batch 000/059 | Loss: 284.3353\n",
      "Epoch: 497/513 Train Loss: 232.0312\n",
      "Time elapsed: 1.26 min\n",
      "Epoch: 498/513 | Batch 000/059 | Loss: 140.0421\n",
      "Epoch: 498/513 Train Loss: 231.7713\n",
      "Time elapsed: 1.26 min\n",
      "Epoch: 499/513 | Batch 000/059 | Loss: 221.9225\n",
      "Epoch: 499/513 Train Loss: 231.6927\n",
      "Time elapsed: 1.27 min\n",
      "Epoch: 500/513 | Batch 000/059 | Loss: 233.2107\n",
      "Epoch: 500/513 Train Loss: 231.9062\n",
      "Time elapsed: 1.27 min\n",
      "Epoch: 501/513 | Batch 000/059 | Loss: 423.7811\n",
      "Epoch: 501/513 Train Loss: 231.6683\n",
      "Time elapsed: 1.27 min\n",
      "Epoch: 502/513 | Batch 000/059 | Loss: 297.7537\n",
      "Epoch: 502/513 Train Loss: 231.8361\n",
      "Time elapsed: 1.27 min\n",
      "Epoch: 503/513 | Batch 000/059 | Loss: 149.5952\n",
      "Epoch: 503/513 Train Loss: 231.7569\n",
      "Time elapsed: 1.28 min\n",
      "Epoch: 504/513 | Batch 000/059 | Loss: 214.5437\n",
      "Epoch: 504/513 Train Loss: 231.6629\n",
      "Time elapsed: 1.28 min\n",
      "Epoch: 505/513 | Batch 000/059 | Loss: 171.7995\n",
      "Epoch: 505/513 Train Loss: 231.6088\n",
      "Time elapsed: 1.28 min\n",
      "Epoch: 506/513 | Batch 000/059 | Loss: 259.0626\n",
      "Epoch: 506/513 Train Loss: 231.5462\n",
      "Time elapsed: 1.28 min\n",
      "Epoch: 507/513 | Batch 000/059 | Loss: 149.0698\n",
      "Epoch: 507/513 Train Loss: 231.5544\n",
      "Time elapsed: 1.29 min\n",
      "Epoch: 508/513 | Batch 000/059 | Loss: 528.5333\n",
      "Epoch: 508/513 Train Loss: 231.4204\n",
      "Time elapsed: 1.29 min\n",
      "Epoch: 509/513 | Batch 000/059 | Loss: 347.0405\n",
      "Epoch: 509/513 Train Loss: 231.3524\n",
      "Time elapsed: 1.29 min\n",
      "Epoch: 510/513 | Batch 000/059 | Loss: 354.8506\n",
      "Epoch: 510/513 Train Loss: 231.4069\n",
      "Time elapsed: 1.29 min\n",
      "Epoch: 511/513 | Batch 000/059 | Loss: 293.3338\n",
      "Epoch: 511/513 Train Loss: 231.5783\n",
      "Time elapsed: 1.30 min\n",
      "Epoch: 512/513 | Batch 000/059 | Loss: 285.8553\n",
      "Epoch: 512/513 Train Loss: 231.3473\n",
      "Time elapsed: 1.30 min\n",
      "Epoch: 513/513 | Batch 000/059 | Loss: 180.1533\n",
      "Epoch: 513/513 Train Loss: 231.4277\n",
      "Time elapsed: 1.30 min\n",
      "Total Training Time: 1.30 min\n",
      "Training Loss: 231.43\n",
      "Test Loss: 238.04\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "minibatch_cost = []\n",
    "epoch_cost = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(device)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        outputs = model(features)\n",
    "        \n",
    "        cost = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        minibatch_cost.append(cost)\n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 1000:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "       \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        cost = compute_loss(model, train_loader)\n",
    "        epoch_cost.append(cost)\n",
    "        print('Epoch: %03d/%03d Train Loss: %.4f' % (\n",
    "                epoch+1, num_epochs, cost))\n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "        \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "    print('Training Loss: %.2f' % compute_loss(model, train_loader))\n",
    "    print('Test Loss: %.2f' % compute_loss(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "eJ-OoOrop-Md",
    "outputId": "60902cb4-b07a-4c79-88a8-3c6a9db776ee"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the loss curve\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(len(epoch_cost)), epoch_cost)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross entropy loss\")\n",
    "plt.title(\"Loss of training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrEfo7CmO9oW"
   },
   "source": [
    "## 5. Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jI94TtqKqjPi"
   },
   "source": [
    "### Model B: 1 Hidden Layer Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6M3g9GkqjPi"
   },
   "source": [
    "### Create model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-r_h226iqjPj",
    "outputId": "ff982517-295c-4379-82e2-8ab7b08a3520"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "n_iters = 30000\n",
    "num_epochs = n_iters / (len(train_data) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "print(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "id": "tgIAa-G9qjPl"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # create your dataloader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "id": "hrkRGlAzqjPl"
   },
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, output_dim):\n",
    "        super(FNN, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear4 = nn.Linear(hidden_dim1, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.linear4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "id": "jO4JsgY6qjPl"
   },
   "outputs": [],
   "source": [
    "input_dim = tensor_x.shape[1]\n",
    "output_dim = 1\n",
    "hidden_dim1 = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "id": "fA45FbYSqjPl"
   },
   "outputs": [],
   "source": [
    "model = FNN(input_dim, hidden_dim1, output_dim)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6plUAzZ3qjPl",
    "outputId": "34d50f32-2899-48e1-b169-45b2b21727fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 256]           2,816\n",
      "           Dropout-2               [-1, 1, 256]               0\n",
      "              ReLU-3               [-1, 1, 256]               0\n",
      "            Linear-4                 [-1, 1, 1]             257\n",
      "================================================================\n",
      "Total params: 3,073\n",
      "Trainable params: 3,073\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1,tensor_x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "id": "mALhV4pSqjPl"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 0.0001\n",
    "LAMBDA = 0.05\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), weight_decay=LAMBDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSqPq_7RqjPm"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z1AJiEn9qjPm",
    "outputId": "2ed140ea-24e5-41c9-f296-4a01a9456e54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/513 | Batch 000/059 | Loss: 6962.6250\n",
      "Epoch: 001/513 Train Loss: 427.7682\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 002/513 | Batch 000/059 | Loss: 308.2392\n",
      "Epoch: 002/513 Train Loss: 364.5264\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 003/513 | Batch 000/059 | Loss: 184.4066\n",
      "Epoch: 003/513 Train Loss: 355.5832\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 004/513 | Batch 000/059 | Loss: 462.9712\n",
      "Epoch: 004/513 Train Loss: 351.3162\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 005/513 | Batch 000/059 | Loss: 315.5490\n",
      "Epoch: 005/513 Train Loss: 348.5631\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 006/513 | Batch 000/059 | Loss: 396.7807\n",
      "Epoch: 006/513 Train Loss: 346.9019\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 007/513 | Batch 000/059 | Loss: 247.3066\n",
      "Epoch: 007/513 Train Loss: 344.6237\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 008/513 | Batch 000/059 | Loss: 299.2118\n",
      "Epoch: 008/513 Train Loss: 343.7583\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 009/513 | Batch 000/059 | Loss: 588.6263\n",
      "Epoch: 009/513 Train Loss: 341.3760\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 010/513 | Batch 000/059 | Loss: 564.2090\n",
      "Epoch: 010/513 Train Loss: 339.9935\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 011/513 | Batch 000/059 | Loss: 452.0656\n",
      "Epoch: 011/513 Train Loss: 338.6529\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 012/513 | Batch 000/059 | Loss: 379.5130\n",
      "Epoch: 012/513 Train Loss: 337.4077\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 013/513 | Batch 000/059 | Loss: 440.6339\n",
      "Epoch: 013/513 Train Loss: 336.5575\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 014/513 | Batch 000/059 | Loss: 333.6814\n",
      "Epoch: 014/513 Train Loss: 336.1353\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 015/513 | Batch 000/059 | Loss: 364.9073\n",
      "Epoch: 015/513 Train Loss: 333.8431\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 016/513 | Batch 000/059 | Loss: 275.1899\n",
      "Epoch: 016/513 Train Loss: 332.4079\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 017/513 | Batch 000/059 | Loss: 485.9779\n",
      "Epoch: 017/513 Train Loss: 331.5601\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 018/513 | Batch 000/059 | Loss: 313.0252\n",
      "Epoch: 018/513 Train Loss: 330.4804\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 019/513 | Batch 000/059 | Loss: 267.1227\n",
      "Epoch: 019/513 Train Loss: 329.2343\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 020/513 | Batch 000/059 | Loss: 302.3174\n",
      "Epoch: 020/513 Train Loss: 329.4587\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 021/513 | Batch 000/059 | Loss: 314.2797\n",
      "Epoch: 021/513 Train Loss: 327.6728\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 022/513 | Batch 000/059 | Loss: 373.9564\n",
      "Epoch: 022/513 Train Loss: 326.2487\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 023/513 | Batch 000/059 | Loss: 377.6274\n",
      "Epoch: 023/513 Train Loss: 325.8516\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 024/513 | Batch 000/059 | Loss: 341.0043\n",
      "Epoch: 024/513 Train Loss: 324.6449\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 025/513 | Batch 000/059 | Loss: 374.9218\n",
      "Epoch: 025/513 Train Loss: 324.0285\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 026/513 | Batch 000/059 | Loss: 635.3700\n",
      "Epoch: 026/513 Train Loss: 322.5566\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 027/513 | Batch 000/059 | Loss: 293.0444\n",
      "Epoch: 027/513 Train Loss: 321.9510\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 028/513 | Batch 000/059 | Loss: 330.3546\n",
      "Epoch: 028/513 Train Loss: 320.8238\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 029/513 | Batch 000/059 | Loss: 330.6440\n",
      "Epoch: 029/513 Train Loss: 320.1666\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 030/513 | Batch 000/059 | Loss: 293.5876\n",
      "Epoch: 030/513 Train Loss: 319.5270\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 031/513 | Batch 000/059 | Loss: 307.2043\n",
      "Epoch: 031/513 Train Loss: 318.2925\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 032/513 | Batch 000/059 | Loss: 223.9887\n",
      "Epoch: 032/513 Train Loss: 317.5142\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 033/513 | Batch 000/059 | Loss: 219.0740\n",
      "Epoch: 033/513 Train Loss: 316.6111\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 034/513 | Batch 000/059 | Loss: 284.0409\n",
      "Epoch: 034/513 Train Loss: 316.2321\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 035/513 | Batch 000/059 | Loss: 276.1648\n",
      "Epoch: 035/513 Train Loss: 315.1928\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 036/513 | Batch 000/059 | Loss: 529.6976\n",
      "Epoch: 036/513 Train Loss: 314.5501\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 037/513 | Batch 000/059 | Loss: 269.8927\n",
      "Epoch: 037/513 Train Loss: 313.4405\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 038/513 | Batch 000/059 | Loss: 332.7500\n",
      "Epoch: 038/513 Train Loss: 313.0445\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 039/513 | Batch 000/059 | Loss: 235.6477\n",
      "Epoch: 039/513 Train Loss: 312.0325\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 040/513 | Batch 000/059 | Loss: 223.4179\n",
      "Epoch: 040/513 Train Loss: 311.1454\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 041/513 | Batch 000/059 | Loss: 263.1925\n",
      "Epoch: 041/513 Train Loss: 310.3852\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 042/513 | Batch 000/059 | Loss: 201.7473\n",
      "Epoch: 042/513 Train Loss: 309.6516\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 043/513 | Batch 000/059 | Loss: 302.7134\n",
      "Epoch: 043/513 Train Loss: 309.1807\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 044/513 | Batch 000/059 | Loss: 471.4805\n",
      "Epoch: 044/513 Train Loss: 308.2040\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 045/513 | Batch 000/059 | Loss: 291.1903\n",
      "Epoch: 045/513 Train Loss: 307.4511\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 046/513 | Batch 000/059 | Loss: 303.2526\n",
      "Epoch: 046/513 Train Loss: 306.6330\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 047/513 | Batch 000/059 | Loss: 465.8367\n",
      "Epoch: 047/513 Train Loss: 305.9583\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 048/513 | Batch 000/059 | Loss: 194.2894\n",
      "Epoch: 048/513 Train Loss: 305.1835\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 049/513 | Batch 000/059 | Loss: 150.8424\n",
      "Epoch: 049/513 Train Loss: 304.7537\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 050/513 | Batch 000/059 | Loss: 334.2747\n",
      "Epoch: 050/513 Train Loss: 304.7364\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 051/513 | Batch 000/059 | Loss: 438.5197\n",
      "Epoch: 051/513 Train Loss: 303.1245\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 052/513 | Batch 000/059 | Loss: 407.2916\n",
      "Epoch: 052/513 Train Loss: 302.5004\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 053/513 | Batch 000/059 | Loss: 313.6537\n",
      "Epoch: 053/513 Train Loss: 302.1264\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 054/513 | Batch 000/059 | Loss: 251.7095\n",
      "Epoch: 054/513 Train Loss: 301.2893\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 055/513 | Batch 000/059 | Loss: 346.1462\n",
      "Epoch: 055/513 Train Loss: 300.7551\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 056/513 | Batch 000/059 | Loss: 332.4149\n",
      "Epoch: 056/513 Train Loss: 299.6710\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 057/513 | Batch 000/059 | Loss: 219.7411\n",
      "Epoch: 057/513 Train Loss: 299.0652\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 058/513 | Batch 000/059 | Loss: 125.9202\n",
      "Epoch: 058/513 Train Loss: 298.2972\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 059/513 | Batch 000/059 | Loss: 203.4467\n",
      "Epoch: 059/513 Train Loss: 297.6503\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 060/513 | Batch 000/059 | Loss: 229.3122\n",
      "Epoch: 060/513 Train Loss: 297.0116\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 061/513 | Batch 000/059 | Loss: 524.7531\n",
      "Epoch: 061/513 Train Loss: 296.3774\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 062/513 | Batch 000/059 | Loss: 245.8264\n",
      "Epoch: 062/513 Train Loss: 295.8360\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 063/513 | Batch 000/059 | Loss: 329.0965\n",
      "Epoch: 063/513 Train Loss: 295.1811\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 064/513 | Batch 000/059 | Loss: 291.0090\n",
      "Epoch: 064/513 Train Loss: 294.6044\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 065/513 | Batch 000/059 | Loss: 338.9576\n",
      "Epoch: 065/513 Train Loss: 293.9437\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 066/513 | Batch 000/059 | Loss: 417.4872\n",
      "Epoch: 066/513 Train Loss: 293.2879\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 067/513 | Batch 000/059 | Loss: 500.2704\n",
      "Epoch: 067/513 Train Loss: 293.1616\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 068/513 | Batch 000/059 | Loss: 237.2882\n",
      "Epoch: 068/513 Train Loss: 292.2409\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 069/513 | Batch 000/059 | Loss: 267.6096\n",
      "Epoch: 069/513 Train Loss: 291.6082\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 070/513 | Batch 000/059 | Loss: 439.4599\n",
      "Epoch: 070/513 Train Loss: 290.8810\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 071/513 | Batch 000/059 | Loss: 186.5421\n",
      "Epoch: 071/513 Train Loss: 290.4314\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 072/513 | Batch 000/059 | Loss: 258.7056\n",
      "Epoch: 072/513 Train Loss: 289.8813\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 073/513 | Batch 000/059 | Loss: 648.8646\n",
      "Epoch: 073/513 Train Loss: 289.4329\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 074/513 | Batch 000/059 | Loss: 267.4052\n",
      "Epoch: 074/513 Train Loss: 288.4609\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 075/513 | Batch 000/059 | Loss: 241.1360\n",
      "Epoch: 075/513 Train Loss: 287.7432\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 076/513 | Batch 000/059 | Loss: 222.8717\n",
      "Epoch: 076/513 Train Loss: 287.2008\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 077/513 | Batch 000/059 | Loss: 189.4340\n",
      "Epoch: 077/513 Train Loss: 286.5613\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 078/513 | Batch 000/059 | Loss: 203.8109\n",
      "Epoch: 078/513 Train Loss: 286.2263\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 079/513 | Batch 000/059 | Loss: 214.4617\n",
      "Epoch: 079/513 Train Loss: 285.6868\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 080/513 | Batch 000/059 | Loss: 161.2861\n",
      "Epoch: 080/513 Train Loss: 284.8822\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 081/513 | Batch 000/059 | Loss: 210.6102\n",
      "Epoch: 081/513 Train Loss: 284.2737\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 082/513 | Batch 000/059 | Loss: 412.8986\n",
      "Epoch: 082/513 Train Loss: 283.8272\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 083/513 | Batch 000/059 | Loss: 177.6530\n",
      "Epoch: 083/513 Train Loss: 283.2910\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 084/513 | Batch 000/059 | Loss: 317.6400\n",
      "Epoch: 084/513 Train Loss: 283.0077\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 085/513 | Batch 000/059 | Loss: 177.1911\n",
      "Epoch: 085/513 Train Loss: 282.3852\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 086/513 | Batch 000/059 | Loss: 384.3145\n",
      "Epoch: 086/513 Train Loss: 281.6812\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 087/513 | Batch 000/059 | Loss: 322.8509\n",
      "Epoch: 087/513 Train Loss: 281.1651\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 088/513 | Batch 000/059 | Loss: 256.6846\n",
      "Epoch: 088/513 Train Loss: 280.6704\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 089/513 | Batch 000/059 | Loss: 139.0733\n",
      "Epoch: 089/513 Train Loss: 280.1183\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 090/513 | Batch 000/059 | Loss: 206.4348\n",
      "Epoch: 090/513 Train Loss: 279.8481\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 091/513 | Batch 000/059 | Loss: 162.2609\n",
      "Epoch: 091/513 Train Loss: 279.0670\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 092/513 | Batch 000/059 | Loss: 135.5758\n",
      "Epoch: 092/513 Train Loss: 278.6099\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 093/513 | Batch 000/059 | Loss: 163.8705\n",
      "Epoch: 093/513 Train Loss: 278.0953\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 094/513 | Batch 000/059 | Loss: 194.1253\n",
      "Epoch: 094/513 Train Loss: 277.6557\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 095/513 | Batch 000/059 | Loss: 154.8033\n",
      "Epoch: 095/513 Train Loss: 277.2168\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 096/513 | Batch 000/059 | Loss: 230.8054\n",
      "Epoch: 096/513 Train Loss: 276.6640\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 097/513 | Batch 000/059 | Loss: 358.6233\n",
      "Epoch: 097/513 Train Loss: 276.8048\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 098/513 | Batch 000/059 | Loss: 271.4841\n",
      "Epoch: 098/513 Train Loss: 275.9544\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 099/513 | Batch 000/059 | Loss: 191.9421\n",
      "Epoch: 099/513 Train Loss: 275.2849\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 100/513 | Batch 000/059 | Loss: 143.9906\n",
      "Epoch: 100/513 Train Loss: 274.8147\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 101/513 | Batch 000/059 | Loss: 223.2823\n",
      "Epoch: 101/513 Train Loss: 274.3571\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 102/513 | Batch 000/059 | Loss: 353.3799\n",
      "Epoch: 102/513 Train Loss: 274.1600\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 103/513 | Batch 000/059 | Loss: 480.1001\n",
      "Epoch: 103/513 Train Loss: 273.6040\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 104/513 | Batch 000/059 | Loss: 460.4164\n",
      "Epoch: 104/513 Train Loss: 273.0148\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 105/513 | Batch 000/059 | Loss: 345.5134\n",
      "Epoch: 105/513 Train Loss: 272.4948\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 106/513 | Batch 000/059 | Loss: 324.7908\n",
      "Epoch: 106/513 Train Loss: 272.0678\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 107/513 | Batch 000/059 | Loss: 274.5117\n",
      "Epoch: 107/513 Train Loss: 271.6819\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 108/513 | Batch 000/059 | Loss: 416.3033\n",
      "Epoch: 108/513 Train Loss: 271.3038\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 109/513 | Batch 000/059 | Loss: 237.8829\n",
      "Epoch: 109/513 Train Loss: 270.7493\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 110/513 | Batch 000/059 | Loss: 300.9086\n",
      "Epoch: 110/513 Train Loss: 270.4864\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 111/513 | Batch 000/059 | Loss: 333.6242\n",
      "Epoch: 111/513 Train Loss: 270.0010\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 112/513 | Batch 000/059 | Loss: 210.0065\n",
      "Epoch: 112/513 Train Loss: 269.5054\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 113/513 | Batch 000/059 | Loss: 342.4156\n",
      "Epoch: 113/513 Train Loss: 269.1802\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 114/513 | Batch 000/059 | Loss: 374.0903\n",
      "Epoch: 114/513 Train Loss: 268.8054\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 115/513 | Batch 000/059 | Loss: 242.8998\n",
      "Epoch: 115/513 Train Loss: 268.3065\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 116/513 | Batch 000/059 | Loss: 111.7011\n",
      "Epoch: 116/513 Train Loss: 267.9809\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 117/513 | Batch 000/059 | Loss: 477.6774\n",
      "Epoch: 117/513 Train Loss: 267.5977\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 118/513 | Batch 000/059 | Loss: 426.1694\n",
      "Epoch: 118/513 Train Loss: 267.2348\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 119/513 | Batch 000/059 | Loss: 338.3867\n",
      "Epoch: 119/513 Train Loss: 266.8665\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 120/513 | Batch 000/059 | Loss: 304.4830\n",
      "Epoch: 120/513 Train Loss: 266.5349\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 121/513 | Batch 000/059 | Loss: 298.2091\n",
      "Epoch: 121/513 Train Loss: 266.0705\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 122/513 | Batch 000/059 | Loss: 223.1565\n",
      "Epoch: 122/513 Train Loss: 265.6954\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 123/513 | Batch 000/059 | Loss: 244.4854\n",
      "Epoch: 123/513 Train Loss: 265.5872\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 124/513 | Batch 000/059 | Loss: 311.7499\n",
      "Epoch: 124/513 Train Loss: 264.9123\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 125/513 | Batch 000/059 | Loss: 239.3828\n",
      "Epoch: 125/513 Train Loss: 264.7354\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 126/513 | Batch 000/059 | Loss: 325.0152\n",
      "Epoch: 126/513 Train Loss: 264.4635\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 127/513 | Batch 000/059 | Loss: 297.9008\n",
      "Epoch: 127/513 Train Loss: 264.0040\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 128/513 | Batch 000/059 | Loss: 203.7689\n",
      "Epoch: 128/513 Train Loss: 263.7736\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 129/513 | Batch 000/059 | Loss: 313.8599\n",
      "Epoch: 129/513 Train Loss: 263.2621\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 130/513 | Batch 000/059 | Loss: 207.7639\n",
      "Epoch: 130/513 Train Loss: 262.9356\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 131/513 | Batch 000/059 | Loss: 238.7420\n",
      "Epoch: 131/513 Train Loss: 262.5424\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 132/513 | Batch 000/059 | Loss: 438.9505\n",
      "Epoch: 132/513 Train Loss: 262.2574\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 133/513 | Batch 000/059 | Loss: 220.8062\n",
      "Epoch: 133/513 Train Loss: 261.9774\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 134/513 | Batch 000/059 | Loss: 326.6099\n",
      "Epoch: 134/513 Train Loss: 261.5824\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 135/513 | Batch 000/059 | Loss: 305.1682\n",
      "Epoch: 135/513 Train Loss: 261.2539\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 136/513 | Batch 000/059 | Loss: 329.6446\n",
      "Epoch: 136/513 Train Loss: 260.9926\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 137/513 | Batch 000/059 | Loss: 224.9436\n",
      "Epoch: 137/513 Train Loss: 260.7920\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 138/513 | Batch 000/059 | Loss: 284.8470\n",
      "Epoch: 138/513 Train Loss: 260.5446\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 139/513 | Batch 000/059 | Loss: 482.5204\n",
      "Epoch: 139/513 Train Loss: 260.3035\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 140/513 | Batch 000/059 | Loss: 252.4353\n",
      "Epoch: 140/513 Train Loss: 259.6662\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 141/513 | Batch 000/059 | Loss: 170.7882\n",
      "Epoch: 141/513 Train Loss: 259.4068\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 142/513 | Batch 000/059 | Loss: 406.5882\n",
      "Epoch: 142/513 Train Loss: 259.0518\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 143/513 | Batch 000/059 | Loss: 134.0370\n",
      "Epoch: 143/513 Train Loss: 258.7713\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 144/513 | Batch 000/059 | Loss: 328.7009\n",
      "Epoch: 144/513 Train Loss: 258.4999\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 145/513 | Batch 000/059 | Loss: 366.2202\n",
      "Epoch: 145/513 Train Loss: 258.2126\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 146/513 | Batch 000/059 | Loss: 311.2004\n",
      "Epoch: 146/513 Train Loss: 257.8979\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 147/513 | Batch 000/059 | Loss: 339.3250\n",
      "Epoch: 147/513 Train Loss: 257.7703\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 148/513 | Batch 000/059 | Loss: 150.7247\n",
      "Epoch: 148/513 Train Loss: 257.3893\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 149/513 | Batch 000/059 | Loss: 156.1652\n",
      "Epoch: 149/513 Train Loss: 257.2239\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 150/513 | Batch 000/059 | Loss: 157.2907\n",
      "Epoch: 150/513 Train Loss: 256.9633\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 151/513 | Batch 000/059 | Loss: 445.8330\n",
      "Epoch: 151/513 Train Loss: 257.2233\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 152/513 | Batch 000/059 | Loss: 242.3778\n",
      "Epoch: 152/513 Train Loss: 256.2580\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 153/513 | Batch 000/059 | Loss: 313.3888\n",
      "Epoch: 153/513 Train Loss: 256.1162\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 154/513 | Batch 000/059 | Loss: 181.8963\n",
      "Epoch: 154/513 Train Loss: 255.7191\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 155/513 | Batch 000/059 | Loss: 247.0635\n",
      "Epoch: 155/513 Train Loss: 255.6196\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 156/513 | Batch 000/059 | Loss: 265.4948\n",
      "Epoch: 156/513 Train Loss: 255.4428\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 157/513 | Batch 000/059 | Loss: 381.7892\n",
      "Epoch: 157/513 Train Loss: 255.3727\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 158/513 | Batch 000/059 | Loss: 249.9551\n",
      "Epoch: 158/513 Train Loss: 254.8148\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 159/513 | Batch 000/059 | Loss: 194.6320\n",
      "Epoch: 159/513 Train Loss: 254.5548\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 160/513 | Batch 000/059 | Loss: 281.6097\n",
      "Epoch: 160/513 Train Loss: 254.5125\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 161/513 | Batch 000/059 | Loss: 269.1087\n",
      "Epoch: 161/513 Train Loss: 254.0401\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 162/513 | Batch 000/059 | Loss: 262.6111\n",
      "Epoch: 162/513 Train Loss: 253.7865\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 163/513 | Batch 000/059 | Loss: 197.1559\n",
      "Epoch: 163/513 Train Loss: 253.5848\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 164/513 | Batch 000/059 | Loss: 122.7820\n",
      "Epoch: 164/513 Train Loss: 253.2176\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 165/513 | Batch 000/059 | Loss: 228.5085\n",
      "Epoch: 165/513 Train Loss: 252.9909\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 166/513 | Batch 000/059 | Loss: 211.0849\n",
      "Epoch: 166/513 Train Loss: 252.7293\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 167/513 | Batch 000/059 | Loss: 333.7779\n",
      "Epoch: 167/513 Train Loss: 252.5684\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 168/513 | Batch 000/059 | Loss: 275.9399\n",
      "Epoch: 168/513 Train Loss: 252.3654\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 169/513 | Batch 000/059 | Loss: 444.6241\n",
      "Epoch: 169/513 Train Loss: 252.1483\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 170/513 | Batch 000/059 | Loss: 265.3675\n",
      "Epoch: 170/513 Train Loss: 251.9507\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 171/513 | Batch 000/059 | Loss: 242.0076\n",
      "Epoch: 171/513 Train Loss: 251.7388\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 172/513 | Batch 000/059 | Loss: 229.3170\n",
      "Epoch: 172/513 Train Loss: 251.5938\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 173/513 | Batch 000/059 | Loss: 196.1069\n",
      "Epoch: 173/513 Train Loss: 251.3125\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 174/513 | Batch 000/059 | Loss: 414.2344\n",
      "Epoch: 174/513 Train Loss: 251.0870\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 175/513 | Batch 000/059 | Loss: 536.9197\n",
      "Epoch: 175/513 Train Loss: 250.9020\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 176/513 | Batch 000/059 | Loss: 227.2686\n",
      "Epoch: 176/513 Train Loss: 250.5998\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 177/513 | Batch 000/059 | Loss: 339.7057\n",
      "Epoch: 177/513 Train Loss: 250.7219\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 178/513 | Batch 000/059 | Loss: 238.5837\n",
      "Epoch: 178/513 Train Loss: 250.2691\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 179/513 | Batch 000/059 | Loss: 306.6331\n",
      "Epoch: 179/513 Train Loss: 250.0167\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 180/513 | Batch 000/059 | Loss: 224.0028\n",
      "Epoch: 180/513 Train Loss: 249.8352\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 181/513 | Batch 000/059 | Loss: 207.2581\n",
      "Epoch: 181/513 Train Loss: 249.7725\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 182/513 | Batch 000/059 | Loss: 119.2687\n",
      "Epoch: 182/513 Train Loss: 249.7113\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 183/513 | Batch 000/059 | Loss: 272.8752\n",
      "Epoch: 183/513 Train Loss: 249.3245\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 184/513 | Batch 000/059 | Loss: 231.1721\n",
      "Epoch: 184/513 Train Loss: 249.2823\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 185/513 | Batch 000/059 | Loss: 320.3199\n",
      "Epoch: 185/513 Train Loss: 249.0450\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 186/513 | Batch 000/059 | Loss: 365.5045\n",
      "Epoch: 186/513 Train Loss: 248.7532\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 187/513 | Batch 000/059 | Loss: 264.9702\n",
      "Epoch: 187/513 Train Loss: 248.6108\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 188/513 | Batch 000/059 | Loss: 360.4861\n",
      "Epoch: 188/513 Train Loss: 248.5062\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 189/513 | Batch 000/059 | Loss: 228.1564\n",
      "Epoch: 189/513 Train Loss: 248.3197\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 190/513 | Batch 000/059 | Loss: 320.2862\n",
      "Epoch: 190/513 Train Loss: 248.2051\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 191/513 | Batch 000/059 | Loss: 198.7190\n",
      "Epoch: 191/513 Train Loss: 247.9336\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 192/513 | Batch 000/059 | Loss: 351.7462\n",
      "Epoch: 192/513 Train Loss: 247.8524\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 193/513 | Batch 000/059 | Loss: 226.3372\n",
      "Epoch: 193/513 Train Loss: 247.5581\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 194/513 | Batch 000/059 | Loss: 404.2914\n",
      "Epoch: 194/513 Train Loss: 247.4166\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 195/513 | Batch 000/059 | Loss: 323.0576\n",
      "Epoch: 195/513 Train Loss: 247.2065\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 196/513 | Batch 000/059 | Loss: 206.6268\n",
      "Epoch: 196/513 Train Loss: 247.0471\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 197/513 | Batch 000/059 | Loss: 398.1416\n",
      "Epoch: 197/513 Train Loss: 246.9088\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 198/513 | Batch 000/059 | Loss: 188.0693\n",
      "Epoch: 198/513 Train Loss: 246.7448\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 199/513 | Batch 000/059 | Loss: 187.3252\n",
      "Epoch: 199/513 Train Loss: 246.6495\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 200/513 | Batch 000/059 | Loss: 263.3825\n",
      "Epoch: 200/513 Train Loss: 246.4132\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 201/513 | Batch 000/059 | Loss: 143.8171\n",
      "Epoch: 201/513 Train Loss: 246.2517\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 202/513 | Batch 000/059 | Loss: 234.4507\n",
      "Epoch: 202/513 Train Loss: 246.1940\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 203/513 | Batch 000/059 | Loss: 267.0545\n",
      "Epoch: 203/513 Train Loss: 245.9311\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 204/513 | Batch 000/059 | Loss: 190.8068\n",
      "Epoch: 204/513 Train Loss: 245.7848\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 205/513 | Batch 000/059 | Loss: 149.7537\n",
      "Epoch: 205/513 Train Loss: 245.6929\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 206/513 | Batch 000/059 | Loss: 287.5768\n",
      "Epoch: 206/513 Train Loss: 245.4426\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 207/513 | Batch 000/059 | Loss: 192.8769\n",
      "Epoch: 207/513 Train Loss: 245.3272\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 208/513 | Batch 000/059 | Loss: 184.1393\n",
      "Epoch: 208/513 Train Loss: 245.2030\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 209/513 | Batch 000/059 | Loss: 283.2074\n",
      "Epoch: 209/513 Train Loss: 245.0377\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 210/513 | Batch 000/059 | Loss: 361.1220\n",
      "Epoch: 210/513 Train Loss: 245.1370\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 211/513 | Batch 000/059 | Loss: 243.2320\n",
      "Epoch: 211/513 Train Loss: 244.8119\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 212/513 | Batch 000/059 | Loss: 219.9165\n",
      "Epoch: 212/513 Train Loss: 245.0070\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 213/513 | Batch 000/059 | Loss: 311.7223\n",
      "Epoch: 213/513 Train Loss: 244.4102\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 214/513 | Batch 000/059 | Loss: 224.0473\n",
      "Epoch: 214/513 Train Loss: 244.3322\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 215/513 | Batch 000/059 | Loss: 272.8049\n",
      "Epoch: 215/513 Train Loss: 244.3168\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 216/513 | Batch 000/059 | Loss: 164.7435\n",
      "Epoch: 216/513 Train Loss: 244.3034\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 217/513 | Batch 000/059 | Loss: 269.4459\n",
      "Epoch: 217/513 Train Loss: 244.0547\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 218/513 | Batch 000/059 | Loss: 230.5285\n",
      "Epoch: 218/513 Train Loss: 243.7936\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 219/513 | Batch 000/059 | Loss: 189.3850\n",
      "Epoch: 219/513 Train Loss: 243.7314\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 220/513 | Batch 000/059 | Loss: 376.0693\n",
      "Epoch: 220/513 Train Loss: 243.4890\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 221/513 | Batch 000/059 | Loss: 259.7851\n",
      "Epoch: 221/513 Train Loss: 243.3939\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 222/513 | Batch 000/059 | Loss: 274.8419\n",
      "Epoch: 222/513 Train Loss: 243.3404\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 223/513 | Batch 000/059 | Loss: 237.9661\n",
      "Epoch: 223/513 Train Loss: 243.3887\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 224/513 | Batch 000/059 | Loss: 134.3664\n",
      "Epoch: 224/513 Train Loss: 243.1544\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 225/513 | Batch 000/059 | Loss: 200.4175\n",
      "Epoch: 225/513 Train Loss: 242.9267\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 226/513 | Batch 000/059 | Loss: 396.2607\n",
      "Epoch: 226/513 Train Loss: 242.9575\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 227/513 | Batch 000/059 | Loss: 196.3089\n",
      "Epoch: 227/513 Train Loss: 242.7654\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 228/513 | Batch 000/059 | Loss: 279.8719\n",
      "Epoch: 228/513 Train Loss: 242.6306\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 229/513 | Batch 000/059 | Loss: 192.2340\n",
      "Epoch: 229/513 Train Loss: 242.4542\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 230/513 | Batch 000/059 | Loss: 280.4783\n",
      "Epoch: 230/513 Train Loss: 242.5328\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 231/513 | Batch 000/059 | Loss: 249.4417\n",
      "Epoch: 231/513 Train Loss: 242.2938\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 232/513 | Batch 000/059 | Loss: 277.1597\n",
      "Epoch: 232/513 Train Loss: 242.1430\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 233/513 | Batch 000/059 | Loss: 257.4519\n",
      "Epoch: 233/513 Train Loss: 242.1569\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 234/513 | Batch 000/059 | Loss: 223.4176\n",
      "Epoch: 234/513 Train Loss: 242.1697\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 235/513 | Batch 000/059 | Loss: 256.9804\n",
      "Epoch: 235/513 Train Loss: 241.9474\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 236/513 | Batch 000/059 | Loss: 202.9655\n",
      "Epoch: 236/513 Train Loss: 241.9184\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 237/513 | Batch 000/059 | Loss: 239.2789\n",
      "Epoch: 237/513 Train Loss: 241.6902\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 238/513 | Batch 000/059 | Loss: 342.1111\n",
      "Epoch: 238/513 Train Loss: 241.5683\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 239/513 | Batch 000/059 | Loss: 285.3810\n",
      "Epoch: 239/513 Train Loss: 241.3968\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 240/513 | Batch 000/059 | Loss: 328.3625\n",
      "Epoch: 240/513 Train Loss: 241.3540\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 241/513 | Batch 000/059 | Loss: 218.4285\n",
      "Epoch: 241/513 Train Loss: 241.2658\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 242/513 | Batch 000/059 | Loss: 204.9079\n",
      "Epoch: 242/513 Train Loss: 241.1446\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 243/513 | Batch 000/059 | Loss: 241.7029\n",
      "Epoch: 243/513 Train Loss: 240.9605\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 244/513 | Batch 000/059 | Loss: 287.4677\n",
      "Epoch: 244/513 Train Loss: 240.9001\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 245/513 | Batch 000/059 | Loss: 129.1073\n",
      "Epoch: 245/513 Train Loss: 240.8039\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 246/513 | Batch 000/059 | Loss: 236.1557\n",
      "Epoch: 246/513 Train Loss: 240.7196\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 247/513 | Batch 000/059 | Loss: 255.4708\n",
      "Epoch: 247/513 Train Loss: 240.6567\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 248/513 | Batch 000/059 | Loss: 315.6967\n",
      "Epoch: 248/513 Train Loss: 240.4912\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 249/513 | Batch 000/059 | Loss: 202.9363\n",
      "Epoch: 249/513 Train Loss: 240.4596\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 250/513 | Batch 000/059 | Loss: 192.3160\n",
      "Epoch: 250/513 Train Loss: 240.4508\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 251/513 | Batch 000/059 | Loss: 326.5523\n",
      "Epoch: 251/513 Train Loss: 240.3180\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 252/513 | Batch 000/059 | Loss: 111.1805\n",
      "Epoch: 252/513 Train Loss: 240.1749\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 253/513 | Batch 000/059 | Loss: 285.1193\n",
      "Epoch: 253/513 Train Loss: 240.1721\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 254/513 | Batch 000/059 | Loss: 269.0270\n",
      "Epoch: 254/513 Train Loss: 240.0405\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 255/513 | Batch 000/059 | Loss: 201.0400\n",
      "Epoch: 255/513 Train Loss: 239.8920\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 256/513 | Batch 000/059 | Loss: 204.0159\n",
      "Epoch: 256/513 Train Loss: 239.8359\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 257/513 | Batch 000/059 | Loss: 342.9568\n",
      "Epoch: 257/513 Train Loss: 239.7715\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 258/513 | Batch 000/059 | Loss: 199.5898\n",
      "Epoch: 258/513 Train Loss: 239.6400\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 259/513 | Batch 000/059 | Loss: 384.7550\n",
      "Epoch: 259/513 Train Loss: 239.6054\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 260/513 | Batch 000/059 | Loss: 383.2224\n",
      "Epoch: 260/513 Train Loss: 239.9870\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 261/513 | Batch 000/059 | Loss: 195.3773\n",
      "Epoch: 261/513 Train Loss: 239.3304\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 262/513 | Batch 000/059 | Loss: 238.1506\n",
      "Epoch: 262/513 Train Loss: 239.4144\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 263/513 | Batch 000/059 | Loss: 246.4963\n",
      "Epoch: 263/513 Train Loss: 239.2682\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 264/513 | Batch 000/059 | Loss: 271.2118\n",
      "Epoch: 264/513 Train Loss: 239.1762\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 265/513 | Batch 000/059 | Loss: 364.5158\n",
      "Epoch: 265/513 Train Loss: 239.0565\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 266/513 | Batch 000/059 | Loss: 232.2942\n",
      "Epoch: 266/513 Train Loss: 239.0957\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 267/513 | Batch 000/059 | Loss: 465.5600\n",
      "Epoch: 267/513 Train Loss: 238.9473\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 268/513 | Batch 000/059 | Loss: 276.0060\n",
      "Epoch: 268/513 Train Loss: 238.8564\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 269/513 | Batch 000/059 | Loss: 290.8906\n",
      "Epoch: 269/513 Train Loss: 238.9152\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 270/513 | Batch 000/059 | Loss: 414.7529\n",
      "Epoch: 270/513 Train Loss: 238.8880\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 271/513 | Batch 000/059 | Loss: 259.1740\n",
      "Epoch: 271/513 Train Loss: 238.7434\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 272/513 | Batch 000/059 | Loss: 261.8586\n",
      "Epoch: 272/513 Train Loss: 238.6860\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 273/513 | Batch 000/059 | Loss: 259.6974\n",
      "Epoch: 273/513 Train Loss: 238.5980\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 274/513 | Batch 000/059 | Loss: 310.7964\n",
      "Epoch: 274/513 Train Loss: 238.5333\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 275/513 | Batch 000/059 | Loss: 264.5100\n",
      "Epoch: 275/513 Train Loss: 238.6156\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 276/513 | Batch 000/059 | Loss: 230.5635\n",
      "Epoch: 276/513 Train Loss: 238.4359\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 277/513 | Batch 000/059 | Loss: 281.4941\n",
      "Epoch: 277/513 Train Loss: 238.4432\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 278/513 | Batch 000/059 | Loss: 269.0473\n",
      "Epoch: 278/513 Train Loss: 238.3256\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 279/513 | Batch 000/059 | Loss: 372.3141\n",
      "Epoch: 279/513 Train Loss: 238.2767\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 280/513 | Batch 000/059 | Loss: 274.2305\n",
      "Epoch: 280/513 Train Loss: 238.2613\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 281/513 | Batch 000/059 | Loss: 237.4948\n",
      "Epoch: 281/513 Train Loss: 238.1280\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 282/513 | Batch 000/059 | Loss: 152.1215\n",
      "Epoch: 282/513 Train Loss: 238.1542\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 283/513 | Batch 000/059 | Loss: 136.8911\n",
      "Epoch: 283/513 Train Loss: 237.9239\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 284/513 | Batch 000/059 | Loss: 211.0710\n",
      "Epoch: 284/513 Train Loss: 238.0429\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 285/513 | Batch 000/059 | Loss: 256.8827\n",
      "Epoch: 285/513 Train Loss: 238.0615\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 286/513 | Batch 000/059 | Loss: 235.9189\n",
      "Epoch: 286/513 Train Loss: 237.8632\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 287/513 | Batch 000/059 | Loss: 306.9565\n",
      "Epoch: 287/513 Train Loss: 237.6630\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 288/513 | Batch 000/059 | Loss: 153.0455\n",
      "Epoch: 288/513 Train Loss: 237.5697\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 289/513 | Batch 000/059 | Loss: 385.3513\n",
      "Epoch: 289/513 Train Loss: 237.5034\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 290/513 | Batch 000/059 | Loss: 331.6878\n",
      "Epoch: 290/513 Train Loss: 237.4346\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 291/513 | Batch 000/059 | Loss: 239.8732\n",
      "Epoch: 291/513 Train Loss: 237.3469\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 292/513 | Batch 000/059 | Loss: 236.7110\n",
      "Epoch: 292/513 Train Loss: 237.3927\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 293/513 | Batch 000/059 | Loss: 352.1118\n",
      "Epoch: 293/513 Train Loss: 237.2714\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 294/513 | Batch 000/059 | Loss: 246.0784\n",
      "Epoch: 294/513 Train Loss: 237.1501\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 295/513 | Batch 000/059 | Loss: 145.8986\n",
      "Epoch: 295/513 Train Loss: 237.0920\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 296/513 | Batch 000/059 | Loss: 247.4926\n",
      "Epoch: 296/513 Train Loss: 237.0393\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 297/513 | Batch 000/059 | Loss: 359.2873\n",
      "Epoch: 297/513 Train Loss: 236.9948\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 298/513 | Batch 000/059 | Loss: 326.7647\n",
      "Epoch: 298/513 Train Loss: 236.9740\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 299/513 | Batch 000/059 | Loss: 212.0803\n",
      "Epoch: 299/513 Train Loss: 236.9361\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 300/513 | Batch 000/059 | Loss: 193.1814\n",
      "Epoch: 300/513 Train Loss: 236.8618\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 301/513 | Batch 000/059 | Loss: 221.2643\n",
      "Epoch: 301/513 Train Loss: 236.8167\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 302/513 | Batch 000/059 | Loss: 188.1437\n",
      "Epoch: 302/513 Train Loss: 236.9522\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 303/513 | Batch 000/059 | Loss: 250.5057\n",
      "Epoch: 303/513 Train Loss: 236.7695\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 304/513 | Batch 000/059 | Loss: 166.9192\n",
      "Epoch: 304/513 Train Loss: 236.6935\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 305/513 | Batch 000/059 | Loss: 287.0836\n",
      "Epoch: 305/513 Train Loss: 236.7613\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 306/513 | Batch 000/059 | Loss: 94.2253\n",
      "Epoch: 306/513 Train Loss: 236.6332\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 307/513 | Batch 000/059 | Loss: 245.7750\n",
      "Epoch: 307/513 Train Loss: 236.6947\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 308/513 | Batch 000/059 | Loss: 399.3929\n",
      "Epoch: 308/513 Train Loss: 236.6791\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 309/513 | Batch 000/059 | Loss: 401.9819\n",
      "Epoch: 309/513 Train Loss: 236.4017\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 310/513 | Batch 000/059 | Loss: 249.2592\n",
      "Epoch: 310/513 Train Loss: 236.4892\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 311/513 | Batch 000/059 | Loss: 225.3143\n",
      "Epoch: 311/513 Train Loss: 236.2338\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 312/513 | Batch 000/059 | Loss: 271.2393\n",
      "Epoch: 312/513 Train Loss: 236.1905\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 313/513 | Batch 000/059 | Loss: 108.9262\n",
      "Epoch: 313/513 Train Loss: 236.2232\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 314/513 | Batch 000/059 | Loss: 349.7632\n",
      "Epoch: 314/513 Train Loss: 236.1444\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 315/513 | Batch 000/059 | Loss: 220.4960\n",
      "Epoch: 315/513 Train Loss: 236.0350\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 316/513 | Batch 000/059 | Loss: 218.2826\n",
      "Epoch: 316/513 Train Loss: 235.9917\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 317/513 | Batch 000/059 | Loss: 206.1096\n",
      "Epoch: 317/513 Train Loss: 235.9562\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 318/513 | Batch 000/059 | Loss: 207.1387\n",
      "Epoch: 318/513 Train Loss: 235.9920\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 319/513 | Batch 000/059 | Loss: 131.3994\n",
      "Epoch: 319/513 Train Loss: 235.9036\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 320/513 | Batch 000/059 | Loss: 235.0247\n",
      "Epoch: 320/513 Train Loss: 235.8391\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 321/513 | Batch 000/059 | Loss: 129.9223\n",
      "Epoch: 321/513 Train Loss: 235.8762\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 322/513 | Batch 000/059 | Loss: 303.7926\n",
      "Epoch: 322/513 Train Loss: 235.7190\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 323/513 | Batch 000/059 | Loss: 286.6002\n",
      "Epoch: 323/513 Train Loss: 235.6367\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 324/513 | Batch 000/059 | Loss: 236.1222\n",
      "Epoch: 324/513 Train Loss: 235.6187\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 325/513 | Batch 000/059 | Loss: 289.3148\n",
      "Epoch: 325/513 Train Loss: 235.6564\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 326/513 | Batch 000/059 | Loss: 284.0439\n",
      "Epoch: 326/513 Train Loss: 235.8199\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 327/513 | Batch 000/059 | Loss: 409.8668\n",
      "Epoch: 327/513 Train Loss: 235.6243\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 328/513 | Batch 000/059 | Loss: 304.6462\n",
      "Epoch: 328/513 Train Loss: 235.6084\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 329/513 | Batch 000/059 | Loss: 226.0127\n",
      "Epoch: 329/513 Train Loss: 235.4449\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 330/513 | Batch 000/059 | Loss: 226.8514\n",
      "Epoch: 330/513 Train Loss: 235.4160\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 331/513 | Batch 000/059 | Loss: 297.6573\n",
      "Epoch: 331/513 Train Loss: 235.3873\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 332/513 | Batch 000/059 | Loss: 257.8700\n",
      "Epoch: 332/513 Train Loss: 235.6392\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 333/513 | Batch 000/059 | Loss: 220.3088\n",
      "Epoch: 333/513 Train Loss: 235.3098\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 334/513 | Batch 000/059 | Loss: 134.7631\n",
      "Epoch: 334/513 Train Loss: 235.2521\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 335/513 | Batch 000/059 | Loss: 299.1898\n",
      "Epoch: 335/513 Train Loss: 235.2479\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 336/513 | Batch 000/059 | Loss: 172.3920\n",
      "Epoch: 336/513 Train Loss: 235.2154\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 337/513 | Batch 000/059 | Loss: 197.2207\n",
      "Epoch: 337/513 Train Loss: 235.1684\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 338/513 | Batch 000/059 | Loss: 277.0836\n",
      "Epoch: 338/513 Train Loss: 235.2187\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 339/513 | Batch 000/059 | Loss: 355.6145\n",
      "Epoch: 339/513 Train Loss: 235.0653\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 340/513 | Batch 000/059 | Loss: 143.4392\n",
      "Epoch: 340/513 Train Loss: 235.0233\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 341/513 | Batch 000/059 | Loss: 277.4339\n",
      "Epoch: 341/513 Train Loss: 234.9416\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 342/513 | Batch 000/059 | Loss: 269.7829\n",
      "Epoch: 342/513 Train Loss: 235.1130\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 343/513 | Batch 000/059 | Loss: 307.6530\n",
      "Epoch: 343/513 Train Loss: 234.9121\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 344/513 | Batch 000/059 | Loss: 180.1410\n",
      "Epoch: 344/513 Train Loss: 234.8609\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 345/513 | Batch 000/059 | Loss: 197.5106\n",
      "Epoch: 345/513 Train Loss: 234.7930\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 346/513 | Batch 000/059 | Loss: 304.3040\n",
      "Epoch: 346/513 Train Loss: 234.7213\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 347/513 | Batch 000/059 | Loss: 358.7434\n",
      "Epoch: 347/513 Train Loss: 234.6978\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 348/513 | Batch 000/059 | Loss: 452.6088\n",
      "Epoch: 348/513 Train Loss: 234.6902\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 349/513 | Batch 000/059 | Loss: 331.2304\n",
      "Epoch: 349/513 Train Loss: 234.7297\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 350/513 | Batch 000/059 | Loss: 172.4303\n",
      "Epoch: 350/513 Train Loss: 234.6652\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 351/513 | Batch 000/059 | Loss: 174.0685\n",
      "Epoch: 351/513 Train Loss: 234.6497\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 352/513 | Batch 000/059 | Loss: 195.9175\n",
      "Epoch: 352/513 Train Loss: 234.5851\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 353/513 | Batch 000/059 | Loss: 198.4681\n",
      "Epoch: 353/513 Train Loss: 234.5889\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 354/513 | Batch 000/059 | Loss: 260.8553\n",
      "Epoch: 354/513 Train Loss: 234.5360\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 355/513 | Batch 000/059 | Loss: 472.0881\n",
      "Epoch: 355/513 Train Loss: 234.5023\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 356/513 | Batch 000/059 | Loss: 223.2194\n",
      "Epoch: 356/513 Train Loss: 234.4842\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 357/513 | Batch 000/059 | Loss: 251.8240\n",
      "Epoch: 357/513 Train Loss: 234.4380\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 358/513 | Batch 000/059 | Loss: 259.0773\n",
      "Epoch: 358/513 Train Loss: 234.4214\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 359/513 | Batch 000/059 | Loss: 223.9177\n",
      "Epoch: 359/513 Train Loss: 234.4377\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 360/513 | Batch 000/059 | Loss: 389.9793\n",
      "Epoch: 360/513 Train Loss: 234.3364\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 361/513 | Batch 000/059 | Loss: 129.0553\n",
      "Epoch: 361/513 Train Loss: 234.3878\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 362/513 | Batch 000/059 | Loss: 345.3801\n",
      "Epoch: 362/513 Train Loss: 234.3382\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 363/513 | Batch 000/059 | Loss: 273.7728\n",
      "Epoch: 363/513 Train Loss: 234.2563\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 364/513 | Batch 000/059 | Loss: 287.4235\n",
      "Epoch: 364/513 Train Loss: 234.2670\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 365/513 | Batch 000/059 | Loss: 270.2974\n",
      "Epoch: 365/513 Train Loss: 234.2139\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 366/513 | Batch 000/059 | Loss: 194.1028\n",
      "Epoch: 366/513 Train Loss: 234.3180\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 367/513 | Batch 000/059 | Loss: 202.3862\n",
      "Epoch: 367/513 Train Loss: 234.1611\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 368/513 | Batch 000/059 | Loss: 330.0318\n",
      "Epoch: 368/513 Train Loss: 234.1449\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 369/513 | Batch 000/059 | Loss: 353.3576\n",
      "Epoch: 369/513 Train Loss: 234.1293\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 370/513 | Batch 000/059 | Loss: 178.9641\n",
      "Epoch: 370/513 Train Loss: 234.1419\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 371/513 | Batch 000/059 | Loss: 252.2629\n",
      "Epoch: 371/513 Train Loss: 234.0720\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 372/513 | Batch 000/059 | Loss: 305.8112\n",
      "Epoch: 372/513 Train Loss: 234.1763\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 373/513 | Batch 000/059 | Loss: 273.5397\n",
      "Epoch: 373/513 Train Loss: 234.0305\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 374/513 | Batch 000/059 | Loss: 393.4609\n",
      "Epoch: 374/513 Train Loss: 233.9977\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 375/513 | Batch 000/059 | Loss: 115.2503\n",
      "Epoch: 375/513 Train Loss: 233.9496\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 376/513 | Batch 000/059 | Loss: 286.8474\n",
      "Epoch: 376/513 Train Loss: 233.9326\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 377/513 | Batch 000/059 | Loss: 192.8387\n",
      "Epoch: 377/513 Train Loss: 234.0300\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 378/513 | Batch 000/059 | Loss: 225.9761\n",
      "Epoch: 378/513 Train Loss: 233.8976\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 379/513 | Batch 000/059 | Loss: 203.3836\n",
      "Epoch: 379/513 Train Loss: 233.9568\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 380/513 | Batch 000/059 | Loss: 250.3375\n",
      "Epoch: 380/513 Train Loss: 233.8215\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 381/513 | Batch 000/059 | Loss: 294.2908\n",
      "Epoch: 381/513 Train Loss: 233.8101\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 382/513 | Batch 000/059 | Loss: 192.6432\n",
      "Epoch: 382/513 Train Loss: 233.7511\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 383/513 | Batch 000/059 | Loss: 240.2339\n",
      "Epoch: 383/513 Train Loss: 233.6703\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 384/513 | Batch 000/059 | Loss: 408.5650\n",
      "Epoch: 384/513 Train Loss: 233.7692\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 385/513 | Batch 000/059 | Loss: 157.0185\n",
      "Epoch: 385/513 Train Loss: 233.6572\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 386/513 | Batch 000/059 | Loss: 366.1508\n",
      "Epoch: 386/513 Train Loss: 233.6236\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 387/513 | Batch 000/059 | Loss: 237.1815\n",
      "Epoch: 387/513 Train Loss: 233.6126\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 388/513 | Batch 000/059 | Loss: 230.1686\n",
      "Epoch: 388/513 Train Loss: 233.5790\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 389/513 | Batch 000/059 | Loss: 229.5209\n",
      "Epoch: 389/513 Train Loss: 233.5355\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 390/513 | Batch 000/059 | Loss: 247.8456\n",
      "Epoch: 390/513 Train Loss: 233.4869\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 391/513 | Batch 000/059 | Loss: 297.6707\n",
      "Epoch: 391/513 Train Loss: 233.4776\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 392/513 | Batch 000/059 | Loss: 427.8605\n",
      "Epoch: 392/513 Train Loss: 233.4447\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 393/513 | Batch 000/059 | Loss: 145.3223\n",
      "Epoch: 393/513 Train Loss: 233.4352\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 394/513 | Batch 000/059 | Loss: 227.8453\n",
      "Epoch: 394/513 Train Loss: 233.6126\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 395/513 | Batch 000/059 | Loss: 393.6295\n",
      "Epoch: 395/513 Train Loss: 233.3087\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 396/513 | Batch 000/059 | Loss: 144.4128\n",
      "Epoch: 396/513 Train Loss: 233.3116\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 397/513 | Batch 000/059 | Loss: 354.5673\n",
      "Epoch: 397/513 Train Loss: 233.3590\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 398/513 | Batch 000/059 | Loss: 247.1667\n",
      "Epoch: 398/513 Train Loss: 233.2600\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 399/513 | Batch 000/059 | Loss: 213.4062\n",
      "Epoch: 399/513 Train Loss: 233.2324\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 400/513 | Batch 000/059 | Loss: 299.7057\n",
      "Epoch: 400/513 Train Loss: 233.2055\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 401/513 | Batch 000/059 | Loss: 317.1537\n",
      "Epoch: 401/513 Train Loss: 233.3170\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 402/513 | Batch 000/059 | Loss: 270.7787\n",
      "Epoch: 402/513 Train Loss: 233.1438\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 403/513 | Batch 000/059 | Loss: 210.1542\n",
      "Epoch: 403/513 Train Loss: 233.1464\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 404/513 | Batch 000/059 | Loss: 181.8123\n",
      "Epoch: 404/513 Train Loss: 233.2700\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 405/513 | Batch 000/059 | Loss: 224.2107\n",
      "Epoch: 405/513 Train Loss: 233.1088\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 406/513 | Batch 000/059 | Loss: 345.6285\n",
      "Epoch: 406/513 Train Loss: 233.1240\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 407/513 | Batch 000/059 | Loss: 264.3267\n",
      "Epoch: 407/513 Train Loss: 233.1000\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 408/513 | Batch 000/059 | Loss: 203.7983\n",
      "Epoch: 408/513 Train Loss: 233.0966\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 409/513 | Batch 000/059 | Loss: 247.2730\n",
      "Epoch: 409/513 Train Loss: 233.0934\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 410/513 | Batch 000/059 | Loss: 365.5269\n",
      "Epoch: 410/513 Train Loss: 233.0467\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 411/513 | Batch 000/059 | Loss: 169.5303\n",
      "Epoch: 411/513 Train Loss: 232.9982\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 412/513 | Batch 000/059 | Loss: 333.9543\n",
      "Epoch: 412/513 Train Loss: 232.9516\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 413/513 | Batch 000/059 | Loss: 180.7970\n",
      "Epoch: 413/513 Train Loss: 232.9863\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 414/513 | Batch 000/059 | Loss: 204.8379\n",
      "Epoch: 414/513 Train Loss: 232.9664\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 415/513 | Batch 000/059 | Loss: 128.5642\n",
      "Epoch: 415/513 Train Loss: 232.9560\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 416/513 | Batch 000/059 | Loss: 397.9500\n",
      "Epoch: 416/513 Train Loss: 232.8877\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 417/513 | Batch 000/059 | Loss: 261.1821\n",
      "Epoch: 417/513 Train Loss: 232.8774\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 418/513 | Batch 000/059 | Loss: 240.5134\n",
      "Epoch: 418/513 Train Loss: 232.8920\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 419/513 | Batch 000/059 | Loss: 247.4074\n",
      "Epoch: 419/513 Train Loss: 232.7375\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 420/513 | Batch 000/059 | Loss: 215.5357\n",
      "Epoch: 420/513 Train Loss: 232.7191\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 421/513 | Batch 000/059 | Loss: 355.9758\n",
      "Epoch: 421/513 Train Loss: 232.7618\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 422/513 | Batch 000/059 | Loss: 257.3049\n",
      "Epoch: 422/513 Train Loss: 232.6822\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 423/513 | Batch 000/059 | Loss: 367.6857\n",
      "Epoch: 423/513 Train Loss: 232.6866\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 424/513 | Batch 000/059 | Loss: 118.7898\n",
      "Epoch: 424/513 Train Loss: 232.6942\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 425/513 | Batch 000/059 | Loss: 270.4139\n",
      "Epoch: 425/513 Train Loss: 232.7054\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 426/513 | Batch 000/059 | Loss: 232.6420\n",
      "Epoch: 426/513 Train Loss: 232.6569\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 427/513 | Batch 000/059 | Loss: 268.6076\n",
      "Epoch: 427/513 Train Loss: 232.6441\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 428/513 | Batch 000/059 | Loss: 259.7386\n",
      "Epoch: 428/513 Train Loss: 232.6890\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 429/513 | Batch 000/059 | Loss: 269.5148\n",
      "Epoch: 429/513 Train Loss: 232.7469\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 430/513 | Batch 000/059 | Loss: 382.6272\n",
      "Epoch: 430/513 Train Loss: 232.6487\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 431/513 | Batch 000/059 | Loss: 331.9545\n",
      "Epoch: 431/513 Train Loss: 232.6217\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 432/513 | Batch 000/059 | Loss: 167.7401\n",
      "Epoch: 432/513 Train Loss: 232.5941\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 433/513 | Batch 000/059 | Loss: 166.2482\n",
      "Epoch: 433/513 Train Loss: 232.5996\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 434/513 | Batch 000/059 | Loss: 275.1216\n",
      "Epoch: 434/513 Train Loss: 232.5682\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 435/513 | Batch 000/059 | Loss: 346.3158\n",
      "Epoch: 435/513 Train Loss: 232.5031\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 436/513 | Batch 000/059 | Loss: 226.0852\n",
      "Epoch: 436/513 Train Loss: 232.5317\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 437/513 | Batch 000/059 | Loss: 233.5388\n",
      "Epoch: 437/513 Train Loss: 232.4180\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 438/513 | Batch 000/059 | Loss: 266.1707\n",
      "Epoch: 438/513 Train Loss: 232.4396\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 439/513 | Batch 000/059 | Loss: 281.2879\n",
      "Epoch: 439/513 Train Loss: 232.4713\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 440/513 | Batch 000/059 | Loss: 242.7426\n",
      "Epoch: 440/513 Train Loss: 232.4172\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 441/513 | Batch 000/059 | Loss: 272.1079\n",
      "Epoch: 441/513 Train Loss: 232.3629\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 442/513 | Batch 000/059 | Loss: 282.9465\n",
      "Epoch: 442/513 Train Loss: 232.3876\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 443/513 | Batch 000/059 | Loss: 175.6825\n",
      "Epoch: 443/513 Train Loss: 232.4233\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 444/513 | Batch 000/059 | Loss: 159.1095\n",
      "Epoch: 444/513 Train Loss: 232.3693\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 445/513 | Batch 000/059 | Loss: 327.4276\n",
      "Epoch: 445/513 Train Loss: 232.3217\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 446/513 | Batch 000/059 | Loss: 232.1573\n",
      "Epoch: 446/513 Train Loss: 232.2145\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 447/513 | Batch 000/059 | Loss: 246.9225\n",
      "Epoch: 447/513 Train Loss: 232.2244\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 448/513 | Batch 000/059 | Loss: 393.5850\n",
      "Epoch: 448/513 Train Loss: 232.2619\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 449/513 | Batch 000/059 | Loss: 257.4395\n",
      "Epoch: 449/513 Train Loss: 232.2015\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 450/513 | Batch 000/059 | Loss: 318.1772\n",
      "Epoch: 450/513 Train Loss: 232.2083\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 451/513 | Batch 000/059 | Loss: 276.2323\n",
      "Epoch: 451/513 Train Loss: 232.2032\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 452/513 | Batch 000/059 | Loss: 195.3746\n",
      "Epoch: 452/513 Train Loss: 232.2553\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 453/513 | Batch 000/059 | Loss: 249.7507\n",
      "Epoch: 453/513 Train Loss: 232.3382\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 454/513 | Batch 000/059 | Loss: 171.1055\n",
      "Epoch: 454/513 Train Loss: 232.1436\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 455/513 | Batch 000/059 | Loss: 231.9312\n",
      "Epoch: 455/513 Train Loss: 232.1322\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 456/513 | Batch 000/059 | Loss: 316.8412\n",
      "Epoch: 456/513 Train Loss: 232.1037\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 457/513 | Batch 000/059 | Loss: 226.8115\n",
      "Epoch: 457/513 Train Loss: 232.0772\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 458/513 | Batch 000/059 | Loss: 119.2343\n",
      "Epoch: 458/513 Train Loss: 232.0615\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 459/513 | Batch 000/059 | Loss: 332.3289\n",
      "Epoch: 459/513 Train Loss: 232.1693\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 460/513 | Batch 000/059 | Loss: 312.0012\n",
      "Epoch: 460/513 Train Loss: 232.0266\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 461/513 | Batch 000/059 | Loss: 357.1791\n",
      "Epoch: 461/513 Train Loss: 232.0495\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 462/513 | Batch 000/059 | Loss: 189.4463\n",
      "Epoch: 462/513 Train Loss: 232.0064\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 463/513 | Batch 000/059 | Loss: 175.9782\n",
      "Epoch: 463/513 Train Loss: 231.9659\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 464/513 | Batch 000/059 | Loss: 282.4084\n",
      "Epoch: 464/513 Train Loss: 231.9433\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 465/513 | Batch 000/059 | Loss: 246.3569\n",
      "Epoch: 465/513 Train Loss: 231.9416\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 466/513 | Batch 000/059 | Loss: 206.6501\n",
      "Epoch: 466/513 Train Loss: 231.9413\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 467/513 | Batch 000/059 | Loss: 216.1781\n",
      "Epoch: 467/513 Train Loss: 231.9254\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 468/513 | Batch 000/059 | Loss: 160.5988\n",
      "Epoch: 468/513 Train Loss: 231.9009\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 469/513 | Batch 000/059 | Loss: 191.8916\n",
      "Epoch: 469/513 Train Loss: 231.9025\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 470/513 | Batch 000/059 | Loss: 373.3649\n",
      "Epoch: 470/513 Train Loss: 231.9865\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 471/513 | Batch 000/059 | Loss: 185.3037\n",
      "Epoch: 471/513 Train Loss: 231.9327\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 472/513 | Batch 000/059 | Loss: 218.0268\n",
      "Epoch: 472/513 Train Loss: 231.8845\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 473/513 | Batch 000/059 | Loss: 155.0834\n",
      "Epoch: 473/513 Train Loss: 231.8407\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 474/513 | Batch 000/059 | Loss: 406.5330\n",
      "Epoch: 474/513 Train Loss: 231.8609\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 475/513 | Batch 000/059 | Loss: 133.6150\n",
      "Epoch: 475/513 Train Loss: 231.8210\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 476/513 | Batch 000/059 | Loss: 422.7078\n",
      "Epoch: 476/513 Train Loss: 231.8019\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 477/513 | Batch 000/059 | Loss: 238.2182\n",
      "Epoch: 477/513 Train Loss: 231.8875\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 478/513 | Batch 000/059 | Loss: 176.3280\n",
      "Epoch: 478/513 Train Loss: 232.0958\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 479/513 | Batch 000/059 | Loss: 212.0261\n",
      "Epoch: 479/513 Train Loss: 231.7682\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 480/513 | Batch 000/059 | Loss: 263.1533\n",
      "Epoch: 480/513 Train Loss: 231.7656\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 481/513 | Batch 000/059 | Loss: 200.6257\n",
      "Epoch: 481/513 Train Loss: 231.7130\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 482/513 | Batch 000/059 | Loss: 154.1082\n",
      "Epoch: 482/513 Train Loss: 231.6859\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 483/513 | Batch 000/059 | Loss: 161.2981\n",
      "Epoch: 483/513 Train Loss: 231.7056\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 484/513 | Batch 000/059 | Loss: 272.4614\n",
      "Epoch: 484/513 Train Loss: 231.6966\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 485/513 | Batch 000/059 | Loss: 236.6491\n",
      "Epoch: 485/513 Train Loss: 231.6798\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 486/513 | Batch 000/059 | Loss: 192.3044\n",
      "Epoch: 486/513 Train Loss: 231.6821\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 487/513 | Batch 000/059 | Loss: 280.8093\n",
      "Epoch: 487/513 Train Loss: 231.6353\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 488/513 | Batch 000/059 | Loss: 288.0739\n",
      "Epoch: 488/513 Train Loss: 231.5898\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 489/513 | Batch 000/059 | Loss: 149.3666\n",
      "Epoch: 489/513 Train Loss: 231.5829\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 490/513 | Batch 000/059 | Loss: 291.9302\n",
      "Epoch: 490/513 Train Loss: 231.5967\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 491/513 | Batch 000/059 | Loss: 218.9394\n",
      "Epoch: 491/513 Train Loss: 231.5275\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 492/513 | Batch 000/059 | Loss: 238.8352\n",
      "Epoch: 492/513 Train Loss: 231.5060\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 493/513 | Batch 000/059 | Loss: 220.2466\n",
      "Epoch: 493/513 Train Loss: 231.4602\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 494/513 | Batch 000/059 | Loss: 241.8558\n",
      "Epoch: 494/513 Train Loss: 231.4304\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 495/513 | Batch 000/059 | Loss: 196.5791\n",
      "Epoch: 495/513 Train Loss: 231.4639\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 496/513 | Batch 000/059 | Loss: 177.2496\n",
      "Epoch: 496/513 Train Loss: 231.4734\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 497/513 | Batch 000/059 | Loss: 230.6862\n",
      "Epoch: 497/513 Train Loss: 231.4238\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 498/513 | Batch 000/059 | Loss: 222.5497\n",
      "Epoch: 498/513 Train Loss: 231.4563\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 499/513 | Batch 000/059 | Loss: 406.7719\n",
      "Epoch: 499/513 Train Loss: 231.3989\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 500/513 | Batch 000/059 | Loss: 225.9092\n",
      "Epoch: 500/513 Train Loss: 231.4223\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 501/513 | Batch 000/059 | Loss: 255.1620\n",
      "Epoch: 501/513 Train Loss: 231.3813\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 502/513 | Batch 000/059 | Loss: 231.6516\n",
      "Epoch: 502/513 Train Loss: 231.3999\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 503/513 | Batch 000/059 | Loss: 178.1463\n",
      "Epoch: 503/513 Train Loss: 231.3459\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 504/513 | Batch 000/059 | Loss: 227.8184\n",
      "Epoch: 504/513 Train Loss: 231.4158\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 505/513 | Batch 000/059 | Loss: 397.7305\n",
      "Epoch: 505/513 Train Loss: 231.3519\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 506/513 | Batch 000/059 | Loss: 345.7938\n",
      "Epoch: 506/513 Train Loss: 231.4259\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 507/513 | Batch 000/059 | Loss: 268.8580\n",
      "Epoch: 507/513 Train Loss: 231.3647\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 508/513 | Batch 000/059 | Loss: 190.8009\n",
      "Epoch: 508/513 Train Loss: 231.2852\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 509/513 | Batch 000/059 | Loss: 423.2733\n",
      "Epoch: 509/513 Train Loss: 231.2602\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 510/513 | Batch 000/059 | Loss: 371.5189\n",
      "Epoch: 510/513 Train Loss: 231.2465\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 511/513 | Batch 000/059 | Loss: 223.5700\n",
      "Epoch: 511/513 Train Loss: 231.2862\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 512/513 | Batch 000/059 | Loss: 208.1749\n",
      "Epoch: 512/513 Train Loss: 231.2205\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 513/513 | Batch 000/059 | Loss: 220.6204\n",
      "Epoch: 513/513 Train Loss: 231.2184\n",
      "Time elapsed: 1.18 min\n",
      "Total Training Time: 1.18 min\n",
      "Training Loss: 231.22\n",
      "Test Loss: 237.06\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "minibatch_cost = []\n",
    "epoch_cost = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(device)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        outputs = model(features)\n",
    "        \n",
    "        cost = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        minibatch_cost.append(cost)\n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 1000:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "       \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        cost = compute_loss(model, train_loader)\n",
    "        epoch_cost.append(cost)\n",
    "        print('Epoch: %03d/%03d Train Loss: %.4f' % (\n",
    "                epoch+1, num_epochs, cost))\n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "        \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "    print('Training Loss: %.2f' % compute_loss(model, train_loader))\n",
    "    print('Test Loss: %.2f' % compute_loss(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "FJ27AoqMqjPm",
    "outputId": "19ac8391-3fbe-443e-c00f-329a568d7b73"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the loss curve\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(len(epoch_cost)), epoch_cost)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross entropy loss\")\n",
    "plt.title(\"Loss of training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCvcd-CDO9oa"
   },
   "source": [
    "## 6. Adadelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4fx2qkwrfr8"
   },
   "source": [
    "### Model B: 1 Hidden Layer Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGp134nArfr8"
   },
   "source": [
    "### Create model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L_poSpRmrfr8",
    "outputId": "cfe65ea1-2a1b-429f-8b97-483aff2dba45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "n_iters = 30000\n",
    "num_epochs = n_iters / (len(train_data) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "print(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "id": "3sFuAxd_rfr-"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # create your dataloader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "id": "PVlIj8B6rfr-"
   },
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, output_dim):\n",
    "        super(FNN, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear4 = nn.Linear(hidden_dim1, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.linear4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "id": "7r7bmNJmrfr-"
   },
   "outputs": [],
   "source": [
    "input_dim = tensor_x.shape[1]\n",
    "output_dim = 1\n",
    "hidden_dim1 = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "id": "afmPa3_Krfr-"
   },
   "outputs": [],
   "source": [
    "model = FNN(input_dim, hidden_dim1, output_dim)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zKW2664Crfr-",
    "outputId": "a45145b9-b388-448b-fbc7-e441accc3675"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 256]           2,816\n",
      "           Dropout-2               [-1, 1, 256]               0\n",
      "              ReLU-3               [-1, 1, 256]               0\n",
      "            Linear-4                 [-1, 1, 1]             257\n",
      "================================================================\n",
      "Total params: 3,073\n",
      "Trainable params: 3,073\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1,tensor_x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "id": "n3Alqbc1rfr_"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 0.0001\n",
    "LAMBDA = 0.05\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), weight_decay=LAMBDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6Bo0sOorfr_"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "roJfNXobrfr_",
    "outputId": "ad88f8f8-d5ad-4934-95a3-1d95e31fc0e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/513 | Batch 000/059 | Loss: 7344.2681\n",
      "Epoch: 001/513 Train Loss: 344.5282\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 002/513 | Batch 000/059 | Loss: 364.7456\n",
      "Epoch: 002/513 Train Loss: 335.6779\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 003/513 | Batch 000/059 | Loss: 352.8939\n",
      "Epoch: 003/513 Train Loss: 328.5791\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 004/513 | Batch 000/059 | Loss: 421.0411\n",
      "Epoch: 004/513 Train Loss: 320.0314\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 005/513 | Batch 000/059 | Loss: 270.8228\n",
      "Epoch: 005/513 Train Loss: 316.2087\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 006/513 | Batch 000/059 | Loss: 288.1021\n",
      "Epoch: 006/513 Train Loss: 306.3942\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 007/513 | Batch 000/059 | Loss: 304.2310\n",
      "Epoch: 007/513 Train Loss: 292.7584\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 008/513 | Batch 000/059 | Loss: 305.4404\n",
      "Epoch: 008/513 Train Loss: 285.8483\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 009/513 | Batch 000/059 | Loss: 497.8827\n",
      "Epoch: 009/513 Train Loss: 291.2030\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 010/513 | Batch 000/059 | Loss: 266.5574\n",
      "Epoch: 010/513 Train Loss: 270.5646\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 011/513 | Batch 000/059 | Loss: 299.3615\n",
      "Epoch: 011/513 Train Loss: 264.4852\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 012/513 | Batch 000/059 | Loss: 226.1874\n",
      "Epoch: 012/513 Train Loss: 274.2625\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 013/513 | Batch 000/059 | Loss: 395.0005\n",
      "Epoch: 013/513 Train Loss: 256.2564\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 014/513 | Batch 000/059 | Loss: 483.0730\n",
      "Epoch: 014/513 Train Loss: 258.7394\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 015/513 | Batch 000/059 | Loss: 371.6564\n",
      "Epoch: 015/513 Train Loss: 246.6057\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 016/513 | Batch 000/059 | Loss: 396.2076\n",
      "Epoch: 016/513 Train Loss: 242.8594\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 017/513 | Batch 000/059 | Loss: 295.2253\n",
      "Epoch: 017/513 Train Loss: 242.2455\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 018/513 | Batch 000/059 | Loss: 247.8990\n",
      "Epoch: 018/513 Train Loss: 238.4264\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 019/513 | Batch 000/059 | Loss: 228.2097\n",
      "Epoch: 019/513 Train Loss: 236.8278\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 020/513 | Batch 000/059 | Loss: 486.9811\n",
      "Epoch: 020/513 Train Loss: 234.4610\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 021/513 | Batch 000/059 | Loss: 134.5056\n",
      "Epoch: 021/513 Train Loss: 233.0831\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 022/513 | Batch 000/059 | Loss: 427.0422\n",
      "Epoch: 022/513 Train Loss: 232.7227\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 023/513 | Batch 000/059 | Loss: 242.2581\n",
      "Epoch: 023/513 Train Loss: 233.2262\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 024/513 | Batch 000/059 | Loss: 227.2420\n",
      "Epoch: 024/513 Train Loss: 232.1527\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 025/513 | Batch 000/059 | Loss: 150.5407\n",
      "Epoch: 025/513 Train Loss: 236.1129\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 026/513 | Batch 000/059 | Loss: 253.8732\n",
      "Epoch: 026/513 Train Loss: 232.6298\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 027/513 | Batch 000/059 | Loss: 250.4611\n",
      "Epoch: 027/513 Train Loss: 244.3617\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 028/513 | Batch 000/059 | Loss: 364.1388\n",
      "Epoch: 028/513 Train Loss: 231.2435\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 029/513 | Batch 000/059 | Loss: 169.1823\n",
      "Epoch: 029/513 Train Loss: 233.5753\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 030/513 | Batch 000/059 | Loss: 192.4735\n",
      "Epoch: 030/513 Train Loss: 232.3414\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 031/513 | Batch 000/059 | Loss: 305.6637\n",
      "Epoch: 031/513 Train Loss: 231.7105\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 032/513 | Batch 000/059 | Loss: 268.2758\n",
      "Epoch: 032/513 Train Loss: 228.1789\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 033/513 | Batch 000/059 | Loss: 262.5223\n",
      "Epoch: 033/513 Train Loss: 231.7226\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 034/513 | Batch 000/059 | Loss: 413.7255\n",
      "Epoch: 034/513 Train Loss: 227.9772\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 035/513 | Batch 000/059 | Loss: 271.2361\n",
      "Epoch: 035/513 Train Loss: 229.3525\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 036/513 | Batch 000/059 | Loss: 252.3697\n",
      "Epoch: 036/513 Train Loss: 247.4084\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 037/513 | Batch 000/059 | Loss: 339.2846\n",
      "Epoch: 037/513 Train Loss: 228.0703\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 038/513 | Batch 000/059 | Loss: 281.2952\n",
      "Epoch: 038/513 Train Loss: 226.4897\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 039/513 | Batch 000/059 | Loss: 105.3804\n",
      "Epoch: 039/513 Train Loss: 225.8730\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 040/513 | Batch 000/059 | Loss: 268.8988\n",
      "Epoch: 040/513 Train Loss: 232.1698\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 041/513 | Batch 000/059 | Loss: 293.5271\n",
      "Epoch: 041/513 Train Loss: 249.8988\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 042/513 | Batch 000/059 | Loss: 365.8143\n",
      "Epoch: 042/513 Train Loss: 225.5552\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 043/513 | Batch 000/059 | Loss: 293.5506\n",
      "Epoch: 043/513 Train Loss: 225.9063\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 044/513 | Batch 000/059 | Loss: 115.0457\n",
      "Epoch: 044/513 Train Loss: 225.2851\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 045/513 | Batch 000/059 | Loss: 212.2409\n",
      "Epoch: 045/513 Train Loss: 227.6207\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 046/513 | Batch 000/059 | Loss: 152.8527\n",
      "Epoch: 046/513 Train Loss: 226.9381\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 047/513 | Batch 000/059 | Loss: 206.0782\n",
      "Epoch: 047/513 Train Loss: 237.2602\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 048/513 | Batch 000/059 | Loss: 318.7211\n",
      "Epoch: 048/513 Train Loss: 226.9323\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 049/513 | Batch 000/059 | Loss: 177.6420\n",
      "Epoch: 049/513 Train Loss: 225.0832\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 050/513 | Batch 000/059 | Loss: 228.9492\n",
      "Epoch: 050/513 Train Loss: 224.6029\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 051/513 | Batch 000/059 | Loss: 134.5542\n",
      "Epoch: 051/513 Train Loss: 228.9797\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 052/513 | Batch 000/059 | Loss: 272.8163\n",
      "Epoch: 052/513 Train Loss: 222.7031\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 053/513 | Batch 000/059 | Loss: 319.2466\n",
      "Epoch: 053/513 Train Loss: 230.6542\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 054/513 | Batch 000/059 | Loss: 341.1033\n",
      "Epoch: 054/513 Train Loss: 224.5925\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 055/513 | Batch 000/059 | Loss: 314.7144\n",
      "Epoch: 055/513 Train Loss: 223.6024\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 056/513 | Batch 000/059 | Loss: 270.8265\n",
      "Epoch: 056/513 Train Loss: 222.0250\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 057/513 | Batch 000/059 | Loss: 272.2429\n",
      "Epoch: 057/513 Train Loss: 225.5326\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 058/513 | Batch 000/059 | Loss: 209.3364\n",
      "Epoch: 058/513 Train Loss: 226.7713\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 059/513 | Batch 000/059 | Loss: 213.7885\n",
      "Epoch: 059/513 Train Loss: 221.6121\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 060/513 | Batch 000/059 | Loss: 253.1235\n",
      "Epoch: 060/513 Train Loss: 221.3334\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 061/513 | Batch 000/059 | Loss: 213.0854\n",
      "Epoch: 061/513 Train Loss: 235.4041\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 062/513 | Batch 000/059 | Loss: 121.2917\n",
      "Epoch: 062/513 Train Loss: 234.8599\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 063/513 | Batch 000/059 | Loss: 234.5018\n",
      "Epoch: 063/513 Train Loss: 220.4054\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 064/513 | Batch 000/059 | Loss: 188.7537\n",
      "Epoch: 064/513 Train Loss: 229.1742\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 065/513 | Batch 000/059 | Loss: 219.3054\n",
      "Epoch: 065/513 Train Loss: 225.5112\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 066/513 | Batch 000/059 | Loss: 397.7038\n",
      "Epoch: 066/513 Train Loss: 221.8316\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 067/513 | Batch 000/059 | Loss: 313.1558\n",
      "Epoch: 067/513 Train Loss: 220.6385\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 068/513 | Batch 000/059 | Loss: 174.7356\n",
      "Epoch: 068/513 Train Loss: 221.8314\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 069/513 | Batch 000/059 | Loss: 185.1719\n",
      "Epoch: 069/513 Train Loss: 220.5193\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 070/513 | Batch 000/059 | Loss: 125.7489\n",
      "Epoch: 070/513 Train Loss: 229.0556\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 071/513 | Batch 000/059 | Loss: 316.8006\n",
      "Epoch: 071/513 Train Loss: 220.3248\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 072/513 | Batch 000/059 | Loss: 335.8236\n",
      "Epoch: 072/513 Train Loss: 238.4930\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 073/513 | Batch 000/059 | Loss: 163.5389\n",
      "Epoch: 073/513 Train Loss: 220.6386\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 074/513 | Batch 000/059 | Loss: 254.2556\n",
      "Epoch: 074/513 Train Loss: 218.8692\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 075/513 | Batch 000/059 | Loss: 234.4866\n",
      "Epoch: 075/513 Train Loss: 222.6137\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 076/513 | Batch 000/059 | Loss: 260.2130\n",
      "Epoch: 076/513 Train Loss: 220.7704\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 077/513 | Batch 000/059 | Loss: 150.4459\n",
      "Epoch: 077/513 Train Loss: 218.7429\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 078/513 | Batch 000/059 | Loss: 371.7187\n",
      "Epoch: 078/513 Train Loss: 218.0972\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 079/513 | Batch 000/059 | Loss: 165.9531\n",
      "Epoch: 079/513 Train Loss: 218.5663\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 080/513 | Batch 000/059 | Loss: 318.9330\n",
      "Epoch: 080/513 Train Loss: 219.5264\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 081/513 | Batch 000/059 | Loss: 531.4579\n",
      "Epoch: 081/513 Train Loss: 221.1564\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 082/513 | Batch 000/059 | Loss: 273.3244\n",
      "Epoch: 082/513 Train Loss: 218.3813\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 083/513 | Batch 000/059 | Loss: 199.5781\n",
      "Epoch: 083/513 Train Loss: 220.2620\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 084/513 | Batch 000/059 | Loss: 188.7647\n",
      "Epoch: 084/513 Train Loss: 216.7603\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 085/513 | Batch 000/059 | Loss: 201.8946\n",
      "Epoch: 085/513 Train Loss: 217.8992\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 086/513 | Batch 000/059 | Loss: 165.2809\n",
      "Epoch: 086/513 Train Loss: 217.5341\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 087/513 | Batch 000/059 | Loss: 273.6877\n",
      "Epoch: 087/513 Train Loss: 217.8910\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 088/513 | Batch 000/059 | Loss: 394.5933\n",
      "Epoch: 088/513 Train Loss: 219.2629\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 089/513 | Batch 000/059 | Loss: 157.9715\n",
      "Epoch: 089/513 Train Loss: 218.0464\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 090/513 | Batch 000/059 | Loss: 178.9199\n",
      "Epoch: 090/513 Train Loss: 216.8354\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 091/513 | Batch 000/059 | Loss: 200.8354\n",
      "Epoch: 091/513 Train Loss: 217.4264\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 092/513 | Batch 000/059 | Loss: 190.0397\n",
      "Epoch: 092/513 Train Loss: 216.4971\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 093/513 | Batch 000/059 | Loss: 283.2169\n",
      "Epoch: 093/513 Train Loss: 218.5042\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 094/513 | Batch 000/059 | Loss: 272.1683\n",
      "Epoch: 094/513 Train Loss: 217.2462\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 095/513 | Batch 000/059 | Loss: 167.0266\n",
      "Epoch: 095/513 Train Loss: 224.7210\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 096/513 | Batch 000/059 | Loss: 323.5961\n",
      "Epoch: 096/513 Train Loss: 216.2446\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 097/513 | Batch 000/059 | Loss: 254.5667\n",
      "Epoch: 097/513 Train Loss: 218.6678\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 098/513 | Batch 000/059 | Loss: 200.4108\n",
      "Epoch: 098/513 Train Loss: 215.9468\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 099/513 | Batch 000/059 | Loss: 456.0364\n",
      "Epoch: 099/513 Train Loss: 217.1719\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 100/513 | Batch 000/059 | Loss: 264.2181\n",
      "Epoch: 100/513 Train Loss: 215.8011\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 101/513 | Batch 000/059 | Loss: 246.4396\n",
      "Epoch: 101/513 Train Loss: 217.7605\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 102/513 | Batch 000/059 | Loss: 212.3169\n",
      "Epoch: 102/513 Train Loss: 216.8351\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 103/513 | Batch 000/059 | Loss: 142.5576\n",
      "Epoch: 103/513 Train Loss: 219.3854\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 104/513 | Batch 000/059 | Loss: 305.9477\n",
      "Epoch: 104/513 Train Loss: 217.4547\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 105/513 | Batch 000/059 | Loss: 193.0476\n",
      "Epoch: 105/513 Train Loss: 216.9797\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 106/513 | Batch 000/059 | Loss: 177.3540\n",
      "Epoch: 106/513 Train Loss: 216.4728\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 107/513 | Batch 000/059 | Loss: 314.5934\n",
      "Epoch: 107/513 Train Loss: 221.3297\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 108/513 | Batch 000/059 | Loss: 323.7443\n",
      "Epoch: 108/513 Train Loss: 215.9529\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 109/513 | Batch 000/059 | Loss: 234.4545\n",
      "Epoch: 109/513 Train Loss: 216.7300\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 110/513 | Batch 000/059 | Loss: 235.1496\n",
      "Epoch: 110/513 Train Loss: 214.2281\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 111/513 | Batch 000/059 | Loss: 275.4653\n",
      "Epoch: 111/513 Train Loss: 214.4051\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 112/513 | Batch 000/059 | Loss: 92.9196\n",
      "Epoch: 112/513 Train Loss: 224.6088\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 113/513 | Batch 000/059 | Loss: 273.3788\n",
      "Epoch: 113/513 Train Loss: 214.3964\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 114/513 | Batch 000/059 | Loss: 137.2124\n",
      "Epoch: 114/513 Train Loss: 215.4327\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 115/513 | Batch 000/059 | Loss: 132.2842\n",
      "Epoch: 115/513 Train Loss: 216.0065\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 116/513 | Batch 000/059 | Loss: 166.9827\n",
      "Epoch: 116/513 Train Loss: 218.9389\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 117/513 | Batch 000/059 | Loss: 236.9801\n",
      "Epoch: 117/513 Train Loss: 215.6812\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 118/513 | Batch 000/059 | Loss: 258.8484\n",
      "Epoch: 118/513 Train Loss: 215.1140\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 119/513 | Batch 000/059 | Loss: 370.5231\n",
      "Epoch: 119/513 Train Loss: 215.3518\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 120/513 | Batch 000/059 | Loss: 294.3632\n",
      "Epoch: 120/513 Train Loss: 217.2369\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 121/513 | Batch 000/059 | Loss: 415.2817\n",
      "Epoch: 121/513 Train Loss: 214.0648\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 122/513 | Batch 000/059 | Loss: 271.7859\n",
      "Epoch: 122/513 Train Loss: 219.7970\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 123/513 | Batch 000/059 | Loss: 266.8906\n",
      "Epoch: 123/513 Train Loss: 214.1099\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 124/513 | Batch 000/059 | Loss: 268.2043\n",
      "Epoch: 124/513 Train Loss: 215.9947\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 125/513 | Batch 000/059 | Loss: 97.3874\n",
      "Epoch: 125/513 Train Loss: 213.2381\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 126/513 | Batch 000/059 | Loss: 350.8323\n",
      "Epoch: 126/513 Train Loss: 215.1159\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 127/513 | Batch 000/059 | Loss: 252.6662\n",
      "Epoch: 127/513 Train Loss: 214.0070\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 128/513 | Batch 000/059 | Loss: 276.6149\n",
      "Epoch: 128/513 Train Loss: 214.8080\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 129/513 | Batch 000/059 | Loss: 168.9563\n",
      "Epoch: 129/513 Train Loss: 213.5156\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 130/513 | Batch 000/059 | Loss: 214.3266\n",
      "Epoch: 130/513 Train Loss: 231.0269\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 131/513 | Batch 000/059 | Loss: 228.9802\n",
      "Epoch: 131/513 Train Loss: 217.6148\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 132/513 | Batch 000/059 | Loss: 253.4001\n",
      "Epoch: 132/513 Train Loss: 213.9831\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 133/513 | Batch 000/059 | Loss: 194.3946\n",
      "Epoch: 133/513 Train Loss: 214.6385\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 134/513 | Batch 000/059 | Loss: 199.1890\n",
      "Epoch: 134/513 Train Loss: 225.9442\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 135/513 | Batch 000/059 | Loss: 162.2879\n",
      "Epoch: 135/513 Train Loss: 215.3708\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 136/513 | Batch 000/059 | Loss: 260.8283\n",
      "Epoch: 136/513 Train Loss: 226.4458\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 137/513 | Batch 000/059 | Loss: 230.9692\n",
      "Epoch: 137/513 Train Loss: 214.0268\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 138/513 | Batch 000/059 | Loss: 292.5612\n",
      "Epoch: 138/513 Train Loss: 215.5743\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 139/513 | Batch 000/059 | Loss: 241.4117\n",
      "Epoch: 139/513 Train Loss: 218.6086\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 140/513 | Batch 000/059 | Loss: 296.5932\n",
      "Epoch: 140/513 Train Loss: 213.8858\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 141/513 | Batch 000/059 | Loss: 192.2584\n",
      "Epoch: 141/513 Train Loss: 213.2473\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 142/513 | Batch 000/059 | Loss: 306.7739\n",
      "Epoch: 142/513 Train Loss: 225.7828\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 143/513 | Batch 000/059 | Loss: 234.1893\n",
      "Epoch: 143/513 Train Loss: 214.8270\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 144/513 | Batch 000/059 | Loss: 238.4550\n",
      "Epoch: 144/513 Train Loss: 222.3393\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 145/513 | Batch 000/059 | Loss: 312.5650\n",
      "Epoch: 145/513 Train Loss: 217.0245\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 146/513 | Batch 000/059 | Loss: 137.5589\n",
      "Epoch: 146/513 Train Loss: 213.4343\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 147/513 | Batch 000/059 | Loss: 271.3159\n",
      "Epoch: 147/513 Train Loss: 218.0774\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 148/513 | Batch 000/059 | Loss: 207.5875\n",
      "Epoch: 148/513 Train Loss: 215.8585\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 149/513 | Batch 000/059 | Loss: 253.8076\n",
      "Epoch: 149/513 Train Loss: 214.0709\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 150/513 | Batch 000/059 | Loss: 201.1843\n",
      "Epoch: 150/513 Train Loss: 214.4969\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 151/513 | Batch 000/059 | Loss: 191.9577\n",
      "Epoch: 151/513 Train Loss: 235.4297\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 152/513 | Batch 000/059 | Loss: 333.9601\n",
      "Epoch: 152/513 Train Loss: 227.7908\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 153/513 | Batch 000/059 | Loss: 256.8469\n",
      "Epoch: 153/513 Train Loss: 213.4064\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 154/513 | Batch 000/059 | Loss: 148.2831\n",
      "Epoch: 154/513 Train Loss: 212.7720\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 155/513 | Batch 000/059 | Loss: 259.5583\n",
      "Epoch: 155/513 Train Loss: 214.9662\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 156/513 | Batch 000/059 | Loss: 173.8442\n",
      "Epoch: 156/513 Train Loss: 213.4386\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 157/513 | Batch 000/059 | Loss: 238.1739\n",
      "Epoch: 157/513 Train Loss: 216.4302\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 158/513 | Batch 000/059 | Loss: 307.7834\n",
      "Epoch: 158/513 Train Loss: 212.4446\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 159/513 | Batch 000/059 | Loss: 236.9822\n",
      "Epoch: 159/513 Train Loss: 212.4064\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 160/513 | Batch 000/059 | Loss: 212.5837\n",
      "Epoch: 160/513 Train Loss: 222.9212\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 161/513 | Batch 000/059 | Loss: 207.4291\n",
      "Epoch: 161/513 Train Loss: 213.7384\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 162/513 | Batch 000/059 | Loss: 215.4148\n",
      "Epoch: 162/513 Train Loss: 213.8260\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 163/513 | Batch 000/059 | Loss: 195.4557\n",
      "Epoch: 163/513 Train Loss: 216.3000\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 164/513 | Batch 000/059 | Loss: 158.4282\n",
      "Epoch: 164/513 Train Loss: 214.8663\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 165/513 | Batch 000/059 | Loss: 268.3563\n",
      "Epoch: 165/513 Train Loss: 213.7842\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 166/513 | Batch 000/059 | Loss: 280.0971\n",
      "Epoch: 166/513 Train Loss: 216.7782\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 167/513 | Batch 000/059 | Loss: 207.7505\n",
      "Epoch: 167/513 Train Loss: 228.1075\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 168/513 | Batch 000/059 | Loss: 293.5629\n",
      "Epoch: 168/513 Train Loss: 213.0215\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 169/513 | Batch 000/059 | Loss: 221.6481\n",
      "Epoch: 169/513 Train Loss: 213.2541\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 170/513 | Batch 000/059 | Loss: 229.3736\n",
      "Epoch: 170/513 Train Loss: 219.9999\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 171/513 | Batch 000/059 | Loss: 297.9545\n",
      "Epoch: 171/513 Train Loss: 213.3451\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 172/513 | Batch 000/059 | Loss: 204.6052\n",
      "Epoch: 172/513 Train Loss: 220.8508\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 173/513 | Batch 000/059 | Loss: 354.1512\n",
      "Epoch: 173/513 Train Loss: 211.7772\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 174/513 | Batch 000/059 | Loss: 302.2325\n",
      "Epoch: 174/513 Train Loss: 218.5925\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 175/513 | Batch 000/059 | Loss: 232.0992\n",
      "Epoch: 175/513 Train Loss: 212.5535\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 176/513 | Batch 000/059 | Loss: 133.8658\n",
      "Epoch: 176/513 Train Loss: 213.0957\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 177/513 | Batch 000/059 | Loss: 276.0569\n",
      "Epoch: 177/513 Train Loss: 217.7185\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 178/513 | Batch 000/059 | Loss: 234.4787\n",
      "Epoch: 178/513 Train Loss: 213.4406\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 179/513 | Batch 000/059 | Loss: 92.2626\n",
      "Epoch: 179/513 Train Loss: 222.2016\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 180/513 | Batch 000/059 | Loss: 196.4020\n",
      "Epoch: 180/513 Train Loss: 222.6670\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 181/513 | Batch 000/059 | Loss: 98.7200\n",
      "Epoch: 181/513 Train Loss: 212.5632\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 182/513 | Batch 000/059 | Loss: 400.6514\n",
      "Epoch: 182/513 Train Loss: 216.3334\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 183/513 | Batch 000/059 | Loss: 162.7430\n",
      "Epoch: 183/513 Train Loss: 219.8757\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 184/513 | Batch 000/059 | Loss: 252.3078\n",
      "Epoch: 184/513 Train Loss: 215.9668\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 185/513 | Batch 000/059 | Loss: 284.9315\n",
      "Epoch: 185/513 Train Loss: 212.9266\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 186/513 | Batch 000/059 | Loss: 260.3507\n",
      "Epoch: 186/513 Train Loss: 219.6567\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 187/513 | Batch 000/059 | Loss: 186.7505\n",
      "Epoch: 187/513 Train Loss: 213.3090\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 188/513 | Batch 000/059 | Loss: 127.8878\n",
      "Epoch: 188/513 Train Loss: 212.3154\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 189/513 | Batch 000/059 | Loss: 147.8845\n",
      "Epoch: 189/513 Train Loss: 216.4526\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 190/513 | Batch 000/059 | Loss: 216.2805\n",
      "Epoch: 190/513 Train Loss: 217.0168\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 191/513 | Batch 000/059 | Loss: 142.5877\n",
      "Epoch: 191/513 Train Loss: 211.6454\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 192/513 | Batch 000/059 | Loss: 203.6682\n",
      "Epoch: 192/513 Train Loss: 214.7918\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 193/513 | Batch 000/059 | Loss: 158.0868\n",
      "Epoch: 193/513 Train Loss: 211.3777\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 194/513 | Batch 000/059 | Loss: 370.6198\n",
      "Epoch: 194/513 Train Loss: 236.2373\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 195/513 | Batch 000/059 | Loss: 270.7450\n",
      "Epoch: 195/513 Train Loss: 213.6557\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 196/513 | Batch 000/059 | Loss: 200.1196\n",
      "Epoch: 196/513 Train Loss: 211.4222\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 197/513 | Batch 000/059 | Loss: 184.9702\n",
      "Epoch: 197/513 Train Loss: 215.6556\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 198/513 | Batch 000/059 | Loss: 177.5159\n",
      "Epoch: 198/513 Train Loss: 212.4372\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 199/513 | Batch 000/059 | Loss: 250.8672\n",
      "Epoch: 199/513 Train Loss: 215.6133\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 200/513 | Batch 000/059 | Loss: 230.7310\n",
      "Epoch: 200/513 Train Loss: 212.5035\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 201/513 | Batch 000/059 | Loss: 147.8817\n",
      "Epoch: 201/513 Train Loss: 211.1564\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 202/513 | Batch 000/059 | Loss: 183.8504\n",
      "Epoch: 202/513 Train Loss: 212.0696\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 203/513 | Batch 000/059 | Loss: 163.6868\n",
      "Epoch: 203/513 Train Loss: 219.0159\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 204/513 | Batch 000/059 | Loss: 312.1990\n",
      "Epoch: 204/513 Train Loss: 211.2168\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 205/513 | Batch 000/059 | Loss: 202.1538\n",
      "Epoch: 205/513 Train Loss: 212.4000\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 206/513 | Batch 000/059 | Loss: 246.6543\n",
      "Epoch: 206/513 Train Loss: 219.5425\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 207/513 | Batch 000/059 | Loss: 231.7610\n",
      "Epoch: 207/513 Train Loss: 214.5477\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 208/513 | Batch 000/059 | Loss: 193.9543\n",
      "Epoch: 208/513 Train Loss: 214.1998\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 209/513 | Batch 000/059 | Loss: 290.7039\n",
      "Epoch: 209/513 Train Loss: 215.4014\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 210/513 | Batch 000/059 | Loss: 206.9035\n",
      "Epoch: 210/513 Train Loss: 218.2819\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 211/513 | Batch 000/059 | Loss: 128.8229\n",
      "Epoch: 211/513 Train Loss: 232.8647\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 212/513 | Batch 000/059 | Loss: 188.4314\n",
      "Epoch: 212/513 Train Loss: 230.6833\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 213/513 | Batch 000/059 | Loss: 138.5815\n",
      "Epoch: 213/513 Train Loss: 211.1946\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 214/513 | Batch 000/059 | Loss: 256.0115\n",
      "Epoch: 214/513 Train Loss: 215.1571\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 215/513 | Batch 000/059 | Loss: 151.6673\n",
      "Epoch: 215/513 Train Loss: 211.5511\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 216/513 | Batch 000/059 | Loss: 111.6685\n",
      "Epoch: 216/513 Train Loss: 212.1563\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 217/513 | Batch 000/059 | Loss: 173.9857\n",
      "Epoch: 217/513 Train Loss: 211.9486\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 218/513 | Batch 000/059 | Loss: 197.1537\n",
      "Epoch: 218/513 Train Loss: 223.9162\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 219/513 | Batch 000/059 | Loss: 137.6258\n",
      "Epoch: 219/513 Train Loss: 214.0783\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 220/513 | Batch 000/059 | Loss: 266.4216\n",
      "Epoch: 220/513 Train Loss: 221.5129\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 221/513 | Batch 000/059 | Loss: 161.7446\n",
      "Epoch: 221/513 Train Loss: 212.0484\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 222/513 | Batch 000/059 | Loss: 200.2007\n",
      "Epoch: 222/513 Train Loss: 211.5647\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 223/513 | Batch 000/059 | Loss: 256.4432\n",
      "Epoch: 223/513 Train Loss: 229.5776\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 224/513 | Batch 000/059 | Loss: 274.1998\n",
      "Epoch: 224/513 Train Loss: 235.2269\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 225/513 | Batch 000/059 | Loss: 265.1630\n",
      "Epoch: 225/513 Train Loss: 213.0865\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 226/513 | Batch 000/059 | Loss: 240.9543\n",
      "Epoch: 226/513 Train Loss: 216.8951\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 227/513 | Batch 000/059 | Loss: 154.3548\n",
      "Epoch: 227/513 Train Loss: 225.0173\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 228/513 | Batch 000/059 | Loss: 325.5962\n",
      "Epoch: 228/513 Train Loss: 218.0138\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 229/513 | Batch 000/059 | Loss: 274.8735\n",
      "Epoch: 229/513 Train Loss: 211.5760\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 230/513 | Batch 000/059 | Loss: 142.7883\n",
      "Epoch: 230/513 Train Loss: 213.8285\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 231/513 | Batch 000/059 | Loss: 261.8211\n",
      "Epoch: 231/513 Train Loss: 211.6823\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 232/513 | Batch 000/059 | Loss: 349.7657\n",
      "Epoch: 232/513 Train Loss: 218.6538\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 233/513 | Batch 000/059 | Loss: 266.8955\n",
      "Epoch: 233/513 Train Loss: 215.8634\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 234/513 | Batch 000/059 | Loss: 247.7497\n",
      "Epoch: 234/513 Train Loss: 211.0524\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 235/513 | Batch 000/059 | Loss: 417.8865\n",
      "Epoch: 235/513 Train Loss: 210.8770\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 236/513 | Batch 000/059 | Loss: 278.1077\n",
      "Epoch: 236/513 Train Loss: 216.8983\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 237/513 | Batch 000/059 | Loss: 241.4776\n",
      "Epoch: 237/513 Train Loss: 214.6540\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 238/513 | Batch 000/059 | Loss: 142.0128\n",
      "Epoch: 238/513 Train Loss: 211.5665\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 239/513 | Batch 000/059 | Loss: 370.8881\n",
      "Epoch: 239/513 Train Loss: 211.8301\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 240/513 | Batch 000/059 | Loss: 131.7527\n",
      "Epoch: 240/513 Train Loss: 212.0719\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 241/513 | Batch 000/059 | Loss: 456.7025\n",
      "Epoch: 241/513 Train Loss: 211.7648\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 242/513 | Batch 000/059 | Loss: 180.6208\n",
      "Epoch: 242/513 Train Loss: 210.7891\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 243/513 | Batch 000/059 | Loss: 215.1372\n",
      "Epoch: 243/513 Train Loss: 216.4760\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 244/513 | Batch 000/059 | Loss: 126.0532\n",
      "Epoch: 244/513 Train Loss: 212.9971\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 245/513 | Batch 000/059 | Loss: 187.8669\n",
      "Epoch: 245/513 Train Loss: 222.8663\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 246/513 | Batch 000/059 | Loss: 218.8144\n",
      "Epoch: 246/513 Train Loss: 215.5232\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 247/513 | Batch 000/059 | Loss: 293.6558\n",
      "Epoch: 247/513 Train Loss: 215.0426\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 248/513 | Batch 000/059 | Loss: 159.6353\n",
      "Epoch: 248/513 Train Loss: 212.2541\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 249/513 | Batch 000/059 | Loss: 142.4312\n",
      "Epoch: 249/513 Train Loss: 211.5148\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 250/513 | Batch 000/059 | Loss: 348.3208\n",
      "Epoch: 250/513 Train Loss: 212.1144\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 251/513 | Batch 000/059 | Loss: 180.4682\n",
      "Epoch: 251/513 Train Loss: 211.4605\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 252/513 | Batch 000/059 | Loss: 330.3796\n",
      "Epoch: 252/513 Train Loss: 211.6538\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 253/513 | Batch 000/059 | Loss: 332.2055\n",
      "Epoch: 253/513 Train Loss: 213.3802\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 254/513 | Batch 000/059 | Loss: 163.5725\n",
      "Epoch: 254/513 Train Loss: 212.0217\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 255/513 | Batch 000/059 | Loss: 227.9657\n",
      "Epoch: 255/513 Train Loss: 212.6477\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 256/513 | Batch 000/059 | Loss: 195.3347\n",
      "Epoch: 256/513 Train Loss: 224.6412\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 257/513 | Batch 000/059 | Loss: 267.5022\n",
      "Epoch: 257/513 Train Loss: 212.4644\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 258/513 | Batch 000/059 | Loss: 206.6155\n",
      "Epoch: 258/513 Train Loss: 212.1666\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 259/513 | Batch 000/059 | Loss: 112.4405\n",
      "Epoch: 259/513 Train Loss: 215.5891\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 260/513 | Batch 000/059 | Loss: 264.9073\n",
      "Epoch: 260/513 Train Loss: 213.7365\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 261/513 | Batch 000/059 | Loss: 223.2059\n",
      "Epoch: 261/513 Train Loss: 213.2669\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 262/513 | Batch 000/059 | Loss: 231.2141\n",
      "Epoch: 262/513 Train Loss: 214.5402\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 263/513 | Batch 000/059 | Loss: 242.6883\n",
      "Epoch: 263/513 Train Loss: 211.3584\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 264/513 | Batch 000/059 | Loss: 185.8329\n",
      "Epoch: 264/513 Train Loss: 214.9960\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 265/513 | Batch 000/059 | Loss: 278.5468\n",
      "Epoch: 265/513 Train Loss: 215.0062\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 266/513 | Batch 000/059 | Loss: 175.3681\n",
      "Epoch: 266/513 Train Loss: 210.9577\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 267/513 | Batch 000/059 | Loss: 232.5142\n",
      "Epoch: 267/513 Train Loss: 210.3289\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 268/513 | Batch 000/059 | Loss: 226.0884\n",
      "Epoch: 268/513 Train Loss: 213.1587\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 269/513 | Batch 000/059 | Loss: 184.1418\n",
      "Epoch: 269/513 Train Loss: 211.2391\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 270/513 | Batch 000/059 | Loss: 263.7020\n",
      "Epoch: 270/513 Train Loss: 216.2366\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 271/513 | Batch 000/059 | Loss: 221.3315\n",
      "Epoch: 271/513 Train Loss: 213.3051\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 272/513 | Batch 000/059 | Loss: 153.4503\n",
      "Epoch: 272/513 Train Loss: 215.2214\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 273/513 | Batch 000/059 | Loss: 111.4632\n",
      "Epoch: 273/513 Train Loss: 212.9542\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 274/513 | Batch 000/059 | Loss: 169.5962\n",
      "Epoch: 274/513 Train Loss: 210.8018\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 275/513 | Batch 000/059 | Loss: 116.9153\n",
      "Epoch: 275/513 Train Loss: 210.9886\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 276/513 | Batch 000/059 | Loss: 202.9255\n",
      "Epoch: 276/513 Train Loss: 213.2189\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 277/513 | Batch 000/059 | Loss: 234.9053\n",
      "Epoch: 277/513 Train Loss: 211.2265\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 278/513 | Batch 000/059 | Loss: 180.7728\n",
      "Epoch: 278/513 Train Loss: 241.2805\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 279/513 | Batch 000/059 | Loss: 164.5967\n",
      "Epoch: 279/513 Train Loss: 210.9245\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 280/513 | Batch 000/059 | Loss: 293.3054\n",
      "Epoch: 280/513 Train Loss: 212.5347\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 281/513 | Batch 000/059 | Loss: 192.6227\n",
      "Epoch: 281/513 Train Loss: 213.4651\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 282/513 | Batch 000/059 | Loss: 239.9545\n",
      "Epoch: 282/513 Train Loss: 218.9730\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 283/513 | Batch 000/059 | Loss: 159.6348\n",
      "Epoch: 283/513 Train Loss: 218.6926\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 284/513 | Batch 000/059 | Loss: 439.0748\n",
      "Epoch: 284/513 Train Loss: 212.9109\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 285/513 | Batch 000/059 | Loss: 170.1853\n",
      "Epoch: 285/513 Train Loss: 212.3510\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 286/513 | Batch 000/059 | Loss: 197.1177\n",
      "Epoch: 286/513 Train Loss: 221.4253\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 287/513 | Batch 000/059 | Loss: 276.3021\n",
      "Epoch: 287/513 Train Loss: 215.3736\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 288/513 | Batch 000/059 | Loss: 369.2745\n",
      "Epoch: 288/513 Train Loss: 211.1955\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 289/513 | Batch 000/059 | Loss: 270.6091\n",
      "Epoch: 289/513 Train Loss: 222.5152\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 290/513 | Batch 000/059 | Loss: 299.5789\n",
      "Epoch: 290/513 Train Loss: 215.7457\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 291/513 | Batch 000/059 | Loss: 110.1993\n",
      "Epoch: 291/513 Train Loss: 217.0279\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 292/513 | Batch 000/059 | Loss: 209.6130\n",
      "Epoch: 292/513 Train Loss: 213.7448\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 293/513 | Batch 000/059 | Loss: 287.9704\n",
      "Epoch: 293/513 Train Loss: 211.4886\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 294/513 | Batch 000/059 | Loss: 232.2525\n",
      "Epoch: 294/513 Train Loss: 211.2516\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 295/513 | Batch 000/059 | Loss: 158.7294\n",
      "Epoch: 295/513 Train Loss: 210.8540\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 296/513 | Batch 000/059 | Loss: 219.2912\n",
      "Epoch: 296/513 Train Loss: 215.4081\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 297/513 | Batch 000/059 | Loss: 149.1527\n",
      "Epoch: 297/513 Train Loss: 210.8176\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 298/513 | Batch 000/059 | Loss: 313.5054\n",
      "Epoch: 298/513 Train Loss: 220.4735\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 299/513 | Batch 000/059 | Loss: 182.2706\n",
      "Epoch: 299/513 Train Loss: 211.3288\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 300/513 | Batch 000/059 | Loss: 196.9763\n",
      "Epoch: 300/513 Train Loss: 211.9952\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 301/513 | Batch 000/059 | Loss: 134.0063\n",
      "Epoch: 301/513 Train Loss: 210.6250\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 302/513 | Batch 000/059 | Loss: 286.4789\n",
      "Epoch: 302/513 Train Loss: 214.5818\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 303/513 | Batch 000/059 | Loss: 232.4559\n",
      "Epoch: 303/513 Train Loss: 220.2724\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 304/513 | Batch 000/059 | Loss: 320.1782\n",
      "Epoch: 304/513 Train Loss: 232.3368\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 305/513 | Batch 000/059 | Loss: 203.3075\n",
      "Epoch: 305/513 Train Loss: 212.2287\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 306/513 | Batch 000/059 | Loss: 215.7351\n",
      "Epoch: 306/513 Train Loss: 210.6979\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 307/513 | Batch 000/059 | Loss: 136.1277\n",
      "Epoch: 307/513 Train Loss: 210.8742\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 308/513 | Batch 000/059 | Loss: 180.7758\n",
      "Epoch: 308/513 Train Loss: 213.5719\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 309/513 | Batch 000/059 | Loss: 167.6469\n",
      "Epoch: 309/513 Train Loss: 221.0847\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 310/513 | Batch 000/059 | Loss: 326.7739\n",
      "Epoch: 310/513 Train Loss: 211.8431\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 311/513 | Batch 000/059 | Loss: 254.0272\n",
      "Epoch: 311/513 Train Loss: 211.6945\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 312/513 | Batch 000/059 | Loss: 147.4623\n",
      "Epoch: 312/513 Train Loss: 215.2071\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 313/513 | Batch 000/059 | Loss: 218.5861\n",
      "Epoch: 313/513 Train Loss: 215.1264\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 314/513 | Batch 000/059 | Loss: 155.6997\n",
      "Epoch: 314/513 Train Loss: 228.4310\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 315/513 | Batch 000/059 | Loss: 162.1979\n",
      "Epoch: 315/513 Train Loss: 221.7414\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 316/513 | Batch 000/059 | Loss: 185.4751\n",
      "Epoch: 316/513 Train Loss: 211.4751\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 317/513 | Batch 000/059 | Loss: 187.1375\n",
      "Epoch: 317/513 Train Loss: 214.7827\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 318/513 | Batch 000/059 | Loss: 223.2375\n",
      "Epoch: 318/513 Train Loss: 210.5483\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 319/513 | Batch 000/059 | Loss: 178.1061\n",
      "Epoch: 319/513 Train Loss: 212.8741\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 320/513 | Batch 000/059 | Loss: 214.2855\n",
      "Epoch: 320/513 Train Loss: 211.9451\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 321/513 | Batch 000/059 | Loss: 243.0088\n",
      "Epoch: 321/513 Train Loss: 210.8184\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 322/513 | Batch 000/059 | Loss: 187.9135\n",
      "Epoch: 322/513 Train Loss: 211.9437\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 323/513 | Batch 000/059 | Loss: 363.6791\n",
      "Epoch: 323/513 Train Loss: 210.9174\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 324/513 | Batch 000/059 | Loss: 223.6596\n",
      "Epoch: 324/513 Train Loss: 221.3480\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 325/513 | Batch 000/059 | Loss: 242.9363\n",
      "Epoch: 325/513 Train Loss: 210.3775\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 326/513 | Batch 000/059 | Loss: 174.1177\n",
      "Epoch: 326/513 Train Loss: 212.2454\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 327/513 | Batch 000/059 | Loss: 207.8808\n",
      "Epoch: 327/513 Train Loss: 217.3631\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 328/513 | Batch 000/059 | Loss: 228.6470\n",
      "Epoch: 328/513 Train Loss: 211.7619\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 329/513 | Batch 000/059 | Loss: 179.6574\n",
      "Epoch: 329/513 Train Loss: 212.5552\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 330/513 | Batch 000/059 | Loss: 262.7167\n",
      "Epoch: 330/513 Train Loss: 235.7403\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 331/513 | Batch 000/059 | Loss: 243.9704\n",
      "Epoch: 331/513 Train Loss: 212.8882\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 332/513 | Batch 000/059 | Loss: 102.2904\n",
      "Epoch: 332/513 Train Loss: 213.1999\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 333/513 | Batch 000/059 | Loss: 150.6409\n",
      "Epoch: 333/513 Train Loss: 210.6929\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 334/513 | Batch 000/059 | Loss: 262.7870\n",
      "Epoch: 334/513 Train Loss: 231.2289\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 335/513 | Batch 000/059 | Loss: 366.1168\n",
      "Epoch: 335/513 Train Loss: 210.7739\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 336/513 | Batch 000/059 | Loss: 154.7703\n",
      "Epoch: 336/513 Train Loss: 213.8999\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 337/513 | Batch 000/059 | Loss: 285.4683\n",
      "Epoch: 337/513 Train Loss: 211.2375\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 338/513 | Batch 000/059 | Loss: 211.8954\n",
      "Epoch: 338/513 Train Loss: 218.3851\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 339/513 | Batch 000/059 | Loss: 335.6442\n",
      "Epoch: 339/513 Train Loss: 211.0300\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 340/513 | Batch 000/059 | Loss: 352.2137\n",
      "Epoch: 340/513 Train Loss: 210.7703\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 341/513 | Batch 000/059 | Loss: 268.0049\n",
      "Epoch: 341/513 Train Loss: 211.3695\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 342/513 | Batch 000/059 | Loss: 246.3993\n",
      "Epoch: 342/513 Train Loss: 212.4188\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 343/513 | Batch 000/059 | Loss: 445.1328\n",
      "Epoch: 343/513 Train Loss: 214.9774\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 344/513 | Batch 000/059 | Loss: 206.5065\n",
      "Epoch: 344/513 Train Loss: 211.7036\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 345/513 | Batch 000/059 | Loss: 251.2000\n",
      "Epoch: 345/513 Train Loss: 210.2804\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 346/513 | Batch 000/059 | Loss: 244.6938\n",
      "Epoch: 346/513 Train Loss: 219.0346\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 347/513 | Batch 000/059 | Loss: 274.6236\n",
      "Epoch: 347/513 Train Loss: 210.0686\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 348/513 | Batch 000/059 | Loss: 244.8458\n",
      "Epoch: 348/513 Train Loss: 213.5156\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 349/513 | Batch 000/059 | Loss: 232.1266\n",
      "Epoch: 349/513 Train Loss: 214.2131\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 350/513 | Batch 000/059 | Loss: 147.3612\n",
      "Epoch: 350/513 Train Loss: 211.1868\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 351/513 | Batch 000/059 | Loss: 309.9578\n",
      "Epoch: 351/513 Train Loss: 210.4450\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 352/513 | Batch 000/059 | Loss: 270.5369\n",
      "Epoch: 352/513 Train Loss: 210.6051\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 353/513 | Batch 000/059 | Loss: 248.0984\n",
      "Epoch: 353/513 Train Loss: 213.0893\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 354/513 | Batch 000/059 | Loss: 258.2935\n",
      "Epoch: 354/513 Train Loss: 212.7565\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 355/513 | Batch 000/059 | Loss: 126.9977\n",
      "Epoch: 355/513 Train Loss: 215.1424\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 356/513 | Batch 000/059 | Loss: 246.0171\n",
      "Epoch: 356/513 Train Loss: 211.7459\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 357/513 | Batch 000/059 | Loss: 354.9526\n",
      "Epoch: 357/513 Train Loss: 210.4213\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 358/513 | Batch 000/059 | Loss: 379.4571\n",
      "Epoch: 358/513 Train Loss: 211.2285\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 359/513 | Batch 000/059 | Loss: 184.8139\n",
      "Epoch: 359/513 Train Loss: 214.5337\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 360/513 | Batch 000/059 | Loss: 185.6498\n",
      "Epoch: 360/513 Train Loss: 210.5999\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 361/513 | Batch 000/059 | Loss: 256.0376\n",
      "Epoch: 361/513 Train Loss: 212.1834\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 362/513 | Batch 000/059 | Loss: 159.8795\n",
      "Epoch: 362/513 Train Loss: 210.1593\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 363/513 | Batch 000/059 | Loss: 294.9739\n",
      "Epoch: 363/513 Train Loss: 213.2446\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 364/513 | Batch 000/059 | Loss: 297.6779\n",
      "Epoch: 364/513 Train Loss: 212.6791\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 365/513 | Batch 000/059 | Loss: 179.4312\n",
      "Epoch: 365/513 Train Loss: 214.8824\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 366/513 | Batch 000/059 | Loss: 188.7269\n",
      "Epoch: 366/513 Train Loss: 210.7234\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 367/513 | Batch 000/059 | Loss: 203.6980\n",
      "Epoch: 367/513 Train Loss: 222.2857\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 368/513 | Batch 000/059 | Loss: 218.1965\n",
      "Epoch: 368/513 Train Loss: 211.9499\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 369/513 | Batch 000/059 | Loss: 131.3620\n",
      "Epoch: 369/513 Train Loss: 211.2355\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 370/513 | Batch 000/059 | Loss: 209.9975\n",
      "Epoch: 370/513 Train Loss: 210.8621\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 371/513 | Batch 000/059 | Loss: 166.3430\n",
      "Epoch: 371/513 Train Loss: 212.1830\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 372/513 | Batch 000/059 | Loss: 326.3658\n",
      "Epoch: 372/513 Train Loss: 212.1189\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 373/513 | Batch 000/059 | Loss: 170.7776\n",
      "Epoch: 373/513 Train Loss: 212.3379\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 374/513 | Batch 000/059 | Loss: 192.2652\n",
      "Epoch: 374/513 Train Loss: 211.6473\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 375/513 | Batch 000/059 | Loss: 144.9563\n",
      "Epoch: 375/513 Train Loss: 224.9317\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 376/513 | Batch 000/059 | Loss: 288.9326\n",
      "Epoch: 376/513 Train Loss: 216.5367\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 377/513 | Batch 000/059 | Loss: 139.9029\n",
      "Epoch: 377/513 Train Loss: 214.0236\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 378/513 | Batch 000/059 | Loss: 217.4331\n",
      "Epoch: 378/513 Train Loss: 210.7426\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 379/513 | Batch 000/059 | Loss: 145.7426\n",
      "Epoch: 379/513 Train Loss: 213.9196\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 380/513 | Batch 000/059 | Loss: 164.3503\n",
      "Epoch: 380/513 Train Loss: 210.9955\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 381/513 | Batch 000/059 | Loss: 275.3935\n",
      "Epoch: 381/513 Train Loss: 212.0945\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 382/513 | Batch 000/059 | Loss: 249.0234\n",
      "Epoch: 382/513 Train Loss: 225.4983\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 383/513 | Batch 000/059 | Loss: 174.8878\n",
      "Epoch: 383/513 Train Loss: 217.9636\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 384/513 | Batch 000/059 | Loss: 307.9062\n",
      "Epoch: 384/513 Train Loss: 211.0689\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 385/513 | Batch 000/059 | Loss: 173.5962\n",
      "Epoch: 385/513 Train Loss: 215.0762\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 386/513 | Batch 000/059 | Loss: 194.4824\n",
      "Epoch: 386/513 Train Loss: 223.9673\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 387/513 | Batch 000/059 | Loss: 202.5978\n",
      "Epoch: 387/513 Train Loss: 214.7622\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 388/513 | Batch 000/059 | Loss: 210.5529\n",
      "Epoch: 388/513 Train Loss: 213.8894\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 389/513 | Batch 000/059 | Loss: 210.9676\n",
      "Epoch: 389/513 Train Loss: 211.2185\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 390/513 | Batch 000/059 | Loss: 351.6338\n",
      "Epoch: 390/513 Train Loss: 211.4367\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 391/513 | Batch 000/059 | Loss: 129.5596\n",
      "Epoch: 391/513 Train Loss: 211.8030\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 392/513 | Batch 000/059 | Loss: 185.5331\n",
      "Epoch: 392/513 Train Loss: 211.4854\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 393/513 | Batch 000/059 | Loss: 147.6268\n",
      "Epoch: 393/513 Train Loss: 211.6755\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 394/513 | Batch 000/059 | Loss: 169.8159\n",
      "Epoch: 394/513 Train Loss: 210.5376\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 395/513 | Batch 000/059 | Loss: 277.9989\n",
      "Epoch: 395/513 Train Loss: 212.4987\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 396/513 | Batch 000/059 | Loss: 176.5387\n",
      "Epoch: 396/513 Train Loss: 221.7118\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 397/513 | Batch 000/059 | Loss: 233.7259\n",
      "Epoch: 397/513 Train Loss: 216.1948\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 398/513 | Batch 000/059 | Loss: 219.6736\n",
      "Epoch: 398/513 Train Loss: 217.1868\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 399/513 | Batch 000/059 | Loss: 190.4232\n",
      "Epoch: 399/513 Train Loss: 226.7356\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 400/513 | Batch 000/059 | Loss: 224.9644\n",
      "Epoch: 400/513 Train Loss: 215.0119\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 401/513 | Batch 000/059 | Loss: 207.7283\n",
      "Epoch: 401/513 Train Loss: 210.2651\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 402/513 | Batch 000/059 | Loss: 146.2773\n",
      "Epoch: 402/513 Train Loss: 211.4021\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 403/513 | Batch 000/059 | Loss: 188.9702\n",
      "Epoch: 403/513 Train Loss: 211.8437\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 404/513 | Batch 000/059 | Loss: 157.5870\n",
      "Epoch: 404/513 Train Loss: 217.7490\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 405/513 | Batch 000/059 | Loss: 260.3528\n",
      "Epoch: 405/513 Train Loss: 212.5456\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 406/513 | Batch 000/059 | Loss: 164.2160\n",
      "Epoch: 406/513 Train Loss: 212.4475\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 407/513 | Batch 000/059 | Loss: 296.2117\n",
      "Epoch: 407/513 Train Loss: 210.8596\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 408/513 | Batch 000/059 | Loss: 144.1227\n",
      "Epoch: 408/513 Train Loss: 211.2012\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 409/513 | Batch 000/059 | Loss: 249.9885\n",
      "Epoch: 409/513 Train Loss: 216.0756\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 410/513 | Batch 000/059 | Loss: 393.4165\n",
      "Epoch: 410/513 Train Loss: 214.0554\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 411/513 | Batch 000/059 | Loss: 139.6129\n",
      "Epoch: 411/513 Train Loss: 211.2239\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 412/513 | Batch 000/059 | Loss: 254.4055\n",
      "Epoch: 412/513 Train Loss: 214.0721\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 413/513 | Batch 000/059 | Loss: 135.0623\n",
      "Epoch: 413/513 Train Loss: 210.2819\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 414/513 | Batch 000/059 | Loss: 153.9593\n",
      "Epoch: 414/513 Train Loss: 210.6930\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 415/513 | Batch 000/059 | Loss: 150.2777\n",
      "Epoch: 415/513 Train Loss: 211.7468\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 416/513 | Batch 000/059 | Loss: 163.0197\n",
      "Epoch: 416/513 Train Loss: 210.5749\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 417/513 | Batch 000/059 | Loss: 174.3760\n",
      "Epoch: 417/513 Train Loss: 211.9052\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 418/513 | Batch 000/059 | Loss: 176.3976\n",
      "Epoch: 418/513 Train Loss: 218.3412\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 419/513 | Batch 000/059 | Loss: 227.5108\n",
      "Epoch: 419/513 Train Loss: 218.2667\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 420/513 | Batch 000/059 | Loss: 310.4735\n",
      "Epoch: 420/513 Train Loss: 210.8285\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 421/513 | Batch 000/059 | Loss: 243.6148\n",
      "Epoch: 421/513 Train Loss: 212.8930\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 422/513 | Batch 000/059 | Loss: 191.7074\n",
      "Epoch: 422/513 Train Loss: 210.8539\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 423/513 | Batch 000/059 | Loss: 142.6268\n",
      "Epoch: 423/513 Train Loss: 211.2008\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 424/513 | Batch 000/059 | Loss: 116.0813\n",
      "Epoch: 424/513 Train Loss: 210.6315\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 425/513 | Batch 000/059 | Loss: 193.6687\n",
      "Epoch: 425/513 Train Loss: 211.8590\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 426/513 | Batch 000/059 | Loss: 160.0633\n",
      "Epoch: 426/513 Train Loss: 210.5631\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 427/513 | Batch 000/059 | Loss: 180.7221\n",
      "Epoch: 427/513 Train Loss: 212.2593\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 428/513 | Batch 000/059 | Loss: 239.3675\n",
      "Epoch: 428/513 Train Loss: 211.4381\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 429/513 | Batch 000/059 | Loss: 257.1945\n",
      "Epoch: 429/513 Train Loss: 212.2935\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 430/513 | Batch 000/059 | Loss: 179.4780\n",
      "Epoch: 430/513 Train Loss: 239.2591\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 431/513 | Batch 000/059 | Loss: 201.0054\n",
      "Epoch: 431/513 Train Loss: 212.1404\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 432/513 | Batch 000/059 | Loss: 257.1288\n",
      "Epoch: 432/513 Train Loss: 226.4939\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 433/513 | Batch 000/059 | Loss: 148.2514\n",
      "Epoch: 433/513 Train Loss: 211.0822\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 434/513 | Batch 000/059 | Loss: 253.1685\n",
      "Epoch: 434/513 Train Loss: 217.8324\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 435/513 | Batch 000/059 | Loss: 262.4891\n",
      "Epoch: 435/513 Train Loss: 211.3573\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 436/513 | Batch 000/059 | Loss: 159.9098\n",
      "Epoch: 436/513 Train Loss: 218.0901\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 437/513 | Batch 000/059 | Loss: 328.0412\n",
      "Epoch: 437/513 Train Loss: 213.7624\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 438/513 | Batch 000/059 | Loss: 278.1470\n",
      "Epoch: 438/513 Train Loss: 210.6836\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 439/513 | Batch 000/059 | Loss: 158.2234\n",
      "Epoch: 439/513 Train Loss: 210.1624\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 440/513 | Batch 000/059 | Loss: 167.0112\n",
      "Epoch: 440/513 Train Loss: 215.0027\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 441/513 | Batch 000/059 | Loss: 168.4718\n",
      "Epoch: 441/513 Train Loss: 218.0906\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 442/513 | Batch 000/059 | Loss: 236.9709\n",
      "Epoch: 442/513 Train Loss: 218.9236\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 443/513 | Batch 000/059 | Loss: 327.0614\n",
      "Epoch: 443/513 Train Loss: 220.9358\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 444/513 | Batch 000/059 | Loss: 118.2523\n",
      "Epoch: 444/513 Train Loss: 210.2439\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 445/513 | Batch 000/059 | Loss: 229.3784\n",
      "Epoch: 445/513 Train Loss: 232.2481\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 446/513 | Batch 000/059 | Loss: 238.9748\n",
      "Epoch: 446/513 Train Loss: 209.8932\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 447/513 | Batch 000/059 | Loss: 208.5823\n",
      "Epoch: 447/513 Train Loss: 211.1459\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 448/513 | Batch 000/059 | Loss: 141.9625\n",
      "Epoch: 448/513 Train Loss: 212.2208\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 449/513 | Batch 000/059 | Loss: 194.5680\n",
      "Epoch: 449/513 Train Loss: 211.2412\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 450/513 | Batch 000/059 | Loss: 207.7344\n",
      "Epoch: 450/513 Train Loss: 219.3174\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 451/513 | Batch 000/059 | Loss: 171.6257\n",
      "Epoch: 451/513 Train Loss: 210.2140\n",
      "Time elapsed: 1.19 min\n",
      "Epoch: 452/513 | Batch 000/059 | Loss: 297.7218\n",
      "Epoch: 452/513 Train Loss: 211.1176\n",
      "Time elapsed: 1.19 min\n",
      "Epoch: 453/513 | Batch 000/059 | Loss: 157.9659\n",
      "Epoch: 453/513 Train Loss: 210.0322\n",
      "Time elapsed: 1.19 min\n",
      "Epoch: 454/513 | Batch 000/059 | Loss: 279.4402\n",
      "Epoch: 454/513 Train Loss: 211.1948\n",
      "Time elapsed: 1.19 min\n",
      "Epoch: 455/513 | Batch 000/059 | Loss: 228.2968\n",
      "Epoch: 455/513 Train Loss: 215.5303\n",
      "Time elapsed: 1.20 min\n",
      "Epoch: 456/513 | Batch 000/059 | Loss: 209.4660\n",
      "Epoch: 456/513 Train Loss: 211.5939\n",
      "Time elapsed: 1.20 min\n",
      "Epoch: 457/513 | Batch 000/059 | Loss: 308.0993\n",
      "Epoch: 457/513 Train Loss: 210.0434\n",
      "Time elapsed: 1.20 min\n",
      "Epoch: 458/513 | Batch 000/059 | Loss: 113.2620\n",
      "Epoch: 458/513 Train Loss: 210.1973\n",
      "Time elapsed: 1.20 min\n",
      "Epoch: 459/513 | Batch 000/059 | Loss: 162.0615\n",
      "Epoch: 459/513 Train Loss: 213.4740\n",
      "Time elapsed: 1.21 min\n",
      "Epoch: 460/513 | Batch 000/059 | Loss: 131.6377\n",
      "Epoch: 460/513 Train Loss: 215.9603\n",
      "Time elapsed: 1.21 min\n",
      "Epoch: 461/513 | Batch 000/059 | Loss: 173.6687\n",
      "Epoch: 461/513 Train Loss: 212.6908\n",
      "Time elapsed: 1.21 min\n",
      "Epoch: 462/513 | Batch 000/059 | Loss: 269.0047\n",
      "Epoch: 462/513 Train Loss: 210.2052\n",
      "Time elapsed: 1.21 min\n",
      "Epoch: 463/513 | Batch 000/059 | Loss: 243.3119\n",
      "Epoch: 463/513 Train Loss: 218.4165\n",
      "Time elapsed: 1.22 min\n",
      "Epoch: 464/513 | Batch 000/059 | Loss: 226.6178\n",
      "Epoch: 464/513 Train Loss: 209.8767\n",
      "Time elapsed: 1.22 min\n",
      "Epoch: 465/513 | Batch 000/059 | Loss: 225.9229\n",
      "Epoch: 465/513 Train Loss: 217.2407\n",
      "Time elapsed: 1.22 min\n",
      "Epoch: 466/513 | Batch 000/059 | Loss: 376.1840\n",
      "Epoch: 466/513 Train Loss: 210.4944\n",
      "Time elapsed: 1.22 min\n",
      "Epoch: 467/513 | Batch 000/059 | Loss: 225.4610\n",
      "Epoch: 467/513 Train Loss: 211.4619\n",
      "Time elapsed: 1.23 min\n",
      "Epoch: 468/513 | Batch 000/059 | Loss: 216.6972\n",
      "Epoch: 468/513 Train Loss: 215.4829\n",
      "Time elapsed: 1.23 min\n",
      "Epoch: 469/513 | Batch 000/059 | Loss: 113.2227\n",
      "Epoch: 469/513 Train Loss: 210.0386\n",
      "Time elapsed: 1.23 min\n",
      "Epoch: 470/513 | Batch 000/059 | Loss: 287.8952\n",
      "Epoch: 470/513 Train Loss: 218.6266\n",
      "Time elapsed: 1.23 min\n",
      "Epoch: 471/513 | Batch 000/059 | Loss: 301.0716\n",
      "Epoch: 471/513 Train Loss: 214.6383\n",
      "Time elapsed: 1.24 min\n",
      "Epoch: 472/513 | Batch 000/059 | Loss: 191.2189\n",
      "Epoch: 472/513 Train Loss: 210.7267\n",
      "Time elapsed: 1.24 min\n",
      "Epoch: 473/513 | Batch 000/059 | Loss: 168.3640\n",
      "Epoch: 473/513 Train Loss: 211.8650\n",
      "Time elapsed: 1.24 min\n",
      "Epoch: 474/513 | Batch 000/059 | Loss: 277.9641\n",
      "Epoch: 474/513 Train Loss: 222.0074\n",
      "Time elapsed: 1.25 min\n",
      "Epoch: 475/513 | Batch 000/059 | Loss: 177.5921\n",
      "Epoch: 475/513 Train Loss: 212.5820\n",
      "Time elapsed: 1.25 min\n",
      "Epoch: 476/513 | Batch 000/059 | Loss: 146.3671\n",
      "Epoch: 476/513 Train Loss: 217.8825\n",
      "Time elapsed: 1.25 min\n",
      "Epoch: 477/513 | Batch 000/059 | Loss: 344.1366\n",
      "Epoch: 477/513 Train Loss: 213.1571\n",
      "Time elapsed: 1.25 min\n",
      "Epoch: 478/513 | Batch 000/059 | Loss: 159.6509\n",
      "Epoch: 478/513 Train Loss: 215.5547\n",
      "Time elapsed: 1.26 min\n",
      "Epoch: 479/513 | Batch 000/059 | Loss: 366.0529\n",
      "Epoch: 479/513 Train Loss: 212.5819\n",
      "Time elapsed: 1.26 min\n",
      "Epoch: 480/513 | Batch 000/059 | Loss: 153.3770\n",
      "Epoch: 480/513 Train Loss: 211.0492\n",
      "Time elapsed: 1.26 min\n",
      "Epoch: 481/513 | Batch 000/059 | Loss: 411.3586\n",
      "Epoch: 481/513 Train Loss: 211.0419\n",
      "Time elapsed: 1.26 min\n",
      "Epoch: 482/513 | Batch 000/059 | Loss: 187.6224\n",
      "Epoch: 482/513 Train Loss: 210.6242\n",
      "Time elapsed: 1.27 min\n",
      "Epoch: 483/513 | Batch 000/059 | Loss: 147.9500\n",
      "Epoch: 483/513 Train Loss: 210.3677\n",
      "Time elapsed: 1.27 min\n",
      "Epoch: 484/513 | Batch 000/059 | Loss: 207.1579\n",
      "Epoch: 484/513 Train Loss: 210.6639\n",
      "Time elapsed: 1.27 min\n",
      "Epoch: 485/513 | Batch 000/059 | Loss: 155.4333\n",
      "Epoch: 485/513 Train Loss: 210.1001\n",
      "Time elapsed: 1.27 min\n",
      "Epoch: 486/513 | Batch 000/059 | Loss: 192.8612\n",
      "Epoch: 486/513 Train Loss: 227.1520\n",
      "Time elapsed: 1.28 min\n",
      "Epoch: 487/513 | Batch 000/059 | Loss: 277.8739\n",
      "Epoch: 487/513 Train Loss: 214.2011\n",
      "Time elapsed: 1.28 min\n",
      "Epoch: 488/513 | Batch 000/059 | Loss: 203.4718\n",
      "Epoch: 488/513 Train Loss: 210.4581\n",
      "Time elapsed: 1.28 min\n",
      "Epoch: 489/513 | Batch 000/059 | Loss: 210.3767\n",
      "Epoch: 489/513 Train Loss: 210.7362\n",
      "Time elapsed: 1.28 min\n",
      "Epoch: 490/513 | Batch 000/059 | Loss: 211.2982\n",
      "Epoch: 490/513 Train Loss: 211.6446\n",
      "Time elapsed: 1.29 min\n",
      "Epoch: 491/513 | Batch 000/059 | Loss: 125.8167\n",
      "Epoch: 491/513 Train Loss: 215.4868\n",
      "Time elapsed: 1.29 min\n",
      "Epoch: 492/513 | Batch 000/059 | Loss: 167.9255\n",
      "Epoch: 492/513 Train Loss: 233.1394\n",
      "Time elapsed: 1.29 min\n",
      "Epoch: 493/513 | Batch 000/059 | Loss: 469.2177\n",
      "Epoch: 493/513 Train Loss: 218.7238\n",
      "Time elapsed: 1.29 min\n",
      "Epoch: 494/513 | Batch 000/059 | Loss: 227.8511\n",
      "Epoch: 494/513 Train Loss: 212.8285\n",
      "Time elapsed: 1.30 min\n",
      "Epoch: 495/513 | Batch 000/059 | Loss: 186.7532\n",
      "Epoch: 495/513 Train Loss: 213.1671\n",
      "Time elapsed: 1.30 min\n",
      "Epoch: 496/513 | Batch 000/059 | Loss: 191.6035\n",
      "Epoch: 496/513 Train Loss: 217.5310\n",
      "Time elapsed: 1.30 min\n",
      "Epoch: 497/513 | Batch 000/059 | Loss: 356.3522\n",
      "Epoch: 497/513 Train Loss: 211.9494\n",
      "Time elapsed: 1.31 min\n",
      "Epoch: 498/513 | Batch 000/059 | Loss: 314.4207\n",
      "Epoch: 498/513 Train Loss: 224.9176\n",
      "Time elapsed: 1.31 min\n",
      "Epoch: 499/513 | Batch 000/059 | Loss: 218.1816\n",
      "Epoch: 499/513 Train Loss: 211.1191\n",
      "Time elapsed: 1.31 min\n",
      "Epoch: 500/513 | Batch 000/059 | Loss: 246.0558\n",
      "Epoch: 500/513 Train Loss: 210.4028\n",
      "Time elapsed: 1.31 min\n",
      "Epoch: 501/513 | Batch 000/059 | Loss: 298.1281\n",
      "Epoch: 501/513 Train Loss: 210.7259\n",
      "Time elapsed: 1.32 min\n",
      "Epoch: 502/513 | Batch 000/059 | Loss: 259.3353\n",
      "Epoch: 502/513 Train Loss: 213.1669\n",
      "Time elapsed: 1.32 min\n",
      "Epoch: 503/513 | Batch 000/059 | Loss: 271.5890\n",
      "Epoch: 503/513 Train Loss: 211.3666\n",
      "Time elapsed: 1.32 min\n",
      "Epoch: 504/513 | Batch 000/059 | Loss: 233.1894\n",
      "Epoch: 504/513 Train Loss: 223.2592\n",
      "Time elapsed: 1.32 min\n",
      "Epoch: 505/513 | Batch 000/059 | Loss: 201.0129\n",
      "Epoch: 505/513 Train Loss: 210.4068\n",
      "Time elapsed: 1.33 min\n",
      "Epoch: 506/513 | Batch 000/059 | Loss: 275.5868\n",
      "Epoch: 506/513 Train Loss: 210.3863\n",
      "Time elapsed: 1.33 min\n",
      "Epoch: 507/513 | Batch 000/059 | Loss: 200.5394\n",
      "Epoch: 507/513 Train Loss: 218.2897\n",
      "Time elapsed: 1.33 min\n",
      "Epoch: 508/513 | Batch 000/059 | Loss: 177.8053\n",
      "Epoch: 508/513 Train Loss: 211.1305\n",
      "Time elapsed: 1.33 min\n",
      "Epoch: 509/513 | Batch 000/059 | Loss: 336.8840\n",
      "Epoch: 509/513 Train Loss: 211.4337\n",
      "Time elapsed: 1.34 min\n",
      "Epoch: 510/513 | Batch 000/059 | Loss: 231.9279\n",
      "Epoch: 510/513 Train Loss: 210.0642\n",
      "Time elapsed: 1.34 min\n",
      "Epoch: 511/513 | Batch 000/059 | Loss: 226.2026\n",
      "Epoch: 511/513 Train Loss: 211.9659\n",
      "Time elapsed: 1.34 min\n",
      "Epoch: 512/513 | Batch 000/059 | Loss: 276.4537\n",
      "Epoch: 512/513 Train Loss: 216.7732\n",
      "Time elapsed: 1.34 min\n",
      "Epoch: 513/513 | Batch 000/059 | Loss: 204.3567\n",
      "Epoch: 513/513 Train Loss: 210.4112\n",
      "Time elapsed: 1.35 min\n",
      "Total Training Time: 1.35 min\n",
      "Training Loss: 210.41\n",
      "Test Loss: 223.24\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "minibatch_cost = []\n",
    "epoch_cost = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(device)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        outputs = model(features)\n",
    "        \n",
    "        cost = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        minibatch_cost.append(cost)\n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 1000:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "       \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        cost = compute_loss(model, train_loader)\n",
    "        epoch_cost.append(cost)\n",
    "        print('Epoch: %03d/%03d Train Loss: %.4f' % (\n",
    "                epoch+1, num_epochs, cost))\n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "        \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "    print('Training Loss: %.2f' % compute_loss(model, train_loader))\n",
    "    print('Test Loss: %.2f' % compute_loss(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "lU2mW8N4rfsA",
    "outputId": "d4e7bea3-8ec5-4369-e0ee-13e5d1b756b2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAFNCAYAAAD2E503AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd5xcVfn48c8zZWdLdpOQbEgnhIQSWsDQBBWQLyAW7AVErPhV9Ccq1i9+bYDta1dQmoKg2IIiVUBKKElIQgophPRsstnsZrN9+pzfH7fMndmZ2dnNzu5M8rxfr7wye6edafe5zznPPUeMMSillFKqsvlGuwFKKaWUOnAa0JVSSqmDgAZ0pZRS6iCgAV0ppZQ6CGhAV0oppQ4CGtCVUkqpg4AGdKVUTiJyvYi0icieEj5Hj4jMHu7bKnUo0oCu1CgTkW0icsFot8NLRGYCXwTmGWMm57j+XBFpOtDnMcaMMcZsGe7bKnUo0oCulMplJrDPGLN3qA8gIoFhbI9SagAa0JUqUyISEpGfichu+9/PRCRkXzdRRB4QkQ4RaReRRSLis6/7iojsEpFuEXlFRN6Y5/HHishdItIqIttF5DoR8dm9BY8BU+1u7t9n3a8OeNhzfY+ITBWRb4nI30TkbhHpAj4sIqeLyAt2O5tF5FciUuV5LCMic+zLvxeRX4vIg3bbl4jIUUO87YX2a+8UkZtE5GkR+fjwfDJKlScN6EqVr/8BzgTmAycDpwPX2dd9EWgCGoHDga8DRkSOAT4DnGaMqQcuArblefxfAmOB2cAbgA8BHzHGPA68Cdhtd3N/2HsnY0xv1vVjjDG77asvBf4GjAPuAZLA54GJwFnAG4FPF3jN7we+DYwHNgE3DPa2IjLRbsPXgAnAK8BrCzyOUgcFDehKla/Lge8YY/YaY1qxgtcV9nVxYApwhDEmboxZZKyFGZJACJgnIkFjzDZjzObsBxYRP1ZA/JoxptsYsw34sefxh+oFY8w/jDEpY0zYGLPcGLPYGJOwn+O3WAcP+dxnjFlqjElgHRDMH8JtLwHWGmMW2tf9AihZYZ9S5UIDulLlayqw3fP3dnsbwI+wstJ/i8gWEfkqgDFmE3AN8C1gr4jcKyJT6W8iEMzx+NMOsM07vX+IyNH20MAeuxv+Rvu58/EG3j5gzBBuO9XbDvtA54AL+JQqdxrQlSpfu4EjPH/PtLdhZ9VfNMbMBt4GfMEZKzfG/NEYc459XwP8IMdjt2Fl+dmPv6vItuVbpjF7+83ABmCuMaYBa2hAinyOoWoGpjt/iIh4/1bqYKUBXanyEBSRas+/APAn4DoRabTHhf8XuBtARN4iInPsYNWJ1dWeEpFjROR8u3guAoSBVPaTGWOSwF+AG0SkXkSOAL7gPH4RWoAJIjJ2gNvVA11Aj4gcC3yqyMc/EA8CJ4rI2+338Wqg36l3Sh1sNKArVR4ewgq+zr9vAdcDy4DVwBpghb0NYC7wONADvADcZIx5Emv8/PtYGfgeYBJWcVgunwV6gS3As8AfgTuKaawxZgPWAccWu4I9V7c+wLXAZUA3cCvw52Ie/0AYY9qA9wA/BPYB87Dex2ipn1up0STW8JJSSh2c7NP5moDL7YMepQ5KmqErpQ46InKRiIyzhx6ccfvFo9wspUpKA7pS6mB0FrAZa+jhrcDbjTHh0W2SUqWlXe5KKaXUQUAzdKWUUuogoAFdKaWUOghU9GpIEydONLNmzRrtZiillFIjZvny5W3GmMbs7RUd0GfNmsWyZctGuxlKKaXUiBGR7bm2a5e7UkopdRDQgK6UUkodBDSgK6WUUgcBDehKKaXUQUADulJKKXUQ0ICulFJKHQQ0oCullFIHAQ3oSiml1EFAA7pSSil1ENCAbtvTGeGPS3awtzsy2k1RSimlBk0Dum1LWw9fv28NW1p7R7spSiml1KBpQLfVVVnT2vdGE6PcEqWUUmrwNKDb6kJ2QI8lR7klSiml1OBpQLfVhfyAZuhKKaUqkwZ0m5uha0BXSilVgUoW0EWkWkSWisgqEVkrIt/Ouv4XItLj+TskIn8WkU0iskREZpWqbbnUBp0MXbvclVJKVZ5SZuhR4HxjzMnAfOBiETkTQEQWAOOzbv8xYL8xZg7wU+AHJWxbPwG/j+qgj76YZuhKKaUqT8kCurE4GXjQ/mdExA/8CPhy1l0uBe60L/8NeKOISKnal0tdVYAe7XJXSilVgUo6hi4ifhFZCewFHjPGLAE+A9xvjGnOuvk0YCeAMSYBdAITStm+bHWhAH1a5a6UUqoCBUr54MaYJDBfRMYB94nI64H3AOcO9TFF5CrgKoCZM2cORzNdtVV+zdCVUkpVpBGpcjfGdABPAucBc4BNIrINqBWRTfbNdgEzAEQkAIwF9uV4rFuMMQuMMQsaGxuHtZ1jQgEdQ1dKKVWRSlnl3mhn5ohIDfBfwHJjzGRjzCxjzCygzy6CA7gfuNK+/G7gP8YYU6r25VIbCtCjVe5KKaUqUCm73KcAd9pFcD7gL8aYBwrc/nbgD3bG3g68v4Rty2lMyE9zR3ikn1YppZQ6YCUL6MaY1cApA9xmjOdyBGt8fdTUVgV0YhmllFIVSWeK8xgTCuhc7koppSqSBnSP2io/vdEEIzx0r5RSSh0wDegedaEAiZQhmkiNdlOUUkqpQdGA7tFQbZUU6LnoSimlKo0GdI/66iAAXeH4KLdEKaWUGhwN6B4NNVaG3hXRDF0ppVRl0YDu4WTo3RHN0JVSSlUWDegeDW6Xu2boSimlKosGdA+ny10zdKWUUpVGA7qHWxSnAV0ppVSF0YDuUVflxyfa5a6UUqryaED3EBHqq4Pa5a6UUqriaEDP0lAT0NPWlFJKVRwN6FkaNENXSilVgTSgZ6mvDugYulJKqYqjAT1LQ3VQq9yVUkpVHA3oWcaEAro4i1JKqYqjAT1LXShArwZ0pZRSFUYDehYroCdHuxlKKaXUoGhAzzIm5CeWTBFLpEa7KUoppVTRNKBnqQtZ87lrt7tSSqlKogE9S12VFdC1ME4ppVQl0YCexc3QYxrQlVJKVQ4N6FnqQn5Au9yVUkpVFg3oWcaEnC53rXRXSilVOTSgZ9GiOKWUUpVIA3qWMRrQlVJKVSAN6Fk0Q1dKKVWJShbQRaRaRJaKyCoRWSsi37a33yMir4jIyyJyh4gE7e0iIr8QkU0islpETi1V2wpxi+JiOoaulFKqcpQyQ48C5xtjTgbmAxeLyJnAPcCxwIlADfBx+/ZvAuba/64Cbi5h2/IKBfwE/aLnoSullKooJQvoxtJj/xm0/xljzEP2dQZYCky3b3MpcJd91WJgnIhMKVX7CtEFWpRSSlWako6hi4hfRFYCe4HHjDFLPNcFgSuAR+xN04Cdnrs32duyH/MqEVkmIstaW1tL0u6G6iAdfbomulJKqcpR0oBujEkaY+ZjZeGni8gJnqtvAp4xxiwa5GPeYoxZYIxZ0NjYOJzNdU0eW82ezkhJHlsppZQqhRGpcjfGdABPAhcDiMg3gUbgC56b7QJmeP6ebm8bcVPHVrO7MzwaT62UUkoNSSmr3BtFZJx9uQb4L2CDiHwcuAj4gDHGu0bp/cCH7Gr3M4FOY0xzqdpXyJRxNbR0RUilzGg8vVJKKTVogRI+9hTgThHxYx04/MUY84CIJIDtwAsiArDQGPMd4CHgEmAT0Ad8pIRtK2jq2GriSUNbT5RJDdWj1QyllFKqaCUL6MaY1cApObbnfE676v3qUrVnMKaMrQFgd2dEA7pSSqmKoDPF5TBlnBXEmzt0HF0ppVRl0ICew6R6K6C39kRHuSVKKaVUcTSg55BeoEWnf1VKKVUZNKDnUB30IQLhmM4Wp5RSqjJoQM9BRKgJ+unTBVqUUkpVCA3oedRW+XXFNaWUUhVDA3oetVUB7XJXSilVMTSg51FbpV3uSimlKocG9DxqqvyE4xrQlVJKVQYN6HnUVvl1TXSllFIVQwN6HrVVAe1yV0opVTE0oOdRq13uSimlKogG9Dy0KE4ppVQl0YCeR00wQJ+OoSullKoQGtDzqAv56YsnsVZ1VUoppcqbBvQ8aqr8GAPRRGq0m6KUUkoNSAN6HrVBP4COoyullKoIGtDzqK1yllDVcXSllFLlTwN6HjVVVoaup64ppZSqBBrQ86it0i53pZRSlUMDeh7V9hh6TIvilFJKVQAN6HmEAtZbE01ohq6UUqr8aUDPIxSwMvRoXDN0pZRS5U8Deh6hoPXWRDRDV0opVQE0oOfhdrlrhq6UUqoCaEDPw+1y16I4pZRSFUADeh5aFKeUUqqSlCygi0i1iCwVkVUislZEvm1vP1JElojIJhH5s4hU2dtD9t+b7OtnlaptxXDG0DVDV0opVQlKmaFHgfONMScD84GLReRM4AfAT40xc4D9wMfs238M2G9v/6l9u1GjVe5KKaUqyYABXUTeIyL19uXrRGShiJw60P2Mpcf+M2j/M8D5wN/s7XcCb7cvX2r/jX39G0VEin4lw8zvE4J+0S53pZRSFaGYDP0bxphuETkHuAC4Hbi5mAcXEb+IrAT2Ao8Bm4EOY4yz4kkTMM2+PA3YCWBf3wlMKPaFlEIo4Ncud6WUUhWhmIDupKhvBm4xxjwIVBXz4MaYpDFmPjAdOB04dkit9BCRq0RkmYgsa21tPdCHKygU8BHRxVmUUkpVgGIC+i4R+S3wPuAhEQkVeT+XMaYDeBI4CxgnIgH7qunALud5gBkA9vVjgX05HusWY8wCY8yCxsbGwTRj0EIBn2boSimlKkIxgfm9wKPARXZgPgz40kB3EpFGERlnX64B/gtYjxXY323f7Ergn/bl++2/sa//jzHGFPk6SiIU1C53pZRSlSEw8E2YAjxojImKyLnAScBdRd7vThHxYx04/MUY84CIrAPuFZHrgZewxuSx//+DiGwC2oH3D+6lDL9QwEdUu9yVUkpVgGIC+t+BBSIyB7gFK6P+I3BJoTsZY1YDp+TYvgVrPD17ewR4TxHtGTHa5a6UUqpSFNPlnrKrzt8J/NIY8yWs7PugZ1W5a4aulFKq/BUT0OMi8gHgQ8AD9rZg6ZpUPkJBzdCVUkpVhmIC+kewqtNvMMZsFZEjgT+UtlnlIRTw60xxSimlKsKAAd0Ysw64FlgjIicATcaYUZ2WdaRYGbp2uSullCp/AxbF2ZXtdwLbAAFmiMiVxphnStu00WdNLKMZulJKqfJXTJX7j4ELjTGvAIjI0cCfgNeUsmHlQKd+VUopVSmKGUMPOsEcwBizkUOlKC6gXe5KKaUqQzEZ+jIRuQ242/77cmBZ6ZpUPrTKXSmlVKUoJqB/Crga+H/234uAm0rWojJSHfATS6RIpgx+36it5KqUUkoNaMCAboyJAj+x/x1SGmqskYWeSIKxtYfEKINSSqkKlTegi8gaIO/iKMaYk0rSojLSUG29PQtfauL1RzdyVOOYUW6RUkoplVuhDP0tI9aKMuVk6N/+1zr8PmHzjQWnr1dKKaVGTd6AbozZPpINKUcN1elu9mRqVFdyVUoppQoq5rS1Q1ZDTTE1g0oppdTo04BegDdDDwX0rVJKKVW+BoxSIvJWETkko5kzhg5QX61V7koppcpXMYH6fcCrIvJDETm21A0qJ/WhdJe7U/GulFJKlaNiVlv7IHAKsBn4vYi8ICJXiUh9yVs3ynyeyWTqazRDV0opVb6K6ko3xnQBfwPuBaYA7wBWiMhnS9i2slIb9I92E5RSSqm8ihlDf5uI3Ac8hbUoy+nGmDcBJwNfLG3zykfS6GlrSimlylcxA8PvAn6avf65MaZPRD5WmmaVn3hSF2lRSilVvooZQ78S2Ghn6m8Vkcme654oaevKwI/ebc1wm0hqhq6UUqp8FdPl/jFgKfBO4N3AYhH5aKkbVi7es2AGF847XDN0pZRSZa2YLvcvA6cYY/YBiMgE4HngjlI2rJwE/T4N6EoppcpaMVXu+4Buz9/d9rZDRsAvJHQud6WUUmWsmAx9E7BERP6JtZzqpcBqEfkCgDHmoF8nPej3EU9ohq6UUqp8FRPQN9v/HP+0/z/oJ5ZxBP1CXDN0pZRSZWzAgG6M+TaAiIyx/+4pdaPKTcDnI6Fj6EoppcpYMVXuJ4jIS8BaYK2ILBeR44u43wwReVJE1onIWhH5nL19vogsFpGVIrJMRE63t4uI/EJENonIahE59UBf3HCxiuI0Q1dKKVW+iulyvwX4gjHmSQARORe4FXjtAPdLAF80xqyw531fLiKPAT8Evm2MeVhELrH/Phd4EzDX/ncGcLP9/6gL+kWr3JVSSpW1Yqrc65xgDmCMeQqoG+hOxphmY8wK+3I3sB6YhlVY12DfbCyw2758KXCXsSwGxonIlGJfSClplbtSSqlyV0yGvkVEvgH8wf77g8CWwTyJiMzCWrFtCXAN8KiI/B/WAYWT6U8Ddnru1mRvax7Mc5VC0O8jmTKkUiZjBTallFKqXBSToX8UaAQWAn8HJtrbimIX0/0duMZete1TwOeNMTOAzwO3D6bB9tKty0RkWWtr62DuOmRBv/U2xVPa7a6UUqo8FczQRcQPLDTGnDeUBxeRIFYwv8cYs9DefCXwOfvyX4Hb7Mu7gBmeu0+3t2UwxtyCNa7PggULRqQfPOi3svJE0hAqpk9DKaWUGmEFM3RjTBJIicjYwT6wiAhW9r0+a/KZ3cAb7MvnA6/al+8HPmRXu58JdBpjRr27HazT1kBXXFNKKVW+isk3e4A1doV6r7PRGPP/Brjf2cAV9n1X2tu+DnwC+LmIBIAIcJV93UPAJVgz0/UBHyn2RZSak6HrqWtKKaXKVTEBfaH9z2vAyGaMeRbIV0H2mhy3N8DVRbRnxLlj6JqhK6WUKlPFBPRxxpifezc4k8QcKgJ2QNc10ZVSSpWrYqrcr8yx7cPD3I6y5na5a5W7UkqpMpU3QxeRDwCXAUeKyP2eq+qB9lI3rJxol7tSSqlyV6jL/XmsSV0mAj/2bO8GVpeyUeUm4EuftqaUUkqVo7wB3RizHdgOnDVyzSlPToa+qyNMdyTBWUdNGOUWKaWUUpmKWW3tnSLyqoh0ikiXiHSLSNdINK5cOAH9k39YzgduXTzKrVFKKaX6K6bK/YfAW40x60vdmHIV8Ov87UoppcpbMVXuLYdyMId0lbsjqSuvKaWUKjPFZOjLROTPwD+AqLPRMzf7Qc/pcnfEkyn8Pv8otUYppZTqr5iA3oA1FeuFnm2G/rPHHbScudwd0USK6qAGdKWUUuVjwIBujCmbOdVHS1Ugs8tdz0dXSilVboqpcj9aRJ4QkZftv08SketK37TykZ2ha0BXSilVboopirsV+BoQBzDGrAbeX8pGlZvaqszu9XhCi+KUUkqVl2ICeq0xZmnWtkQpGlOuGmqCGX/HNENXSilVZooJ6G0ichT2kqki8m6sKWEPGdVBPzWeIjjtcldKKVVuiqlyvxq4BThWRHYBW4HLS9qqMjS2Jkg4ngQ0oCullCo/xVS5bwEuEJE6wGeM6S59s8pPdTDdmRFLaEBXSilVXorJ0AEwxvSWsiHlrirgCeiaoSullCozxYyhKzIDelyXUVVKKVVmNKAXqcoz/Wtcu9yVUkqVmWImlnmPiNTbl68TkYUicmrpm1ZeMjN0DehKKaXKSzEZ+jeMMd0icg5wAXA7cHNpm1V+vAu06Bi6UkqpclNMQE/a/78ZuMUY8yBQVbomlaeQjqErpZQqY8UE9F0i8lvgfcBDIhIq8n4HlY+dM9u9rKetKaWUKjfFBOb3Ao8CFxljOoDDgC+VtFVl6KyjJrD8ugsAHUNXSilVfoo5D30K8KAxJioi5wInAXeVtFVlKmh3u2tAV0opVW6KydD/DiRFZA7WFLAzgD+WtFVlyjl1TYvilFJKlZtiAnrKGJMA3gn80hjzJays/ZDjVLrr8qlKKaXKTTEBPS4iHwA+BDxgbwsWuD0AIjJDRJ4UkXUislZEPue57rMissHe/kPP9q+JyCYReUVELhrsiyk1v0/w+0S73JVSSpWdYsbQPwL8N3CDMWariBwJ/KGI+yWALxpjVtgT0ywXkceAw4FLgZPtcflJACIyD3g/cDwwFXhcRI42xiTzPP6oCPo1oCullCo/A2boxph1wLXAGhE5AWgyxvygiPs1G2NW2Je7gfXANOBTwPeNMVH7ur32XS4F7jXGRI0xW4FNwOlDeE0lFfT7iOppa0oppcpMMVO/ngu8CvwauAnYKCKvH8yTiMgs4BRgCXA08DoRWSIiT4vIafbNpgE7PXdrsrdlP9ZVIrJMRJa1trYOphnDosrv0wxdKaVU2Smmy/3HwIXGmFcARORo4E/Aa4p5AhEZg1Upf40xpktEAljnsp8JnAb8RURmF3oML2PMLVjV9ixYsGDEq9OCGtCVUkqVoWKK4oJOMAcwxmykiKI4ABEJYgXze4wxC+3NTcBCY1kKpICJwC6sU+Ic0+1tZSUYEP6yrIm7Xtg22k1RSimlXMUE9OUicpuInGv/uxVYNtCdRESwFnJZb4z5ieeqfwDn2bc5Gmte+DbgfuD9IhKyC+/mAksH93JKr6MvDsD//nPtKLdEKaWUSiumy/2/gauB/2f/vQhrLH0gZwNXYBXTrbS3fR24A7hDRF4GYsCVxhgDrBWRvwDrsCrkry63CneA7kgCgCMm1I5yS5RSSqm0ggFdRPzAKmPMscBPCt02mzHmWUDyXP3BPPe5AbhhMM8zWmYepgFdKaVU+SjY5W5nyK+IyMwRak/FqKsqpnNDKaWUGhnFRKXxWN3hS4FeZ6Mx5m0la1UZO3HaWNbs6iSSKLvRAKWUUoewYgL6N0reigryr8+ew3t/8wKRuAZ0pZRS5SNvQLdXVzvcGPN01vZzgOZSN6ycVVf56QzHR7sZSimllKvQGPrPgK4c2zvt6w5Z1QEfUc3QlVJKlZFCAf1wY8ya7I32tlkla1EFqA76tctdKaVUWSkU0McVuK5muBtSSaqDPiJxnf5VKaVU+SgU0JeJyCeyN4rIx4HlpWtS+asO+rXKXSmlVFkpVOV+DXCfiFxOOoAvwJqq9R2lblg5q9Eud6WUUmUmb0A3xrQArxWR84AT7M0PGmP+MyItK2OhoJ9IPIUxBmvKeqWUUmp0DXgeujHmSeDJEWhLxagOWiMV0USK6qB/lFujlFJKFbfamspSHbCCuHa7K6WUKhca0IfAycqHs9J9yZZ9/OzxjcP2eEoppQ4tGtCHwOlyH84M/X23LOZnj786bI+nlFLq0KIBfQjcDF1PXVNKKVUmNKAPQTpD18lllFJKlQcN6EMwHEVxiWSKnmii3/ZUygz5MZVSSh26NKAPQSh44AH9D4u3c8GPn+63PaEBXSml1BBoQB+CmmEI6Hs6I+zpimBMZgBPpLQbXyml1OBpQB+CsbVBAPb3DX1N9GjCCtzxZHZA1wxdKaXU4GlAH4LD60P4BJo7wkN+jHgylfG/I5HUgK6UUmrwNKAPQcDv4/CGanZ1RIb8GE4gzw7g2uWulFJqKDSgD9HUcTU0dxbO0Fft7OCxdS05r3O62mOaoSullBoGGtCHaMrYanYP0OV+yzNbuOHBdTmvi2mXu1JKqWGkAX2Ipo2rYXdn/yp1r75YIu/kM/GEdrkrpZQaPhrQh2jK2GpiiRRN+/Nn6ZF4Ku/0sE5m3q/LXavclVJKDYEG9CF6wzGTCAV8XJ+nSx2sud6jeTJ07XJXSik1nEoW0EVkhog8KSLrRGStiHwu6/oviogRkYn23yIivxCRTSKyWkROLVXbhsORE+u47IyZPLmhNe90reFYkmgimbNbPp6wtvUL6NrlrpRSaghKmaEngC8aY+YBZwJXi8g8sII9cCGww3P7NwFz7X9XATeXsG3DYtaEOmLJFPt6YzmvjyZSpEz/yWPAm6HrxDLq0HPTU5u48aH1o90MpQ4qJQvoxphmY8wK+3I3sB6YZl/9U+DLgDd6XQrcZSyLgXEiMqVU7RsOU8ZWA+Q9fS0cs8bPoznG0XViGXUoW7Klnec3t412M5Q6qIzIGLqIzAJOAZaIyKXALmPMqqybTQN2ev5uIn0AUJamjqsBYHeeCWacgrhcle55A7p2uatDQDJl9OBVqWFW8oAuImOAvwPXYHXDfx343wN4vKtEZJmILGttbR2mVg7NQBm6s3hL7gw9PYae9HSz605OHQoSqRSpAqd8qsq3p3PoM2mqoSlpQBeRIFYwv8cYsxA4CjgSWCUi24DpwAoRmQzsAmZ47j7d3pbBGHOLMWaBMWZBY2NjKZs/oMPqqggFfDTn+OIaY9zMPFeGHvMszuLN0pM6hq4OAamU1osczDbt7eHM7z3BmqbO0W7KIaWUVe4C3A6sN8b8BMAYs8YYM8kYM8sYMwurW/1UY8we4H7gQ3a1+5lApzGmuVTtGw4iknfGOGc1Nety/wzde9qa91z07C744fQ/963RcUtVFhKplB68HsTaeqIZ/6uRUcoM/WzgCuB8EVlp/7ukwO0fArYAm4BbgU+XsG3DZs6ketbs6n8U6l0rfaAx9Lgn+JcqazHGcM+SHSx6VQO6Gn3JlNGAfhBzPlvthRlZgVI9sDHmWUAGuM0sz2UDXF2q9pTK6+ZO5PH1LezY18fMCbXu9rAnoOccQ3e63BMm49Q17w/g+U1t1IYCzJ8x7oDb6Y7ZJ7ToTo2+pNGAfjBz9mNJLfIdUTpT3AE6Z+5EABZtyizQ82bl0RxB1A2wqVRGN3vCc/mGh9bz88c3Dks78001q9RoSCSNZm8HMSeQ62c8sjSgH6DZE+uYUFfFqp0d7rZUyrBwRZP7dzSemaEbY9Jj6IlURsD3/gB6owl6Y7nngh+sfKfJqZHTHYlz/QPrMoZjDlXJlMk7w6KqfM7ZOtoLM7I0oB8gEeG4KQ2sb+52t9330i5++Z9N7t/ZGbo3aGdXuXtPW+uLJd3JaQYST6YKBgrnACKWOPR+YMYY2vPM5jeSlm5t57Znt7J2d9doN2XUJU35Z+irdnbwnX+tK7iiosrNHUPX03BHlAb0YXDs5Ho2tnS73eUt3ZmnsWUHWm8Az+5y9445hWNJ+mKJotpw7V9Xcew3HqGzL57z+vRpcodehv6757Zx6gEQFYIAACAASURBVHcfY1tb76i2I+Hu5ErzGTy9sZXP/3llSR57uFVCUdx/Nuzljue2lv2BRzlKj6HrezeSNKAPg+OmNBBNpNi2zwoY2Uel2Rl6zPO3VRTnPW0tfd9wPJl3PfVsj69rAeDmpzfnvN553NghWBS36FWrvmHT3p5RbYfzvShVgLjyjqXc99KuisgoE8nyD+jOrI2aZQ6eVrmPDg3ow+DE6WMBWLZtPwAtXYUz9Ozzzr0B3/khxBIpEilTdIZ+3JQGAPblOe/zUB5Drw76AfKuTT9SnABR6s8g12JA5SZVAVXuCU/hqhoc5zuuVe4jSwP6MJg7aQzTx9fw2LoWYokUO/dnTjRz40MbMormvDvceDKV+bf9A3DGzvuKHEN3DgryVbHHBrj+YOYG9CJ7O0plpMYVK2E9gETKlH07nd+lZuiDpxn66NCAPgxEhAvnTeaJDXs5+rqHeWZj/znm3/PbF9zL3nPB40mT8XfS3nmE3XngU0VVAzsBO5onaB3aGbr1NR/t6vJ0l/vgPoPl2/ezYU/xhXSVkKEnU4aUoayHB9xTrw7B38yBOhjG0HuiCWZ99UHuX7V7tJtSNA3ow+TDr53lLtaSSyyRoitiFazFs7rcM4vkrB+At6s9XEQgciavyTWJjfU8JuP/Q0k6Qx/tLvehfQbvuvl5Lv7ZouKfpwICULIEO/z7XmrKWxQ6FM5vMV7BQWm0DDZDT6YMy7fvL2WTBm2X3dP6yydeHeWWFE8D+jCZOaGWRz//eu696kwA3nXqdO786OkZt/m/R19h7e5Omjxzv2ePoTs7Y29XezHd7gN1ubsTyxyCRXHlE9CdccWhBYhiz9uuhG7O4e6S3djSzef/vIpr/5a9KvPQOb/FSjhAKjeDPaPjPxv28q6bn2dne18pmzUoYs9zWv6/prSSTf16KGqoDnLm7Ak8es3rmXlYLTVVfve6y86Yyd2Lt/PPlbszMvlYMp25Q3pHF86YC774gJ6vyz1WwV3uiWQKv08QKTiTcF5++37DNUnPUCU8S+YOxbZ9vcxuHJPzuuxen3LnHNwM1xKqPVGrR2tv9/AtBuI9KyGaSPLwmj1cOn/qkL+Hh5LBzhTXFbb2gd2R4oqAR4LzKVfSMr+aoZfAMZPr3WC+4IjxXH7GTM448jBSBjrDcTa3pk+fSiQNHXY3YSjgc7tjw3ky9Oc2tdEZ7t+tGPWMuecSr9CiuGTKMOd/HubGh9YP+TGcQsOeUd5ZJA5wOsxciwA5OjxdzZVQxOWUEQxXhu4cCAd8wxds454ixqdeaeWaP6/k1VE+9bFSDHYMPf3bKJ/9k/vdLP+fk0sDeon97VOv5YZ3nMi0cTXutuwq985wnLoqP9VBv3tkm9nlbgWiznCcy29bwtX3rOj3PG6Xe76AfoDZ4WhxXs/vnts25MdwAlx3ZPjGV4fUjiFOLBMKWD/THfvyd0d29KVnwiunnWI+boY+zAHdP4wBPek5zdDpJSt25sZDXXKQcy7ESzxHw1A4+57yadHANKCPkGnja3JujydTdPTFGVsTJOgXNysIxz1FcfZOpNfuVlzd1JHxGMlUehrN/EVx6dXdKonTo3Ag3V5OAHW6ZUdLYoiFic6tCx2Meae2LffCx5Rd4Q7DtwN33lv/MHaHe4NMfIhnKByqBp2hu/UK5fPdjQ/DvmekaUAfIZPqqwn6M3c2Qb8QSxo6w3HG1lYR8PncI1tvhh6OJ2ntjrqBPHsn6M3Kd3WEue4fazLG3Xe29/HEhr3WbSssQ0//qIb+GDH7Pe0a9S73wQcFY0x62t4Cb4L3tZXTTjGXpGcHOVxV7rGk9X0P+IcvoHuL4hJuDUp5v7flYrCz7JV6WuShcPaVFRTPNaCPFL9PmDI2M0s/bkoDiWSKznCMsTUB/D7pN7EMWMH9DT96kv++2+pqzw7o3qw8njTcvXhHxnjr7c9u5V/2uZSVth76cAwRuBn6aAf0IQSFjFkFC3x22esDlDNvEB+ugO5MGuQrkKEv375/UM/nPc3wUJ7HYSgGux56ejnp8omeTptMBXW6a0AfQd5x9KBfqKsKEEtYY+jjaqoI+iVd5e7N0GPJjIw9e6fkjJ/XBNNV9bs9p8Z5i+gqLUMfjtPsnJ1Ld7T4MfQHVu/mzue3HfBzew1lpriMef8LfHb5VuwrR6UJ6HaGnmcMffn2dt518/P8+slNOa/PxbsEqM4aNziDHUMvx1MEnd9emR8fZ9CAPoI+d8Fcfva++UyqD/HLD5zKlLHVbGnrpb3XGkP3+4RE0rC6qYMfP7bRvV/2xDL9ArqdnTTUpM9C3OUJ6N6x40rLMIajvfEhZOif+eNLfPP+tQf83F5D6XL3nrVQKHuJ5ZjLoFx5d/LDNYbuZOj5iuLaeqwag9VN+c8UyObOvZ9KuZcr7YB4tAx2DD3u6Q0pF3G3y7182jQQPQ99BJ05ewIAbz9lGmBVry98aRcA42qDBP0+EqkUf1velHG/QhPLbGzpdnfm9dVBWrqs83B3eeaT7/UE9JSxfmTDWQ1cSsOxfrvzw8x3St9IGUqXe0ZAL9D+jK75Muq2zCVVygw9zxi6c6ZAvqLRXLxZ+cGWoXf0xfD7hPrqYEkef7ATB7kZ+iimw2/88VOcNH0cP33ffMAT0EetRYOnGfooOmfuRPdygydDd4LxO0+ZRn0owMaW7n73NcawYU8XF/70GX5qZ/P11enjM2+Xe29WdXclzRaXK0PviSa47h9r+r2ufJyd8KgH9CEU/hTd5V6hGfqwBXQ7UOcbQ3cmg8k38VIuCc9c7vEyCDjDaf53HuPMG58o2eM7n/H9q3bzq/8MPHVqYgjDUcNtc2sv99kJFnhOW6ugiK4BfRRNqq/m+KnWsqehgI9wPMkTG/byxIa9XDp/Kj9533xOPWI8D65p7nffSDzFQ6ut7c467A2eo+3dHeklXLNnSKukbsPstt781Gbe/uvnuHvxDu58YZu7fW9XhCfWt+R8jLin+280g91Q1kPPLnjMJ3Nug/LeAyUzutyH5/OIxAoH9PTES8Vn6AlPoZZzuZIOhgdSypkTnWK4WCLFvS/uHPD25Vh0OBynzI40Deij7I4Pn8br5k7kdXMbM7YfZU/xedqs8Tl3Ivv7Yvx7nRXAnO9bQ403oOfP0MvpRzOQ7G7mPy3dwSZ7ti7v7+z9ty7mY3cuy5nxeR9jNA9mhjKGXmyG7n1d5Z5Fek9bG66mRhKF58l3p0YeRED29qikM/Ty2rnv2NfH+f/3FHs6IwPfeAR536diDoKGcrBbanGdWEYN1uEN1fzhY2dwzOR6fvmBU/j5++dz7OR6LjjucADOsMfds7X3xtjSamXmTgGct8u9O5pw54jPnlDlQAP6sm3tLN/ePqj7GGN4acfgV1PKDsD5Vp5z3otc8957A9xgulyH22DPzYWsMfQiq9z39cQ44ZuP8vymtiG0cmDPb2o7oEw1mSxBhm5/7vkO2CIDTI2cS8ITxMthpsWP37mM7z6wLmPbxpZutrT15hyWG03e73gxB9He4Y1y4Z625vm5bm7t4Z03PTfqs07mowG9jBw/dSyXzp/GI9e8nnl2V/yCI8bzhqMb+932uU1t7g/F2Uk1ZBW47O4IY4wZ9jH0d//mBd518wsD39Bj6dZ23nHT86zbXfy63tC/rRFPN2Gu3tVcBYTxQe5cvIZralIY2vKp3gOQwl3u6dvtbO+jJ5ooyU6+aX8fl922hMfW5R7eKIY3iA93UVy+gJBevGhoRXFuxfsoDmc8vr6F25/dmrHNeV3OoiY90cSAVdkjUbXt/VyLOYgux+Wd0xPLpNu0pqmTFTs62FFGq8J5aUAvcyLC7z9yGsuuu8DdNmtCLd97eAMAtZ4V3bwZOlgBPRJP9ZtlbTSyjH321KTeKUqLkf0D92boQjqiO8F9uDP04eyiH0olrzMDWk3Qn/G5Ld++n1lffZA19mlY3vdpv71QS2d4+CfSceY0yLVAULFSJZgpzjltLV9AGFqXe/rzcrvcyyiDhHRNQHckTmc4zmnXP87j6/cWvM9IDDt5v+NFZehlWHTonofu+b467/doF9jmowG9AogIE8eE3L8/eOYR7uWTp49zLzfYAb3Kb32suzoiOecvH45TwQbLyZydhWaKlb0sqHeMzZuhOxdzdcl756/PVRTVtD/zaNu70x7Ond9QJpZxDkDqQoGM98IpAHzqFXtKX88OpjNsHTR1DaFbMJZIFQxaTuA8kLXlS1Llbrcn38Gqc/1g2p309KiUQ5d7Ls7n0RWJ09YTJRxPDpg99kVLv8BM9uRBA33Oo30eeq7vfLpQL90m5/32JgYPr2nmf+5bU+IWFkcDegU6/9hJ7uWTpo91LztFcVPGWfPG7+4I5zy169lNrcM+C9pAwnYgzzcGno83UGXvjL1Hzk51c1tPtN9ON55KUeWeh5x53dMbWznnB0/ysOdMgoi3iG6AI/GlW9v5zB9XFFf4kxp8UHAOKMaE/Bk7Fuf1OvvJWDLlrhWQztAHH9CPvu5h3n/L4rzXO13WkUFUi2fzHtAkC3T/PrG+hZau4oq9wgMEdOdzD8eTRXc5p7vcUzl37uUgnaEn3EDdN8DpnH0HcDBWrELrTeS8/SgvzpLrc03PX5F+v9IZenrbM6+28s+Vu0vcwuJoQK8gP3vffP748TM4cmKdu236YbXuZafLPej3MXlsNbs7wqxr7j9mffuzW7n+wXVDyrKGmlGlM/RBBnTPDjp76cqI5yjZydYvu3UJH/39ixm3SyQNY0LWe5Md0F+257xfuTO9gp33eQp1re3vjfHe377AA6ubM84qyGdIp63lydCdeYGceabjiZQ79e9+eynVriF2iy/bnr940QnkkQMoLvQeiOV7L4wxfOzOZbzzpueLekznfcr3eM4OOGWK/w56i+ISI5Sh7+uJcuI3Hy26gNQ7ht5rHzQPdDqaN+CXajw9ez8xcEAf/BkgxeqOxLnz+W0FX2uu9rmLIiWNW0vjZugZiUbqgHqshlPJArqIzBCRJ0VknYisFZHP2dt/JCIbRGS1iNwnIuM89/maiGwSkVdE5KJSta1Svf2Uabx2zkREhC9ddAxXnnUEE+qq3OudWZ8CPmHmYbUserWNT+dYO72lK0o8adxgNhjFTiNrjDWFrfMjCg9xPWnvc2R3IUfzjKcvejWzujueTFEXsoJddpe7k+l6f+reH2ehHdF2T9dmMUEiXVhV/E7Lae+YrIDuTJSy6NU2/rlyF/Fkitoq66Cl8wAy9IGEY4MvLsuW0eWelRn1RhP8c+Uud4e5q4gDJUgfaOT7vLxdpLneF2MMy7a1Z+z0454elZE6ba25M0J3NMFm+6wNR97T8Zwu93Dc7Y3LN6zVtL+PWV990D3dFUo3FpydaUeThb8vpexyf2xdC9+8fy1b2nrz3ibX0FosRzFtrjH0SDxpH/SN/nBMKTP0BPBFY8w84EzgahGZBzwGnGCMOQnYCHwNwL7u/cDxwMXATSLiz/nIiqvPm8O3Lz2Bs+dMpD4UYFxt0F2YIuj3ccFxhw9YgLbCkwW87VfPcuND6wd8Xu/pGoVmalu5s4O3/eo59zmcQH4gXe4dfVkB3bszKjCTbTxpqLODXfYO38nsvdXsuQL6pr09bNiT2dvhzXS869fnk/BUTRfLeY1WQO/f5b58+34+d+9K4knjFki6GfogV5crJlsbylh0tozx1aznvP7BdXzu3pU8++rgTrlzq9ztx+6JJnjTzxe5B63eA7lcB18rd3bw7t+8wIod6Z4ap53W4izW51DqiWUi7oFv5mfnbX/GMJS9vSuScDPz3jxj5E4v1K2LtqQft0SnceZbbyKXhSua3M+pFEHR2U8V2l9lzOOQY5KbqFs70v+ANjyEUyJLpWQB3RjTbIxZYV/uBtYD04wx/zbGOO/sYmC6fflS4F5jTNQYsxXYBJxeqvYdLMbWBFn5zQt57ivnuz+igF9468lTB5yvfcV26wfe1hNldVMntzyzhRc27yu4Y+/2BIlcBXeOnfb0tbvsGeuGoyguO6B7g0qhl5pIpfJ2uae7rtO8Bx3OjvSCnzzNxT9blHFf7+svLkNPB4hiOe2tCwUydnbZp+zFkilCQT8i6XH1wXa5F7NDGo4u90KrrTkTpDR3Fs7Mb1u0JePUuXSVu/X/sm3trG/u4oePvgJkBpRcvUTOwW+HfTBkjMkoihvKpEBD4fZkZR0wedvvDUzO9u7IwBl6KGAd8Hl/R4M9wC5W9vuUr7g0HEvyhb+sct//UvSA9A1woAOZk09FEjkCejJz7Dw7Q/f+P5pGZAxdRGYBpwBLsq76KPCwfXka4J0jsMnepgbg9wl1oQDzpjZw7OR6vvGWeUwcE+Kuj57OfZ9+LZ98w2yOn9rAabPGu/ep8vtYsWM/xhie37zP3f6BWxcXPO3FG9AL/UD29ViLxLTb/w80ht4XS/C35U39Dia83V4d4fwBXQqk6ImkoS5PQHd4x3W9O/zNrT289ze5z7nvjWUG9Fue2cyrLd30RhO02a87ox2e1buKFfMEdO970a/wL5miyi8Efemf9GADejEHJW6V+wEUxWVO/Zr5eQftMzQG6l343XPbuO+l9CJGThBzej+cz9M5YPN+7rkCXnYg9faGZMwUV+KirXCe34m3/d4Dyag3Q48WHkPPNYVpqYJQsUVxL+/OHPYrRY1CMclELCMb7z980y9DzxpDz942Wkq+2pqIjAH+DlxjjOnybP8frG75ewb5eFcBVwHMnDlzGFta+WqrAjxyzevdv8+eYy3+csrM8fAma9usrz4IwBuOaeSxdS3s6gjz1IbMAO5UFvfFEvx1WROXnTHT3dH2eNYUz87QN7Z0c/uirVz/jhPcgNZuZwNOl3S+MfSH1+zh2r+uYv6MscyZVO9uz+xyzxxC8GaJ2Rl6KmXw2RtjyXSGnr1jcX6o3n2dN2tZsqWdpdtyz4rnPaDZ2xXhxoc20NodJZZIsWRre8ZnAcV3ub/1l89SW+Xnz588i2giRZXfR5VfMnZ22TviWMKq5A/4Bect7o4mBrWynneHZ4xxx+m9hiMb8e7ssyfuCdpnI+zrKTxc1BdLuO+/McbNOp0ds/NW+e3X4G1vripvZ6fvfD+9GaZ3priBTmN0Xo9viKsZOtlh9u/E2+XuPZBMF8XF3fcjX5V7rt9eyTL0ZHEBfaVniMN7v7aeKH9asoOrz5sz5PfS4Xz2hYoFYwNl6FnzGHg/j0MmQxeRIFYwv8cYs9Cz/cPAW4DLTTol2wXM8Nx9ur0tgzHmFmPMAmPMgsbG/jOoqcKOOdwKlhcfPxmAR17ewwOrm5lxWI17m9ZuKxj/+cWdfPP+tTz88h73uj2d6czzvpeaMn4IX1+4hj8v28nKnR20dTsTyfTP0HujCS791bMs91RT77Nvl70j9/6osouZIokk2/f1sr831i/4eAvoEgWK4pydeyzZ/4jb+144NrZ0uxmmt+vTmV++uTPCro5wxvK1bjsKnLbmFCw9vKaZNbs6WbK13W1vKOCzltb1Vvxn7Tx6Y0mCfp9bR+Fwah6eemXvgNmPd4efb0fvjvEeSJV7Vob+9+VNnP39/7B4yz53DoWBTlfrjSXdA5DuaKLfSnbOdU4wiCZSbo1BJMeOPXun7D3oiCdTRZ9W9Ym7lvHN+9cWvE0hkTwZuvc72ePpvXDa2x1JuK85X+DqzZGhlioI9atyz/Pdyz5YdnqvvvK31fz4sY0ZdT6D0RmOu7NSpg/WBpmhez7raCLz++EdAkl/d0Y/Qy9llbsAtwPrjTE/8Wy/GPgy8DZjjHcGhPuB94tISESOBOYCS0vVvkPVrR9awMfOOZK3nDyFyQ3VXP/gemLJFDe+40T3Nk12MPr3WmuM8p/2koKPvNzM1z0TKNy9eAcPrE6ff+kUOK3a2eFm6Pt77QzdUxS3YU83q5o6ec4z13i7fbv9ffkDeq4x9Df86Cne8stn+43ZbW3rZcOeLlIpQ8qQ7nLP+tG5XZzR/jtJgNasrvMLf/oMv7SXg/QG9FftgL6nM0JXOGEFmaydWKE1op2dz99XZB7DOpl3MODLOcGFo7MvRtDvc3tSHF3hBCt27OfDv3uRH9njyfl4A0F3ni7v4c7Qk6kUL2zZx66OMD969BW32M87hp49DJNIpoglUm5G2mF/d8bWBN33yOk9cjL0aCLJ+FrrjJBcQwt9nu+n9RzeLvfMiWW6I3He/uvn+hVJAmxp62VLW09xb0QOzlBGvzF0z4FoZpd7OkPvHmAMPdeEMkPJ0G95ZjPv+c3zLNmyL+9t+o2hJ1L0RBMZp4dubOnm8awVEp333Tl4H6jeJJpI8uZfLMrYlwDc8exW3nXz8yRTZvBj6HGnALJ/IWWuGQed23eEY3zpr6tyDreNlFJm6GcDVwDni8hK+98lwK+AeuAxe9tvAIwxa4G/AOuAR4CrjTGj34dxkJk5oZZvvGUeoYCf773zREIBH9+99HjOmTORb751HtPG1fD3FU1c/LNnWLqtnfpQgKc3trKrI5yRqTs2tvRgjOFrC9fwkt19tmLHftrsIhcn8/aetrbNPn3EO6OV052+PytoZ3S5Z2XozsHCLnuKW693/+YFLv7ZIjcDz1cU5wT03jzZaXaGDvDitnbuX7Wbvy5vcies8Wbozs4oexy40PShzn7Lm2AbY9jbHWV8XRVBnxBLptzglh1QO8JxK0O3J5dxutk7w3H3NS7dWnhBHW8gyDf+nqvKd7CSGXO5pzPOfT1R93V5Vw/L/mydz9Rpb7v93ZlUH3LfY+eAxCkpiMRTjKsNZtw/4zHdLK7/Z5TwVLnHk4bt+/pYubPD/b579UQTBQPHQMJZXf8O7/fW+/jO9pRJf1fzPX+uDN05wL3j2a088nL/ZZpzeWB1My9u259zWWdHdiC+/LYlnHXjE7z918+5B8L3vbSLgE84eUZ6tkvnQMAZ7x+oYmFfT4y1u7tY3ZQ5Fr+v15o1z5o9z3q+7zywjqvuWpbzcTIy9ES6jsKZ26OjL86mvd3ssQ80vQdYzv7ipR0d/HV5E0u2DG7hquFUsjF0Y8yz5D6Z6KEC97kBuKFUbVKZzjt2Equ/daFb/fqRs49k0att7OoIs2FPNydMa+CrFx/HB29fwtnf/0/Ox7h78XY6w3H+tHSHu+2ZjW1uYHKqV72FKVuzAvr//vNlFtqZafapdt4fWnY37CsFFh9xdihO5luXZww9nBUcIHNnmutoO5WC3z69mebOCJPqQ7T3xthr70xbuiLE7bkBOsNx/CKs3tXB6+Y2ZlRNZ0vvyNLbeqLWezV7Yp2beSdThoBf+mfo4ThVASFgR7DJDdXs6gjTFYm7772zw+/oizEmFCCQlc17X3e+orTh6F70Hs8kUyk3IO/vi7tBp9nzWd/30i4uOyNdL+O0s8cOXE6vzqSGEJtbrQMrZ6jBOXaIJpIc3lBt3z9/13M4nuTKO5YydVy1e10ilcqocneGc3KtuNUXTRQsvhqI875mP4b3AC6zyj293TkIypuhFxhD/469itu27795wDY6B0v5enEgdy+U04PQ2h2lLhRga2svMw+rZdq4albZ5dDuCmdZ7cvH6a146pW9bGnt4YfvPgkRcQ9qmjsjGa/733kWFYrnGHKLJ1M01ofojiTo6ItxwU+W9buNddl6fGdfMZQpl4eLzhR3iHOCucM5Arv3qjP5x6fP5uw5E6gOpr8m08fXcO2FR7t/90QTGcH8c2+cS02V3w0IG1t6+N9/vpxRvesE9J3tfbR2R7nrhe1u4M4ufIsnjZtZbWkdfFfmyp3WGFyVPb7sHFnv742xra03ozsulbJmhPLuRHIF35QxbnAcEwpQ41kgJ5EybnDf1xPl1Osf44rbl9IdiRc89cnJUr07473dUbbv6+XIxjq3WMxpT3aGbgx2l7v1CToBqTMcdwNPa0+UVMow/zuP8aW/rc64f3ckzn88xZH5lod03psDmvo1q+Bsf2967nmnZ8Dby/71+9aw09Obk316lnP/xjEhUsYao3ffT885wuPsLncnC/dyHiscs4YovPUdiaTxzBqWoiucO6ClUobeWPLAMvQiTlvzdrl7pylu7kyfIpprlUDvgcBh9kHnUIZOnB6oQkuIFuoqdw7Mt+3rZdaEOqqDnt9PMvPAdqB55533YsnWdv66vMntaXO27+kMF3X2RkZFu2eSosPrqzPanH0bY4zbS9Jm1/8478vWtl6O/NqDI7q0rQZ0leHblx7Pje84kTOOPIyA34eI8K/PnMMvPnAKIvC9d57I1efNyXnfmy4/lc+cP4ffffi0jJXf7nphu/sDC8eTbha1pyvC85szx74eXduSsTONJ1JMqg9RHfS5P5iBeAvDnDG7YMBHKOBzf3wX//wZzv2/p9wdWl8swdk/+A9X/WH5gLOTJVLGXT2uOuh3i62yi8IfW9/i7tj29cQKFlY5R/Xene7qpg7iScNRE8e4r8k58Mm1Iw76fe5Bw1y7+LErnA6SsUTKzWbveylzrP7av67iniXpA7P8Y+j5nz9bLJHipG89ysIVTRnbs89D3++e+50OStl25JiVzwlczjDNJDsDj6dSbvud2ohIPElt0E9VwEdfjkmAnCBvHQAlMoZarEWB0l3u3W6Gnvk4zsFDrq7tYkU8Q1Ne3i73vZ62ReNW0SRk9iblymy9ge0Ke4GnSDyVswL97sXbc07uY4zJO6TkVeh88pbuKMYYdrT3ccSEOnfKYu/9nKGlfO9lR1+Mx9a19JssxvmuOwdozZ2Rfu9lroOdWI76lFgyxWF1VQR8wsaWzGQi11h6m/25ON+L+1fuxhjr/5GiAV1lmD6+lsvOmJlRNT738HredvJUttx4Ca+b24iIcNuHFvDdS493b1NX5eeSE6cQ9Ps4YdpYll/3X9x0+anu9c5OYPu+Pjbs6WbupDEYA3c8ty3j+Xe09/Gum9PzpsjCNAAAHkJJREFUd8eSVlHYtHE1FGuKp7t02Tbr4CDoE0JBv7vzaunK7B5r743T3Bnh8fUtbN7bw7GT68mnvTfmBqVoIulOuXrc5IaM261vTh+Zt/fFMqrcP37nMr77wDo+8ruldmZqZ5ueGoKlW622H9lY547VOwcFuTLkoN/n7tSd9neG4xk7Xm+bvLJ3WPlmGYx6Jpb5/XNb+cPi7TlvB1aPQFckwdcWrskIGt6dfcIOyNPHW59vvoDuZOid4XjG2G04nqSjL4ZPcKdBjieN+5p77aDfE0kQCvqoCfrznL5l3X63PRGS93PInsvdeezsrlXn4OFAVjNLH2BmV7lbf1cHfbzqyfhiCatbOFuuQNgbTTChroovX3wMHzrLCujO+5ftun+8zAdvz542xHo/ne9+wS73AmdU7O2K0NoTpS+W5IgJtRlLQDtd304PjROwd7b38cNHNrjB+FN3r+ATdy1j+77MleU6+uLctmiLewC4pzPSbwjiU/csZ29Wxp0rQ4/b+55xtcF+WbbTY+L9Ljn1Qs5BhXMQ6NS1jAQN6Kpo3iB/wbzDueKsWWz93iU8+5XzePLaczNuWxXwccmJU1j46df2e5x5Uxq45xNn0FgfYtXO/oVFAJf8fBFPb2xldVMnVX4fU+2AXl8d4LC6KiY3VOe8H1hdrw4nmwn4fVT5fXRF4nzfXksecHcI3uxmydZ2N8PNxbvcam806WYYZx01IeN2r3iqoFfu6HB3gPGk4fH1Ldz+7FaefKWV1Ts73eDgre7+99o9+ATmThrjjqE7vQq5xrCrPGPrcyaNwe+TjG5sgFf3pndM3tfs3alOHVvNz5941Z0cyMtb5f6tf63jG/942b3u7sXb+djvX3R7RdrtHpVoIsXR1z3s7vy8GdL+PuvgaHbjmH7PBfDry6yDQmcH/ZcXd3LzU5vd63tjCdp7Y4ytCWYc9DhZdF8swWfvfYnemPU51Vb52drW2682wmlb9lK6YO3YY57elewMPRJPcuND693ZEWPJ3Fnvt+5fy8fvfLHfdsfqpg73NK7+Ve7W4500bVxG7Ug0K6A7PTm5DiqcAPrpc+cwxu5Bi8STGcWm4VgyIxhnZ7Pe4F+oy71Qhm4NJVnv8xETajMydOdgwVl0yDmweWB1Mzc9tZmd9ufj9OKtz1p86tlNrVz/4Hp2tlufRa4M/dG1LfzCPlPFkWua1+5IgrqQn7E1wX71Ou6BrefAOt3lbn0vnO9M9qmkpaQBXR0QEWH6+Fq3uzPb0TkC4yffMJtJ9dXc+ZHT+dJFx3Ddm4/rd5t1zV1cecdS2nqipAxuBjeuNsjSr7+RM2cfBsCbTpiccb/6UMAdk3POtQcI+oVQ0McDq5v5zdPpgJBvAZOjGuvyTsjiHVfviSbcYDh3UmZQcnoBIF10BP131tvbe92g633sfb0xzp4zkXG1Ve5OoTuSoGl/X84s0wloANPG1dBQHbAzdG9AT2fiTsEgpIsGAX76vvm098ZyrrqWa97qR15u5oO3LeEXT7zKExv28qB9KmNbb2bQXG8f4Hh39k7X9mzPCoJebz5pCkdMqHUDenYG3xdN0tEXZ3xtlXvQE0um3CGe3mjSnbzkvafNoKbKz6JX2zjthsczH8d+P3fn6CFIZmfo7hi69b7+a9VubnlmC99/OL0WQlck3q/m4/fPb3NnYXzqlb185o8rMk7Je9uvnmOLvShLvollTpo+lqb9Yff1ReJJJnoOYI+yD4y8n7OjN5ZwP+cqv4+qgI+ucNytQQAry2z3BO3sU/Cc38vhDaGCUz8XGkPf2xVxVyecNq6G6owM3e75soOqk6Hv7XamBLb+d4Jl9mqSTVnzP+zpjOQ8qyF72CujKC6RJByzvldTxtYwvraK7En2nO+/98C63VMLAkNfv+JAaEBXJTUmFODow62dzCffMJvXHDGei+0gPG9qA1efN4djs7qqs63c2eEW771j/jQCfp+7Y7l0fnp24Ls+ejqrvnmhG9gumHe4e91rjhhPKOAruKPxSXrinckN1W6BWSE90YRbFDc1x7CAcyBSyPZ9ff3GI8+eY2X7736NtdSBN1g37Q/3myAHyDgHffLYahpqgvx7bQuPrWuhwc7INnl29Nv2pVef8r4v86Zan8fmHEWIuXoG/vvuFTy7qc3tDdnTlXuSoLX2AhzJHAH9qMZ0QJ/pWRLY+dvJfrPnBeiNJWjpijCpIeR+XlYWnQ66LV0RPn3uURzVOMY9+MreQReqCUgkjdt9umFPN3cv2W4/tn3aXG8s43+Ay25dzPk/ftrNaL0Ze080wYd/9yIPrG52Dw6ynz/fxDIn2ad4/eapzfzuua39MvR3njqNaeNquOmpTf1eR1806an3EBrHhGjtiWYML7T3xtxJoQCu+fNKjvvGI26m7vzupo+vpTuSyLvuQ8Ex9K6om8021oeyxtAzA7nzPrjfrc5IxnuVnaFvyjqQae7KXRTX72yarNMCd9s9ZVPGVrvFlF5OQM91YN3lnobprA8wclXvJZ/6Vam/fPIsfD6hwV7eNdvZcybwo3efxGmzDmPt7i6u/qO15OtnzpvDr560dkxXvnYWNVV+PnP+XAC++/YTeOqVVs47Nj1bYHXQj88nHGb/ABuqA9z50dOZMb6G6eNr+1X0OybUVbGvN8b5x07i46+bzQduXcxZR00o6tSs6qDP3UlOG19DbZU/Ywdy5MS6jKyhyu/rN2vW9n29/c77/uUHTqWjL8aRdubqDdbOeffvOGUaFx1/OP9994p+twkFrK5Cp2vz5BnjWLWzg83egN6W7l72TuhTXx1kUn2IzXvTAf/Fbe38e+2eoorhWjoj7OuJZhQ3Ary8q4tEMpUxBu5kXN4u99mNdRlFcEdMqLULjAyt3VkZeixJc2eE0488zH398WTKDTzOjnf6eOsgId8UpPkqoQM+6TebmfMYTkB3ztrwFtI5NQmbW3t5zRFVvOh5DG/FfnNXmIBfMoaBwMrqvNMXOwdwx9m1Eb96chPjaoNEE0nG1qR/VzMOs2pgfvToK3SG4xnX9cYS7qqDABPrQ7R2R+kMezL0nljGmO/Lu6yA2dYbZVJ9tfs9nTG+huXb9xOOp2tIvAodODd3hmnriRKw9wneg9VE1sRAbobelc7QvUE7+zeaPda9uyOSsy3eg1lIZ/xVfh8tXRGa7VqKKWNr3LNsvJzTBV/IMbmO8x45B5+lWMY4H83QVcmNq63KG8zByhbes2AGsybWccmJk6mvDnDthUdz7UXH8KdPnMkDnz2HIyfW8ZWLj3V//NPH1/LBM48gFPDz68tOpT4UcDO7694yj0++fjbnHTuJNxzd6AaL04+0uuknN1TzmfPmuNXB7z1tBuu+cxG3XXkaZ86ewNbvvZkjJtS5gfpT5x7FBcdN6tfub751Hgs/dba7Q5s6tsYde3e6kLMLlmY39u9atjL0zB99fXWA2Y1j3LoF7zjc7o4wkXiSKWOrufiEKe52744RcKdRBagJWsU9TnX+3Elj2O7Zqe3PyliOahyT0d36kd+9yK2LttK0P5wxFHHDO05g6tj0cEt9dYA9XREu/fVz7umMx09tYPbEOl7e3ckja/dkTHCzaW8Ph9VVcerM9MJBsydmDl0cN6WBrkiCpv3hjApvsE7329sdYfLYave8eitIxTPe+2l2T4n3TIkrbl/ivgf5AnqhTNPpcnfGV3NVfW/f18vKnR1cflu6wMx7gNfcGeH+VbtzFhdGMwq1rDn9Z3h6Lzr64sSThmrPgerksdVuL1N2ttoXS1IbSt+2cUwVbT2xjAx9R3ufW1/g/f44xYLeDB3gXTe/0K/S3LtSXbaqgI+m/WFaOiNMGFOFzycZB6Jxe/KknrwZetg9gBoT6n8g4f18JzdU523H9n19GfUBzkHazAm17O4Iu7UsU8dVU1fVPxGIJVKs3NnBdz1DaQ7nQK9NA7o61IkIa751kZuJn3XUBE6YNrbgfd580hRWf+tCJtuB5bC6Kr52yXH9pkH94oVH87q5E/n2pcdz7UXHuF3kdVX+nFnGvVedySPXvI6vXHwsv71iQcZ1X7/kWD5y9pHMm9rApIYQ08bVUFPl55YrXsN1bz6OL198DNB/xjXveCdYO6Vt+3ozdkTO3O1e3r//8MJ2Einj1go4Xc1Bv7DsugtY8vU3Apnjzbs6wu5z11cHOKpxDNv2WdPj7mzvozMc580npYsYj5pUx+a9PazYsZ89nZGM8dIjPePdb58/jee/9kb379ccMZ49XZGMoPXg/3sdF50wmY0t3f2ydoArz7J6X5zVAKdlDVMcP9X6/Nfu7uo3c9//b+/Ow6uqzwSOf997s+/JDQkJ2SGENWHfdxVZRHFFSwe1KLbOqFUfl6kd69Q6rbZjLWptnY6741L3bRAKFClK2fc1IHsCSVhCCGT99Y9z7uXcLJCCkHB5P89zn9zzu0vOfXNP3nN+67KdB6mpM6TEhhFix2FjsZVgC9JOfm+8TR/OqtaFW0u5880V1NbVNzs86u5LcpssB+sfd3VtPVv3Nz8/wo7SY77PnGJ/P50nUq8s2sGCzSVNvrayupZ3l+7mic838MJft1FdV09YsJvkGP/vUKhjnoj2MWHk2k1cztqYOrunv7OvRGKUdYXurZ1xCfzsk/W+mc76Z588ySqy27xPJnQrnhuLyv36WiwqLOX3jk6LDRWkxVJbb1i957Dv++hs2qqtN/zxq+2+cejHqq1q/f2OK3Rvx7hh9uJTzemY1Pjkee1jY3ni6h5U1dbzrePvUFNXT5BLSIsPZ9+R475jJzkmjKt6d+D6vml+02NX1dY3OmHy8p6ce4exnc+ErlXuKiA0tTJYQxEhQbw+faBv+4nJPfh6WxmTeze9Sm9+2skpKZ1XpV/+eAR5jmFtd4/J5QdDswHrH8Btw3OoqzfcOjSL6/um+y1H69zNZ6b0wu0S7nprJSdqqn3j5JsaMues6vZeCXqrU2PDgymtsIbFOU8YnOPpdx88zsDsBAqB+IgQMj0RfLmh2G+N974Z8b4r5UE5Ht5YvItrfn9yCKHX/902kJ9+tI6KKv8EAdAnI56/NpGgeqTGUlNnGo1/B7iiwKpleOmW/jw7r5BJ+Sl+Vz5d2kfjdgkrdh1qNFTq+flW8mgfE4bH/uyz11tTFBekxfli39ywx/X7ypkwc6FfO2eHuHBf7O69NJf9R07wzrLdjV5bW2/4eNVeKqpq6ZcZ36gTYXpCON+WVRJRXoUnMoSvHx5Dt0e/9A2lBFiwxYpVpieCZ6b04mpHvP9WWMqD7/tPAARWnwJnh8s4R7V6u+hQXCKEBLn4dM0+Pli5h+zEKNwuq1rZObQyMSqUsmNVbNhXTlJ0KE9el8+tLy/l7aW7CXG7KEiLY1GhVaW8vfQYD/x5NYcqawhyCcmOWpnVuw+TmxTFtJeW+CW5kZ3b+T6fV88OcSzdcYhtJccY2dlqLnOerBYfOeHX/HCsyloXwVu1Xlx+gt0HK/FEhtArI45Z6xtPR+3VqV2Ub///6+qeXN49meiwYN+JwMItJb5OhNW19QTbo2nW7jlC0ZHjeCJDCAt208c+LpydSKtq6/xGsTgdtWuNvMepJnSlzoMhnRIZcpqzfKefTOhC74x4v2QOVg/xhonN7RJ+Nskap3/nqI58tqaIXQcr/YbUTe7dwa9z26Vdk/l8bRFju/v33Af8eh4DPHtTb0Z3sZoBnr6hF9NeWuL75+T1w5Ed+cOCbYzOa8eU/hl8avc+Dw920zcznqBFwq1Ds3nxq+0AxEeeTAwTeqTQP2sHG4uOUlFVS1xEMO/MGIwnKoTEqFBenOZfY+HtG9Cwp79Xjw5WImmqg5C3eSI6LJifTGg84iEs2E2P1BjffjYlJTacrinRRIcFsXBrKWHBLqYMSGfT/qOEul2+2oymEm/DMfgT81OoqKrlewOs+RiaaibxenLWZtITwpkxIodlry/3eyzLE8n2kgqC3C7y2kcjIvToENNkEsr0RJISe/KkwxMZwj1vrwJgzr0jeHfZbl+1d3p8BEsdJwVju7fn4Q+sRZO8yTE5JpSFW0sJcgmLHXOL9844eZLaLjoUY6yaivsv68zovCS6p8awfl85iVEhfv0aXvxquy8xDchOINrxfX96zhbeWLzTrzmkW0oML9/SH5dLfEs2+37/Iuu+9+TTO10xNF5rYemOQ3yw3JqYKD4imG0HKggLdpOeENHoOGyoo+O7OKSjx3fCl+mJJDsxkl9/uZkB2R66pcZQU2cICXKRGhtG2TFr0pqG798tNYa3Zwzi09X7+HjVPjYVNz8D3IAn5gLWSZ12ilOqDZoxouMZve7BcV14cFwXFmwpoU9GHA+My/O12YUGuXng8jxO1NRxQ790osOCfFf7TlcWpPLNtjJ+Nqk70WFBftNljujcji2/GN+oDf3h8V14aFyer/ZiW0kFn68pIi0+nMu6JbP58fG4XMLx6jpeX7zTry3W5RLeuG0gdfWGHaWVZHoiGp20OP3lvpHsPXycmPCTz7kiP4Vr+1i99J09128blk1YsJvn5hfSIS68ydqVn1/V3a8Z5Pmpfbjl5aV+V4BPXtuTh963Epm3DX1Yp0T+f10xee1jSIoO841j93pt+gB2llUy/ndWzUT/rHhW7jrs11beNzOeyx0nVZ0anKTce2lnQoJcPDlrE6UVVTz/vT4Mz228lPPgjh6emmWtcHfLkCwAxvVI8UvGXlGhbhKjrM6coUEufnlNT2a8vpzsxEhyk6N5ZGI333Od7eh9M+NJiAxhcq9UtpWcrEK+77LOrN9bzu0jcthZVskNf/wG8P87eBOq2yX82xhr9sfhue2sWoueKYzpksSN/dP5fG2R31Xm+B7t/b5/4D973U8ndmVSQaqvQ1/PDrF0T41hYE4CE3qm8MB7qzlRU09itPV5G44mGZCVQLfUGBZvL2NT8VEe+9SqrZk+LJvfzN7Ckm8PcmVBarOTP8VFBHO4sgZP5MmT56wGwyIn5acwc14h17ywiNuH5/DK1zuICHH7muFKK6p56rrGx+GgHA9b9x/lzapaFm4tZWLPFCbmpzBz7lY2FR9tdMI4Oi+J1xfv9OvgeC5pQlfqPPFWMUY36CDonEr3V9fmN/na6LBgnmuQnJwaJnMvZ7K8c1RHhnVKpF10KCLiq/5/ZGJXOreP5pKuyX6v9Y4K8A5jO5UMTwQZngiMMXRKiqLwQAWPTupGkj0XtojwxvSB1NTVMyrPmm1w6qCMZjtLThuc5bedFh/BF3cPZ8GWErITIzhyvJa+mfFkJETy9tJdvlnifjiyI4lRodw8JKvxm2I1u3RpH02WJ4I7RnZkYHYCxeUnSIkN56W/WTPfOa9iwfq73T48mzFdkik7VsUV+amANSZ83+HjTMxPaepX8YOh2byzdDdlFdVMKrBeM75Hex7/bAN3jMihXXQo76/Yy8aiclwiBLldPD65B30z4umWGsOce0c0SpxgdeKMCQ9m6sAMX1PQMzf29nvO1b3TuNouSo4JY1JBKgkRwX7fB+/J1439033ld1/SiRG5iQzu6EFE+NW1+azcdZjNJ45yzyW5zN20n4n5KSRFh/GH7/ehID2Ob0uO8d6KPaTFRzB3435uHJDh12Ht07uG+e3bqM5JzFpf7Otx7+yJD/D9wZlcWZDKuGe+8pX17BDLtCFZ/Gb2FsC68m1qcqnkmFD6ZSXw+Zoiauvrefyq7n5NZ173jc3j+n7p3PXWSp6dZ42kqayu833XJxWkMjqvcUdYgCn9M3jtm53sO3yc6cOz6ZMRz5v2UMYxXZP408396PXzOYB1AmWMtTBNw895Lkhz4wgvBP369TPLljW9HJ5SqnVUVteyds8RBuZ4Tv/kNqSqto79R6rI8ESc/slN2HOokn2HT/Dq1zuY0j+dEZ3bcayqFrdL/BLzgfITvpOqmrp6fv3lZm4dmuVX5X4+1NcbPli5l0kFKc0O6QTYVFxO+fFa3yiRs3W8uo7/nr2ZfxmcSaYnEmMM7y3fw86ySt5ZtpuFD44mLNjNxqJy1u49wocr9jJ1UAZX5KfyHx+tY0fZMR67sjsd20XxxdoiNuwr57n5hbx7x2D6Zcaz5cBRbn9tGR/dOdRXzd6cOruD3oZ95ewvP8H9Y/Ooqq07ZTzAbhc3EGsPaTt0rJrCkgp6doglLNjNL7/YSGpcOON6tGd/+Qm6psQ06uh6NkRkuTGmX6NyTehKKaVaW5290mFTw9FOp6yi6rTJO5A0l9B12JpSSqlW53bJGSVz4KJK5qeiCV0ppZQKAJrQlVJKqQCgCV0ppZQKAJrQlVJKqQCgCV0ppZQKAJrQlVJKqQCgCV0ppZQKAJrQlVJKqQCgCV0ppZQKAJrQlVJKqQBwQc/lLiIlwM7v8C0TgdLv8P0uNhq/M6exO3Mau7Oj8TtzrRW7TGNMozV7L+iE/l0TkWVNTXivWkbjd+Y0dmdOY3d2NH5nrq3FTqvclVJKqQCgCV0ppZQKAJrQ/b3Y2jtwgdP4nTmN3ZnT2J0djd+Za1Ox0zZ0pZRSKgDoFbpSSikVADSh20RknIhsFpFCEXm4tfenrRGRl0TkgIisc5QliMgcEdlq/4y3y0VEZtqxXCMifVpvz1ufiKSLyHwR2SAi60XkHrtc49cCIhImIktEZLUdv/+0y7NF5O92nN4RkRC7PNTeLrQfz2rN/W8LRMQtIitF5DN7W2PXAiKyQ0TWisgqEVlml7XZ41YTOtaXHXgeGA90A24SkW6tu1dtzivAuAZlDwNzjTG5wFx7G6w45tq3GcAL52kf26pa4H5jTDdgEPCv9vdL49cyVcAYY0wB0AsYJyKDgCeB3xpjOgGHgOn286cDh+zy39rPu9jdA2x0bGvsWm60MaaXY3hamz1uNaFbBgCFxpjtxphq4G3gqlbepzbFGPMVcLBB8VXAq/b9V4HJjvLXjGUxECciKednT9seY0yRMWaFff8o1j/WDmj8WsSOQ4W9GWzfDDAGeM8ubxg/b1zfAy4RETlPu9vmiEgaMBH4k70taOzORps9bjWhWzoAux3be+wydWrJxpgi+34xkGzf13g2w67C7A38HY1fi9lVxquAA8AcYBtw2BhTaz/FGSNf/OzHjwCe87vHbcozwINAvb3tQWPXUgaYLSLLRWSGXdZmj9ug8/nLVOAyxhgR0SETpyAiUcD7wI+NMeXOCx+N36kZY+qAXiISB3wIdGnlXbogiMgVwAFjzHIRGdXa+3MBGmaM2SsiScAcEdnkfLCtHbd6hW7ZC6Q7ttPsMnVq+71VSvbPA3a5xrMBEQnGSuZvGmM+sIs1fv8kY8xhYD4wGKtK03tR4oyRL37247FA2Xne1bZiKHCliOzAakocA/wOjV2LGGP22j8PYJ1IDqANH7ea0C1LgVy752cIcCPwSSvv04XgE+Bm+/7NwMeO8ml2r89BwBFHFdVFx26D/F9gozHmacdDGr8WEJF29pU5IhIOXIbVD2E+cJ39tIbx88b1OmCeuUgn3DDG/LsxJs0Yk4X1f22eMWYqGrvTEpFIEYn23gfGAutoy8etMUZv1vd1ArAFq23ukdben7Z2A94CioAarLah6Vhta3OBrcBfgAT7uYI1amAbsBbo19r738qxG4bVFrcGWGXfJmj8Why/fGClHb91wKN2eQ6wBCgE/gyE2uVh9nah/XhOa3+GtnADRgGfaexaHK8cYLV9W+/NC235uNWZ4pRSSqkAoFXuSimlVADQhK6UUkoFAE3oSimlVADQhK6UUkoFAE3oSimlVADQhK7URUxE6uyVpLy372ylQRHJEsfqfEqpc0unflXq4nbcGNOrtXdCKXX29ApdKdWIvQ70U/Za0EtEpJNdniUi8+z1nueKSIZdniwiH4q1ZvlqERliv5VbRP5HrHXMZ9szvSmlzgFN6Epd3MIbVLlPcTx2xBjTE3gOa8UugGeBV40x+cCbwEy7fCawwFhrlvfBmlkLrLWhnzfGdAcOA9ee48+j1EVLZ4pT6iImIhXGmKgmyncAY4wx2+2FZYqNMR4RKQVSjDE1dnmRMSZRREqANGNMleM9soA5xphce/shINgY84tz/8mUuvjoFbpSqjmmmfv/jCrH/Tq0345S54wmdKVUc6Y4fn5j3/8aa9UugKnAQvv+XOBHACLiFpHY87WTSimLni0rdXELF5FVju1Zxhjv0LV4EVmDdZV9k112F/CyiDwAlAC32uX3AC+KyHSsK/EfYa3Op5Q6T7QNXSnViN2G3s8YU9ra+6KUahmtcldKKaUCgF6hK6WUUgFAr9CVUkqpAKAJXSmllAoAmtCVUkqpAKAJXSmllAoAmtCVUkqpAKAJXSmllAoA/wDnkPd3wJaDqwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the loss curve\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(len(epoch_cost)), epoch_cost)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross entropy loss\")\n",
    "plt.title(\"Loss of training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1l2uSvRO9oc"
   },
   "source": [
    "## 7. Adamax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMPGYRg1sVR1"
   },
   "source": [
    "### Model B: 1 Hidden Layer Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YviQJ_vysVR1"
   },
   "source": [
    "### Create model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vEr1912vsVR1",
    "outputId": "2ccfc8a5-269a-459a-d334-1b65c426d017"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "n_iters = 30000\n",
    "num_epochs = n_iters / (len(train_data) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "print(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "id": "O5z3B2u4sVR2"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # create your dataloader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "id": "ABp3RfkesVR2"
   },
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, output_dim):\n",
    "        super(FNN, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear4 = nn.Linear(hidden_dim1, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.linear4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "id": "eSxQB3pLsVR2"
   },
   "outputs": [],
   "source": [
    "input_dim = tensor_x.shape[1]\n",
    "output_dim = 1\n",
    "hidden_dim1 = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "id": "mIQa7J_psVR3"
   },
   "outputs": [],
   "source": [
    "model = FNN(input_dim, hidden_dim1, output_dim)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1KkUeC6esVR3",
    "outputId": "cb1635ae-ae26-45d4-94d6-df2d62bffce3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 256]           2,816\n",
      "           Dropout-2               [-1, 1, 256]               0\n",
      "              ReLU-3               [-1, 1, 256]               0\n",
      "            Linear-4                 [-1, 1, 1]             257\n",
      "================================================================\n",
      "Total params: 3,073\n",
      "Trainable params: 3,073\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1,tensor_x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "id": "uMhegWNusVR3"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 0.0001\n",
    "LAMBDA = 0.05\n",
    "optimizer = torch.optim.Adamax(model.parameters(), weight_decay=LAMBDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2ck64lmsVR3"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8-p4DVI9sVR3",
    "outputId": "4c859846-899f-4f6f-9003-3ccf5229f4b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/513 | Batch 000/059 | Loss: 6790.5347\n",
      "Epoch: 001/513 Train Loss: 2398.5016\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 002/513 | Batch 000/059 | Loss: 2249.8865\n",
      "Epoch: 002/513 Train Loss: 403.0018\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 003/513 | Batch 000/059 | Loss: 420.6418\n",
      "Epoch: 003/513 Train Loss: 367.8562\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 004/513 | Batch 000/059 | Loss: 250.3619\n",
      "Epoch: 004/513 Train Loss: 354.9342\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 005/513 | Batch 000/059 | Loss: 454.8141\n",
      "Epoch: 005/513 Train Loss: 351.6441\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 006/513 | Batch 000/059 | Loss: 327.7190\n",
      "Epoch: 006/513 Train Loss: 346.6802\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 007/513 | Batch 000/059 | Loss: 331.3329\n",
      "Epoch: 007/513 Train Loss: 344.0728\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 008/513 | Batch 000/059 | Loss: 308.8621\n",
      "Epoch: 008/513 Train Loss: 341.4399\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 009/513 | Batch 000/059 | Loss: 270.5545\n",
      "Epoch: 009/513 Train Loss: 339.2635\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 010/513 | Batch 000/059 | Loss: 381.2786\n",
      "Epoch: 010/513 Train Loss: 336.7561\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 011/513 | Batch 000/059 | Loss: 237.7526\n",
      "Epoch: 011/513 Train Loss: 335.0025\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 012/513 | Batch 000/059 | Loss: 182.2211\n",
      "Epoch: 012/513 Train Loss: 332.3721\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 013/513 | Batch 000/059 | Loss: 367.7679\n",
      "Epoch: 013/513 Train Loss: 330.3180\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 014/513 | Batch 000/059 | Loss: 284.2404\n",
      "Epoch: 014/513 Train Loss: 328.9311\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 015/513 | Batch 000/059 | Loss: 393.4334\n",
      "Epoch: 015/513 Train Loss: 326.3618\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 016/513 | Batch 000/059 | Loss: 289.5851\n",
      "Epoch: 016/513 Train Loss: 324.0500\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 017/513 | Batch 000/059 | Loss: 353.8034\n",
      "Epoch: 017/513 Train Loss: 321.9951\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 018/513 | Batch 000/059 | Loss: 336.3943\n",
      "Epoch: 018/513 Train Loss: 321.2491\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 019/513 | Batch 000/059 | Loss: 223.5316\n",
      "Epoch: 019/513 Train Loss: 318.0120\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 020/513 | Batch 000/059 | Loss: 186.5773\n",
      "Epoch: 020/513 Train Loss: 315.7656\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 021/513 | Batch 000/059 | Loss: 215.4518\n",
      "Epoch: 021/513 Train Loss: 313.7022\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 022/513 | Batch 000/059 | Loss: 287.2933\n",
      "Epoch: 022/513 Train Loss: 311.5610\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 023/513 | Batch 000/059 | Loss: 209.9408\n",
      "Epoch: 023/513 Train Loss: 309.4840\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 024/513 | Batch 000/059 | Loss: 365.2399\n",
      "Epoch: 024/513 Train Loss: 307.7471\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 025/513 | Batch 000/059 | Loss: 445.3066\n",
      "Epoch: 025/513 Train Loss: 305.7912\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 026/513 | Batch 000/059 | Loss: 275.2857\n",
      "Epoch: 026/513 Train Loss: 302.7383\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 027/513 | Batch 000/059 | Loss: 168.8948\n",
      "Epoch: 027/513 Train Loss: 301.1409\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 028/513 | Batch 000/059 | Loss: 234.7519\n",
      "Epoch: 028/513 Train Loss: 298.1994\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 029/513 | Batch 000/059 | Loss: 322.3074\n",
      "Epoch: 029/513 Train Loss: 295.8651\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 030/513 | Batch 000/059 | Loss: 216.4794\n",
      "Epoch: 030/513 Train Loss: 294.2538\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 031/513 | Batch 000/059 | Loss: 454.7721\n",
      "Epoch: 031/513 Train Loss: 291.5759\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 032/513 | Batch 000/059 | Loss: 223.0713\n",
      "Epoch: 032/513 Train Loss: 291.7378\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 033/513 | Batch 000/059 | Loss: 344.4630\n",
      "Epoch: 033/513 Train Loss: 286.4286\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 034/513 | Batch 000/059 | Loss: 276.6692\n",
      "Epoch: 034/513 Train Loss: 288.8445\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 035/513 | Batch 000/059 | Loss: 412.1751\n",
      "Epoch: 035/513 Train Loss: 281.8112\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 036/513 | Batch 000/059 | Loss: 469.9130\n",
      "Epoch: 036/513 Train Loss: 279.7474\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 037/513 | Batch 000/059 | Loss: 371.1356\n",
      "Epoch: 037/513 Train Loss: 278.0453\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 038/513 | Batch 000/059 | Loss: 373.0995\n",
      "Epoch: 038/513 Train Loss: 276.7952\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 039/513 | Batch 000/059 | Loss: 239.5891\n",
      "Epoch: 039/513 Train Loss: 273.5587\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 040/513 | Batch 000/059 | Loss: 381.6012\n",
      "Epoch: 040/513 Train Loss: 271.5702\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 041/513 | Batch 000/059 | Loss: 165.5257\n",
      "Epoch: 041/513 Train Loss: 269.2780\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 042/513 | Batch 000/059 | Loss: 165.3907\n",
      "Epoch: 042/513 Train Loss: 267.6430\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 043/513 | Batch 000/059 | Loss: 149.4128\n",
      "Epoch: 043/513 Train Loss: 265.2923\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 044/513 | Batch 000/059 | Loss: 332.9926\n",
      "Epoch: 044/513 Train Loss: 263.7163\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 045/513 | Batch 000/059 | Loss: 223.8287\n",
      "Epoch: 045/513 Train Loss: 262.2356\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 046/513 | Batch 000/059 | Loss: 454.3113\n",
      "Epoch: 046/513 Train Loss: 260.6903\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 047/513 | Batch 000/059 | Loss: 189.1841\n",
      "Epoch: 047/513 Train Loss: 258.9123\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 048/513 | Batch 000/059 | Loss: 309.4634\n",
      "Epoch: 048/513 Train Loss: 259.1753\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 049/513 | Batch 000/059 | Loss: 223.6499\n",
      "Epoch: 049/513 Train Loss: 255.8691\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 050/513 | Batch 000/059 | Loss: 260.2309\n",
      "Epoch: 050/513 Train Loss: 255.4242\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 051/513 | Batch 000/059 | Loss: 239.0240\n",
      "Epoch: 051/513 Train Loss: 252.9100\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 052/513 | Batch 000/059 | Loss: 493.1648\n",
      "Epoch: 052/513 Train Loss: 253.2160\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 053/513 | Batch 000/059 | Loss: 312.3174\n",
      "Epoch: 053/513 Train Loss: 250.4875\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 054/513 | Batch 000/059 | Loss: 275.7802\n",
      "Epoch: 054/513 Train Loss: 249.7262\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 055/513 | Batch 000/059 | Loss: 222.1450\n",
      "Epoch: 055/513 Train Loss: 248.0043\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 056/513 | Batch 000/059 | Loss: 189.4466\n",
      "Epoch: 056/513 Train Loss: 246.9303\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 057/513 | Batch 000/059 | Loss: 302.0269\n",
      "Epoch: 057/513 Train Loss: 246.3091\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 058/513 | Batch 000/059 | Loss: 422.3404\n",
      "Epoch: 058/513 Train Loss: 245.4148\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 059/513 | Batch 000/059 | Loss: 312.9708\n",
      "Epoch: 059/513 Train Loss: 244.3358\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 060/513 | Batch 000/059 | Loss: 179.1328\n",
      "Epoch: 060/513 Train Loss: 242.8848\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 061/513 | Batch 000/059 | Loss: 180.9648\n",
      "Epoch: 061/513 Train Loss: 241.7276\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 062/513 | Batch 000/059 | Loss: 429.1162\n",
      "Epoch: 062/513 Train Loss: 243.6960\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 063/513 | Batch 000/059 | Loss: 295.0844\n",
      "Epoch: 063/513 Train Loss: 240.6207\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 064/513 | Batch 000/059 | Loss: 386.5390\n",
      "Epoch: 064/513 Train Loss: 240.6829\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 065/513 | Batch 000/059 | Loss: 155.3262\n",
      "Epoch: 065/513 Train Loss: 238.8962\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 066/513 | Batch 000/059 | Loss: 260.4260\n",
      "Epoch: 066/513 Train Loss: 239.3391\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 067/513 | Batch 000/059 | Loss: 256.1096\n",
      "Epoch: 067/513 Train Loss: 237.7978\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 068/513 | Batch 000/059 | Loss: 357.8664\n",
      "Epoch: 068/513 Train Loss: 238.8620\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 069/513 | Batch 000/059 | Loss: 242.7281\n",
      "Epoch: 069/513 Train Loss: 236.5341\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 070/513 | Batch 000/059 | Loss: 152.8872\n",
      "Epoch: 070/513 Train Loss: 237.8022\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 071/513 | Batch 000/059 | Loss: 220.1774\n",
      "Epoch: 071/513 Train Loss: 239.6238\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 072/513 | Batch 000/059 | Loss: 214.2246\n",
      "Epoch: 072/513 Train Loss: 235.2489\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 073/513 | Batch 000/059 | Loss: 266.8994\n",
      "Epoch: 073/513 Train Loss: 234.8472\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 074/513 | Batch 000/059 | Loss: 243.3670\n",
      "Epoch: 074/513 Train Loss: 236.1389\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 075/513 | Batch 000/059 | Loss: 156.0867\n",
      "Epoch: 075/513 Train Loss: 235.6198\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 076/513 | Batch 000/059 | Loss: 227.1765\n",
      "Epoch: 076/513 Train Loss: 233.4921\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 077/513 | Batch 000/059 | Loss: 173.6637\n",
      "Epoch: 077/513 Train Loss: 233.2090\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 078/513 | Batch 000/059 | Loss: 251.3628\n",
      "Epoch: 078/513 Train Loss: 233.2933\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 079/513 | Batch 000/059 | Loss: 240.5526\n",
      "Epoch: 079/513 Train Loss: 232.7095\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 080/513 | Batch 000/059 | Loss: 248.8190\n",
      "Epoch: 080/513 Train Loss: 232.3935\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 081/513 | Batch 000/059 | Loss: 266.8185\n",
      "Epoch: 081/513 Train Loss: 238.1395\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 082/513 | Batch 000/059 | Loss: 273.2004\n",
      "Epoch: 082/513 Train Loss: 232.3818\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 083/513 | Batch 000/059 | Loss: 268.6286\n",
      "Epoch: 083/513 Train Loss: 234.0009\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 084/513 | Batch 000/059 | Loss: 408.7252\n",
      "Epoch: 084/513 Train Loss: 233.9302\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 085/513 | Batch 000/059 | Loss: 169.0926\n",
      "Epoch: 085/513 Train Loss: 236.9870\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 086/513 | Batch 000/059 | Loss: 204.3027\n",
      "Epoch: 086/513 Train Loss: 231.3555\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 087/513 | Batch 000/059 | Loss: 195.3063\n",
      "Epoch: 087/513 Train Loss: 231.4564\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 088/513 | Batch 000/059 | Loss: 255.9046\n",
      "Epoch: 088/513 Train Loss: 231.9427\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 089/513 | Batch 000/059 | Loss: 299.1885\n",
      "Epoch: 089/513 Train Loss: 230.5749\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 090/513 | Batch 000/059 | Loss: 207.6732\n",
      "Epoch: 090/513 Train Loss: 230.7664\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 091/513 | Batch 000/059 | Loss: 286.6672\n",
      "Epoch: 091/513 Train Loss: 232.5239\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 092/513 | Batch 000/059 | Loss: 165.3940\n",
      "Epoch: 092/513 Train Loss: 230.4060\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 093/513 | Batch 000/059 | Loss: 133.5841\n",
      "Epoch: 093/513 Train Loss: 230.2360\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 094/513 | Batch 000/059 | Loss: 222.8099\n",
      "Epoch: 094/513 Train Loss: 230.2641\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 095/513 | Batch 000/059 | Loss: 427.0654\n",
      "Epoch: 095/513 Train Loss: 229.9196\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 096/513 | Batch 000/059 | Loss: 240.5933\n",
      "Epoch: 096/513 Train Loss: 230.5364\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 097/513 | Batch 000/059 | Loss: 172.1086\n",
      "Epoch: 097/513 Train Loss: 230.2094\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 098/513 | Batch 000/059 | Loss: 279.8398\n",
      "Epoch: 098/513 Train Loss: 229.3163\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 099/513 | Batch 000/059 | Loss: 231.0246\n",
      "Epoch: 099/513 Train Loss: 229.4452\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 100/513 | Batch 000/059 | Loss: 163.7916\n",
      "Epoch: 100/513 Train Loss: 229.8664\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 101/513 | Batch 000/059 | Loss: 211.1295\n",
      "Epoch: 101/513 Train Loss: 229.1649\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 102/513 | Batch 000/059 | Loss: 275.3095\n",
      "Epoch: 102/513 Train Loss: 229.6341\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 103/513 | Batch 000/059 | Loss: 130.4595\n",
      "Epoch: 103/513 Train Loss: 230.5738\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 104/513 | Batch 000/059 | Loss: 196.0001\n",
      "Epoch: 104/513 Train Loss: 228.7105\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 105/513 | Batch 000/059 | Loss: 395.2581\n",
      "Epoch: 105/513 Train Loss: 228.5785\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 106/513 | Batch 000/059 | Loss: 102.6347\n",
      "Epoch: 106/513 Train Loss: 228.2205\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 107/513 | Batch 000/059 | Loss: 331.2133\n",
      "Epoch: 107/513 Train Loss: 230.7041\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 108/513 | Batch 000/059 | Loss: 308.5302\n",
      "Epoch: 108/513 Train Loss: 227.7545\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 109/513 | Batch 000/059 | Loss: 262.1768\n",
      "Epoch: 109/513 Train Loss: 228.9000\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 110/513 | Batch 000/059 | Loss: 178.7476\n",
      "Epoch: 110/513 Train Loss: 227.3924\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 111/513 | Batch 000/059 | Loss: 389.9054\n",
      "Epoch: 111/513 Train Loss: 228.7400\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 112/513 | Batch 000/059 | Loss: 221.8780\n",
      "Epoch: 112/513 Train Loss: 227.4855\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 113/513 | Batch 000/059 | Loss: 215.4579\n",
      "Epoch: 113/513 Train Loss: 229.6272\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 114/513 | Batch 000/059 | Loss: 257.7785\n",
      "Epoch: 114/513 Train Loss: 227.3683\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 115/513 | Batch 000/059 | Loss: 404.5464\n",
      "Epoch: 115/513 Train Loss: 226.9302\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 116/513 | Batch 000/059 | Loss: 275.4006\n",
      "Epoch: 116/513 Train Loss: 226.6322\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 117/513 | Batch 000/059 | Loss: 245.6577\n",
      "Epoch: 117/513 Train Loss: 226.7421\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 118/513 | Batch 000/059 | Loss: 279.1575\n",
      "Epoch: 118/513 Train Loss: 226.4559\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 119/513 | Batch 000/059 | Loss: 262.1974\n",
      "Epoch: 119/513 Train Loss: 226.6138\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 120/513 | Batch 000/059 | Loss: 250.3307\n",
      "Epoch: 120/513 Train Loss: 226.1160\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 121/513 | Batch 000/059 | Loss: 258.9959\n",
      "Epoch: 121/513 Train Loss: 226.8703\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 122/513 | Batch 000/059 | Loss: 200.3429\n",
      "Epoch: 122/513 Train Loss: 231.7362\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 123/513 | Batch 000/059 | Loss: 165.4191\n",
      "Epoch: 123/513 Train Loss: 226.1850\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 124/513 | Batch 000/059 | Loss: 209.8527\n",
      "Epoch: 124/513 Train Loss: 225.6724\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 125/513 | Batch 000/059 | Loss: 220.1200\n",
      "Epoch: 125/513 Train Loss: 226.6083\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 126/513 | Batch 000/059 | Loss: 181.8081\n",
      "Epoch: 126/513 Train Loss: 225.6607\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 127/513 | Batch 000/059 | Loss: 117.9825\n",
      "Epoch: 127/513 Train Loss: 225.5502\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 128/513 | Batch 000/059 | Loss: 241.3201\n",
      "Epoch: 128/513 Train Loss: 226.4165\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 129/513 | Batch 000/059 | Loss: 407.0563\n",
      "Epoch: 129/513 Train Loss: 225.8110\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 130/513 | Batch 000/059 | Loss: 335.3374\n",
      "Epoch: 130/513 Train Loss: 224.8860\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 131/513 | Batch 000/059 | Loss: 259.9123\n",
      "Epoch: 131/513 Train Loss: 224.8730\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 132/513 | Batch 000/059 | Loss: 283.8950\n",
      "Epoch: 132/513 Train Loss: 225.9111\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 133/513 | Batch 000/059 | Loss: 257.8686\n",
      "Epoch: 133/513 Train Loss: 224.9283\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 134/513 | Batch 000/059 | Loss: 204.6344\n",
      "Epoch: 134/513 Train Loss: 224.3878\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 135/513 | Batch 000/059 | Loss: 330.6693\n",
      "Epoch: 135/513 Train Loss: 224.6707\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 136/513 | Batch 000/059 | Loss: 124.9611\n",
      "Epoch: 136/513 Train Loss: 224.3352\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 137/513 | Batch 000/059 | Loss: 234.9205\n",
      "Epoch: 137/513 Train Loss: 224.4386\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 138/513 | Batch 000/059 | Loss: 249.1017\n",
      "Epoch: 138/513 Train Loss: 224.0951\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 139/513 | Batch 000/059 | Loss: 283.0612\n",
      "Epoch: 139/513 Train Loss: 223.8380\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 140/513 | Batch 000/059 | Loss: 179.3581\n",
      "Epoch: 140/513 Train Loss: 223.9632\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 141/513 | Batch 000/059 | Loss: 271.1848\n",
      "Epoch: 141/513 Train Loss: 223.7723\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 142/513 | Batch 000/059 | Loss: 154.8024\n",
      "Epoch: 142/513 Train Loss: 223.8670\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 143/513 | Batch 000/059 | Loss: 215.4408\n",
      "Epoch: 143/513 Train Loss: 223.5972\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 144/513 | Batch 000/059 | Loss: 337.7865\n",
      "Epoch: 144/513 Train Loss: 225.4652\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 145/513 | Batch 000/059 | Loss: 192.4716\n",
      "Epoch: 145/513 Train Loss: 224.5168\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 146/513 | Batch 000/059 | Loss: 252.6974\n",
      "Epoch: 146/513 Train Loss: 223.3401\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 147/513 | Batch 000/059 | Loss: 302.6204\n",
      "Epoch: 147/513 Train Loss: 223.9714\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 148/513 | Batch 000/059 | Loss: 103.1865\n",
      "Epoch: 148/513 Train Loss: 227.5313\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 149/513 | Batch 000/059 | Loss: 308.9379\n",
      "Epoch: 149/513 Train Loss: 223.8428\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 150/513 | Batch 000/059 | Loss: 260.0513\n",
      "Epoch: 150/513 Train Loss: 223.9293\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 151/513 | Batch 000/059 | Loss: 344.7622\n",
      "Epoch: 151/513 Train Loss: 225.3422\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 152/513 | Batch 000/059 | Loss: 195.4572\n",
      "Epoch: 152/513 Train Loss: 223.0855\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 153/513 | Batch 000/059 | Loss: 218.4798\n",
      "Epoch: 153/513 Train Loss: 222.9199\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 154/513 | Batch 000/059 | Loss: 362.4483\n",
      "Epoch: 154/513 Train Loss: 228.0839\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 155/513 | Batch 000/059 | Loss: 325.2267\n",
      "Epoch: 155/513 Train Loss: 227.2258\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 156/513 | Batch 000/059 | Loss: 200.6258\n",
      "Epoch: 156/513 Train Loss: 224.3011\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 157/513 | Batch 000/059 | Loss: 318.6368\n",
      "Epoch: 157/513 Train Loss: 224.1764\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 158/513 | Batch 000/059 | Loss: 478.8178\n",
      "Epoch: 158/513 Train Loss: 225.4450\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 159/513 | Batch 000/059 | Loss: 177.5528\n",
      "Epoch: 159/513 Train Loss: 223.6217\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 160/513 | Batch 000/059 | Loss: 203.8360\n",
      "Epoch: 160/513 Train Loss: 223.0146\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 161/513 | Batch 000/059 | Loss: 381.7328\n",
      "Epoch: 161/513 Train Loss: 222.7782\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 162/513 | Batch 000/059 | Loss: 170.7319\n",
      "Epoch: 162/513 Train Loss: 225.1816\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 163/513 | Batch 000/059 | Loss: 285.0641\n",
      "Epoch: 163/513 Train Loss: 226.8709\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 164/513 | Batch 000/059 | Loss: 256.8329\n",
      "Epoch: 164/513 Train Loss: 222.1445\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 165/513 | Batch 000/059 | Loss: 283.2400\n",
      "Epoch: 165/513 Train Loss: 222.4686\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 166/513 | Batch 000/059 | Loss: 219.2532\n",
      "Epoch: 166/513 Train Loss: 224.1411\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 167/513 | Batch 000/059 | Loss: 268.5569\n",
      "Epoch: 167/513 Train Loss: 221.8779\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 168/513 | Batch 000/059 | Loss: 180.1886\n",
      "Epoch: 168/513 Train Loss: 221.3271\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 169/513 | Batch 000/059 | Loss: 267.2995\n",
      "Epoch: 169/513 Train Loss: 221.4722\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 170/513 | Batch 000/059 | Loss: 162.1438\n",
      "Epoch: 170/513 Train Loss: 221.4661\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 171/513 | Batch 000/059 | Loss: 274.3231\n",
      "Epoch: 171/513 Train Loss: 221.0763\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 172/513 | Batch 000/059 | Loss: 177.4735\n",
      "Epoch: 172/513 Train Loss: 223.7089\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 173/513 | Batch 000/059 | Loss: 178.0784\n",
      "Epoch: 173/513 Train Loss: 224.6122\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 174/513 | Batch 000/059 | Loss: 180.7636\n",
      "Epoch: 174/513 Train Loss: 222.2687\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 175/513 | Batch 000/059 | Loss: 310.4131\n",
      "Epoch: 175/513 Train Loss: 220.8367\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 176/513 | Batch 000/059 | Loss: 250.0276\n",
      "Epoch: 176/513 Train Loss: 221.2880\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 177/513 | Batch 000/059 | Loss: 242.7289\n",
      "Epoch: 177/513 Train Loss: 221.2465\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 178/513 | Batch 000/059 | Loss: 387.1851\n",
      "Epoch: 178/513 Train Loss: 220.9714\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 179/513 | Batch 000/059 | Loss: 347.0104\n",
      "Epoch: 179/513 Train Loss: 220.9335\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 180/513 | Batch 000/059 | Loss: 251.1214\n",
      "Epoch: 180/513 Train Loss: 220.5503\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 181/513 | Batch 000/059 | Loss: 205.0148\n",
      "Epoch: 181/513 Train Loss: 220.6923\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 182/513 | Batch 000/059 | Loss: 308.0587\n",
      "Epoch: 182/513 Train Loss: 220.2668\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 183/513 | Batch 000/059 | Loss: 109.3098\n",
      "Epoch: 183/513 Train Loss: 220.2843\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 184/513 | Batch 000/059 | Loss: 279.2247\n",
      "Epoch: 184/513 Train Loss: 220.3107\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 185/513 | Batch 000/059 | Loss: 121.9838\n",
      "Epoch: 185/513 Train Loss: 220.2788\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 186/513 | Batch 000/059 | Loss: 177.4553\n",
      "Epoch: 186/513 Train Loss: 224.9869\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 187/513 | Batch 000/059 | Loss: 269.2659\n",
      "Epoch: 187/513 Train Loss: 219.9755\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 188/513 | Batch 000/059 | Loss: 279.5392\n",
      "Epoch: 188/513 Train Loss: 222.1082\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 189/513 | Batch 000/059 | Loss: 282.6525\n",
      "Epoch: 189/513 Train Loss: 220.8434\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 190/513 | Batch 000/059 | Loss: 231.0602\n",
      "Epoch: 190/513 Train Loss: 219.6016\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 191/513 | Batch 000/059 | Loss: 218.7458\n",
      "Epoch: 191/513 Train Loss: 220.8215\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 192/513 | Batch 000/059 | Loss: 200.4654\n",
      "Epoch: 192/513 Train Loss: 219.6833\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 193/513 | Batch 000/059 | Loss: 268.5472\n",
      "Epoch: 193/513 Train Loss: 219.9820\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 194/513 | Batch 000/059 | Loss: 197.6633\n",
      "Epoch: 194/513 Train Loss: 220.5592\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 195/513 | Batch 000/059 | Loss: 256.2608\n",
      "Epoch: 195/513 Train Loss: 219.2829\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 196/513 | Batch 000/059 | Loss: 308.9314\n",
      "Epoch: 196/513 Train Loss: 220.2616\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 197/513 | Batch 000/059 | Loss: 281.1649\n",
      "Epoch: 197/513 Train Loss: 220.0748\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 198/513 | Batch 000/059 | Loss: 280.7966\n",
      "Epoch: 198/513 Train Loss: 219.1881\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 199/513 | Batch 000/059 | Loss: 231.6886\n",
      "Epoch: 199/513 Train Loss: 219.2483\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 200/513 | Batch 000/059 | Loss: 360.0813\n",
      "Epoch: 200/513 Train Loss: 219.0298\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 201/513 | Batch 000/059 | Loss: 298.4609\n",
      "Epoch: 201/513 Train Loss: 219.7970\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 202/513 | Batch 000/059 | Loss: 268.2021\n",
      "Epoch: 202/513 Train Loss: 219.1587\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 203/513 | Batch 000/059 | Loss: 204.2477\n",
      "Epoch: 203/513 Train Loss: 219.5029\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 204/513 | Batch 000/059 | Loss: 201.6963\n",
      "Epoch: 204/513 Train Loss: 219.0136\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 205/513 | Batch 000/059 | Loss: 272.6216\n",
      "Epoch: 205/513 Train Loss: 219.1437\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 206/513 | Batch 000/059 | Loss: 213.2722\n",
      "Epoch: 206/513 Train Loss: 221.2730\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 207/513 | Batch 000/059 | Loss: 257.6002\n",
      "Epoch: 207/513 Train Loss: 220.8582\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 208/513 | Batch 000/059 | Loss: 225.7914\n",
      "Epoch: 208/513 Train Loss: 220.6169\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 209/513 | Batch 000/059 | Loss: 183.0323\n",
      "Epoch: 209/513 Train Loss: 219.5310\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 210/513 | Batch 000/059 | Loss: 153.4774\n",
      "Epoch: 210/513 Train Loss: 218.7411\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 211/513 | Batch 000/059 | Loss: 117.7035\n",
      "Epoch: 211/513 Train Loss: 219.4092\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 212/513 | Batch 000/059 | Loss: 201.9687\n",
      "Epoch: 212/513 Train Loss: 220.0716\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 213/513 | Batch 000/059 | Loss: 497.0074\n",
      "Epoch: 213/513 Train Loss: 221.0015\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 214/513 | Batch 000/059 | Loss: 316.2515\n",
      "Epoch: 214/513 Train Loss: 218.9445\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 215/513 | Batch 000/059 | Loss: 266.3373\n",
      "Epoch: 215/513 Train Loss: 220.8953\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 216/513 | Batch 000/059 | Loss: 297.3779\n",
      "Epoch: 216/513 Train Loss: 218.5561\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 217/513 | Batch 000/059 | Loss: 179.2263\n",
      "Epoch: 217/513 Train Loss: 219.0772\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 218/513 | Batch 000/059 | Loss: 234.2686\n",
      "Epoch: 218/513 Train Loss: 218.4144\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 219/513 | Batch 000/059 | Loss: 375.5205\n",
      "Epoch: 219/513 Train Loss: 220.7108\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 220/513 | Batch 000/059 | Loss: 136.8780\n",
      "Epoch: 220/513 Train Loss: 219.1242\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 221/513 | Batch 000/059 | Loss: 220.4272\n",
      "Epoch: 221/513 Train Loss: 218.0378\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 222/513 | Batch 000/059 | Loss: 187.7916\n",
      "Epoch: 222/513 Train Loss: 224.2091\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 223/513 | Batch 000/059 | Loss: 342.6851\n",
      "Epoch: 223/513 Train Loss: 218.0740\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 224/513 | Batch 000/059 | Loss: 202.7722\n",
      "Epoch: 224/513 Train Loss: 218.1241\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 225/513 | Batch 000/059 | Loss: 323.5130\n",
      "Epoch: 225/513 Train Loss: 218.5886\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 226/513 | Batch 000/059 | Loss: 210.1413\n",
      "Epoch: 226/513 Train Loss: 218.0019\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 227/513 | Batch 000/059 | Loss: 256.2429\n",
      "Epoch: 227/513 Train Loss: 218.2132\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 228/513 | Batch 000/059 | Loss: 161.9461\n",
      "Epoch: 228/513 Train Loss: 217.4803\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 229/513 | Batch 000/059 | Loss: 188.3928\n",
      "Epoch: 229/513 Train Loss: 218.8714\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 230/513 | Batch 000/059 | Loss: 223.5171\n",
      "Epoch: 230/513 Train Loss: 217.5550\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 231/513 | Batch 000/059 | Loss: 174.6902\n",
      "Epoch: 231/513 Train Loss: 226.3993\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 232/513 | Batch 000/059 | Loss: 226.4469\n",
      "Epoch: 232/513 Train Loss: 217.6928\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 233/513 | Batch 000/059 | Loss: 206.6486\n",
      "Epoch: 233/513 Train Loss: 219.0633\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 234/513 | Batch 000/059 | Loss: 192.8388\n",
      "Epoch: 234/513 Train Loss: 223.2876\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 235/513 | Batch 000/059 | Loss: 317.2658\n",
      "Epoch: 235/513 Train Loss: 217.3225\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 236/513 | Batch 000/059 | Loss: 178.2282\n",
      "Epoch: 236/513 Train Loss: 217.6892\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 237/513 | Batch 000/059 | Loss: 304.6453\n",
      "Epoch: 237/513 Train Loss: 217.2033\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 238/513 | Batch 000/059 | Loss: 156.3202\n",
      "Epoch: 238/513 Train Loss: 217.2562\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 239/513 | Batch 000/059 | Loss: 201.6670\n",
      "Epoch: 239/513 Train Loss: 217.0736\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 240/513 | Batch 000/059 | Loss: 297.8560\n",
      "Epoch: 240/513 Train Loss: 216.8855\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 241/513 | Batch 000/059 | Loss: 273.3494\n",
      "Epoch: 241/513 Train Loss: 217.0011\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 242/513 | Batch 000/059 | Loss: 211.8784\n",
      "Epoch: 242/513 Train Loss: 216.7775\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 243/513 | Batch 000/059 | Loss: 204.1933\n",
      "Epoch: 243/513 Train Loss: 217.6236\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 244/513 | Batch 000/059 | Loss: 184.4087\n",
      "Epoch: 244/513 Train Loss: 216.7853\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 245/513 | Batch 000/059 | Loss: 141.5406\n",
      "Epoch: 245/513 Train Loss: 217.6195\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 246/513 | Batch 000/059 | Loss: 379.8404\n",
      "Epoch: 246/513 Train Loss: 216.7764\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 247/513 | Batch 000/059 | Loss: 257.8883\n",
      "Epoch: 247/513 Train Loss: 217.2673\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 248/513 | Batch 000/059 | Loss: 156.0069\n",
      "Epoch: 248/513 Train Loss: 216.5641\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 249/513 | Batch 000/059 | Loss: 178.6773\n",
      "Epoch: 249/513 Train Loss: 227.9845\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 250/513 | Batch 000/059 | Loss: 221.3627\n",
      "Epoch: 250/513 Train Loss: 218.1416\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 251/513 | Batch 000/059 | Loss: 277.9196\n",
      "Epoch: 251/513 Train Loss: 217.4588\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 252/513 | Batch 000/059 | Loss: 274.2994\n",
      "Epoch: 252/513 Train Loss: 217.1147\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 253/513 | Batch 000/059 | Loss: 160.6594\n",
      "Epoch: 253/513 Train Loss: 216.2885\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 254/513 | Batch 000/059 | Loss: 151.8646\n",
      "Epoch: 254/513 Train Loss: 218.4307\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 255/513 | Batch 000/059 | Loss: 249.0770\n",
      "Epoch: 255/513 Train Loss: 219.4927\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 256/513 | Batch 000/059 | Loss: 159.5471\n",
      "Epoch: 256/513 Train Loss: 216.1559\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 257/513 | Batch 000/059 | Loss: 292.7348\n",
      "Epoch: 257/513 Train Loss: 217.6274\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 258/513 | Batch 000/059 | Loss: 267.0844\n",
      "Epoch: 258/513 Train Loss: 216.3619\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 259/513 | Batch 000/059 | Loss: 228.3543\n",
      "Epoch: 259/513 Train Loss: 219.2988\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 260/513 | Batch 000/059 | Loss: 230.4816\n",
      "Epoch: 260/513 Train Loss: 215.9954\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 261/513 | Batch 000/059 | Loss: 307.3046\n",
      "Epoch: 261/513 Train Loss: 217.7410\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 262/513 | Batch 000/059 | Loss: 268.1401\n",
      "Epoch: 262/513 Train Loss: 216.1587\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 263/513 | Batch 000/059 | Loss: 260.9485\n",
      "Epoch: 263/513 Train Loss: 221.3327\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 264/513 | Batch 000/059 | Loss: 309.1772\n",
      "Epoch: 264/513 Train Loss: 215.8480\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 265/513 | Batch 000/059 | Loss: 268.8500\n",
      "Epoch: 265/513 Train Loss: 219.6698\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 266/513 | Batch 000/059 | Loss: 125.8873\n",
      "Epoch: 266/513 Train Loss: 216.5051\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 267/513 | Batch 000/059 | Loss: 277.1983\n",
      "Epoch: 267/513 Train Loss: 215.6606\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 268/513 | Batch 000/059 | Loss: 145.4224\n",
      "Epoch: 268/513 Train Loss: 216.1144\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 269/513 | Batch 000/059 | Loss: 137.4449\n",
      "Epoch: 269/513 Train Loss: 215.6067\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 270/513 | Batch 000/059 | Loss: 328.8493\n",
      "Epoch: 270/513 Train Loss: 215.9119\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 271/513 | Batch 000/059 | Loss: 142.0690\n",
      "Epoch: 271/513 Train Loss: 219.3452\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 272/513 | Batch 000/059 | Loss: 157.9071\n",
      "Epoch: 272/513 Train Loss: 215.4084\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 273/513 | Batch 000/059 | Loss: 233.9183\n",
      "Epoch: 273/513 Train Loss: 216.5322\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 274/513 | Batch 000/059 | Loss: 247.5592\n",
      "Epoch: 274/513 Train Loss: 215.2940\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 275/513 | Batch 000/059 | Loss: 377.0729\n",
      "Epoch: 275/513 Train Loss: 216.9732\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 276/513 | Batch 000/059 | Loss: 124.6154\n",
      "Epoch: 276/513 Train Loss: 216.0065\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 277/513 | Batch 000/059 | Loss: 219.1748\n",
      "Epoch: 277/513 Train Loss: 215.3376\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 278/513 | Batch 000/059 | Loss: 289.1783\n",
      "Epoch: 278/513 Train Loss: 215.2215\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 279/513 | Batch 000/059 | Loss: 303.1063\n",
      "Epoch: 279/513 Train Loss: 216.4338\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 280/513 | Batch 000/059 | Loss: 156.3748\n",
      "Epoch: 280/513 Train Loss: 215.1678\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 281/513 | Batch 000/059 | Loss: 257.1779\n",
      "Epoch: 281/513 Train Loss: 215.0749\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 282/513 | Batch 000/059 | Loss: 272.8768\n",
      "Epoch: 282/513 Train Loss: 215.9896\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 283/513 | Batch 000/059 | Loss: 190.9960\n",
      "Epoch: 283/513 Train Loss: 215.0885\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 284/513 | Batch 000/059 | Loss: 389.0033\n",
      "Epoch: 284/513 Train Loss: 214.9316\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 285/513 | Batch 000/059 | Loss: 195.2762\n",
      "Epoch: 285/513 Train Loss: 217.3827\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 286/513 | Batch 000/059 | Loss: 292.3412\n",
      "Epoch: 286/513 Train Loss: 217.8047\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 287/513 | Batch 000/059 | Loss: 244.8981\n",
      "Epoch: 287/513 Train Loss: 216.3164\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 288/513 | Batch 000/059 | Loss: 134.2305\n",
      "Epoch: 288/513 Train Loss: 215.1140\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 289/513 | Batch 000/059 | Loss: 503.7775\n",
      "Epoch: 289/513 Train Loss: 215.5563\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 290/513 | Batch 000/059 | Loss: 252.9353\n",
      "Epoch: 290/513 Train Loss: 214.8416\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 291/513 | Batch 000/059 | Loss: 224.9068\n",
      "Epoch: 291/513 Train Loss: 215.0810\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 292/513 | Batch 000/059 | Loss: 187.9067\n",
      "Epoch: 292/513 Train Loss: 217.4417\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 293/513 | Batch 000/059 | Loss: 159.6129\n",
      "Epoch: 293/513 Train Loss: 217.4653\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 294/513 | Batch 000/059 | Loss: 184.3551\n",
      "Epoch: 294/513 Train Loss: 218.0537\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 295/513 | Batch 000/059 | Loss: 370.7348\n",
      "Epoch: 295/513 Train Loss: 221.7547\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 296/513 | Batch 000/059 | Loss: 188.7154\n",
      "Epoch: 296/513 Train Loss: 215.4158\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 297/513 | Batch 000/059 | Loss: 294.8100\n",
      "Epoch: 297/513 Train Loss: 215.2761\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 298/513 | Batch 000/059 | Loss: 287.7301\n",
      "Epoch: 298/513 Train Loss: 217.5699\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 299/513 | Batch 000/059 | Loss: 181.6826\n",
      "Epoch: 299/513 Train Loss: 214.5242\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 300/513 | Batch 000/059 | Loss: 156.0401\n",
      "Epoch: 300/513 Train Loss: 214.4401\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 301/513 | Batch 000/059 | Loss: 239.3091\n",
      "Epoch: 301/513 Train Loss: 214.7495\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 302/513 | Batch 000/059 | Loss: 133.0897\n",
      "Epoch: 302/513 Train Loss: 214.8068\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 303/513 | Batch 000/059 | Loss: 350.1096\n",
      "Epoch: 303/513 Train Loss: 214.1576\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 304/513 | Batch 000/059 | Loss: 196.4120\n",
      "Epoch: 304/513 Train Loss: 214.1995\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 305/513 | Batch 000/059 | Loss: 144.2469\n",
      "Epoch: 305/513 Train Loss: 214.4354\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 306/513 | Batch 000/059 | Loss: 309.4788\n",
      "Epoch: 306/513 Train Loss: 214.3427\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 307/513 | Batch 000/059 | Loss: 275.7659\n",
      "Epoch: 307/513 Train Loss: 214.1257\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 308/513 | Batch 000/059 | Loss: 135.4867\n",
      "Epoch: 308/513 Train Loss: 214.2353\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 309/513 | Batch 000/059 | Loss: 160.7107\n",
      "Epoch: 309/513 Train Loss: 215.0334\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 310/513 | Batch 000/059 | Loss: 210.9953\n",
      "Epoch: 310/513 Train Loss: 214.0740\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 311/513 | Batch 000/059 | Loss: 253.2923\n",
      "Epoch: 311/513 Train Loss: 214.4747\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 312/513 | Batch 000/059 | Loss: 249.3741\n",
      "Epoch: 312/513 Train Loss: 214.0758\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 313/513 | Batch 000/059 | Loss: 235.9308\n",
      "Epoch: 313/513 Train Loss: 213.9040\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 314/513 | Batch 000/059 | Loss: 181.1659\n",
      "Epoch: 314/513 Train Loss: 214.6341\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 315/513 | Batch 000/059 | Loss: 294.7282\n",
      "Epoch: 315/513 Train Loss: 215.1708\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 316/513 | Batch 000/059 | Loss: 296.7418\n",
      "Epoch: 316/513 Train Loss: 214.0067\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 317/513 | Batch 000/059 | Loss: 206.6568\n",
      "Epoch: 317/513 Train Loss: 213.7095\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 318/513 | Batch 000/059 | Loss: 162.5851\n",
      "Epoch: 318/513 Train Loss: 214.0520\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 319/513 | Batch 000/059 | Loss: 204.2284\n",
      "Epoch: 319/513 Train Loss: 213.6971\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 320/513 | Batch 000/059 | Loss: 518.2775\n",
      "Epoch: 320/513 Train Loss: 215.7285\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 321/513 | Batch 000/059 | Loss: 158.8460\n",
      "Epoch: 321/513 Train Loss: 213.8283\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 322/513 | Batch 000/059 | Loss: 224.8469\n",
      "Epoch: 322/513 Train Loss: 213.9092\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 323/513 | Batch 000/059 | Loss: 259.7309\n",
      "Epoch: 323/513 Train Loss: 215.4284\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 324/513 | Batch 000/059 | Loss: 188.9223\n",
      "Epoch: 324/513 Train Loss: 213.6496\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 325/513 | Batch 000/059 | Loss: 337.3942\n",
      "Epoch: 325/513 Train Loss: 216.3358\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 326/513 | Batch 000/059 | Loss: 214.4072\n",
      "Epoch: 326/513 Train Loss: 213.9533\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 327/513 | Batch 000/059 | Loss: 298.0450\n",
      "Epoch: 327/513 Train Loss: 214.1036\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 328/513 | Batch 000/059 | Loss: 141.9901\n",
      "Epoch: 328/513 Train Loss: 214.4393\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 329/513 | Batch 000/059 | Loss: 345.3453\n",
      "Epoch: 329/513 Train Loss: 213.3917\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 330/513 | Batch 000/059 | Loss: 144.9607\n",
      "Epoch: 330/513 Train Loss: 213.7535\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 331/513 | Batch 000/059 | Loss: 263.0519\n",
      "Epoch: 331/513 Train Loss: 215.0155\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 332/513 | Batch 000/059 | Loss: 170.5555\n",
      "Epoch: 332/513 Train Loss: 213.5563\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 333/513 | Batch 000/059 | Loss: 236.0328\n",
      "Epoch: 333/513 Train Loss: 213.5114\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 334/513 | Batch 000/059 | Loss: 183.6922\n",
      "Epoch: 334/513 Train Loss: 215.7475\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 335/513 | Batch 000/059 | Loss: 284.0763\n",
      "Epoch: 335/513 Train Loss: 215.3185\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 336/513 | Batch 000/059 | Loss: 274.0901\n",
      "Epoch: 336/513 Train Loss: 213.4609\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 337/513 | Batch 000/059 | Loss: 168.4597\n",
      "Epoch: 337/513 Train Loss: 213.3879\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 338/513 | Batch 000/059 | Loss: 167.1486\n",
      "Epoch: 338/513 Train Loss: 214.3505\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 339/513 | Batch 000/059 | Loss: 118.7392\n",
      "Epoch: 339/513 Train Loss: 213.5901\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 340/513 | Batch 000/059 | Loss: 160.9868\n",
      "Epoch: 340/513 Train Loss: 213.2732\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 341/513 | Batch 000/059 | Loss: 195.5747\n",
      "Epoch: 341/513 Train Loss: 213.2241\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 342/513 | Batch 000/059 | Loss: 175.6754\n",
      "Epoch: 342/513 Train Loss: 213.0912\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 343/513 | Batch 000/059 | Loss: 288.2958\n",
      "Epoch: 343/513 Train Loss: 213.3790\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 344/513 | Batch 000/059 | Loss: 293.9704\n",
      "Epoch: 344/513 Train Loss: 213.8079\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 345/513 | Batch 000/059 | Loss: 302.8863\n",
      "Epoch: 345/513 Train Loss: 213.1026\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 346/513 | Batch 000/059 | Loss: 145.6365\n",
      "Epoch: 346/513 Train Loss: 213.0595\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 347/513 | Batch 000/059 | Loss: 279.7779\n",
      "Epoch: 347/513 Train Loss: 214.4617\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 348/513 | Batch 000/059 | Loss: 163.4376\n",
      "Epoch: 348/513 Train Loss: 213.4503\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 349/513 | Batch 000/059 | Loss: 183.1451\n",
      "Epoch: 349/513 Train Loss: 212.9129\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 350/513 | Batch 000/059 | Loss: 270.3777\n",
      "Epoch: 350/513 Train Loss: 213.5607\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 351/513 | Batch 000/059 | Loss: 182.6275\n",
      "Epoch: 351/513 Train Loss: 213.1544\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 352/513 | Batch 000/059 | Loss: 144.5294\n",
      "Epoch: 352/513 Train Loss: 213.5085\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 353/513 | Batch 000/059 | Loss: 260.7592\n",
      "Epoch: 353/513 Train Loss: 212.8709\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 354/513 | Batch 000/059 | Loss: 205.8252\n",
      "Epoch: 354/513 Train Loss: 212.7857\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 355/513 | Batch 000/059 | Loss: 174.0938\n",
      "Epoch: 355/513 Train Loss: 212.7298\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 356/513 | Batch 000/059 | Loss: 240.6797\n",
      "Epoch: 356/513 Train Loss: 212.8447\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 357/513 | Batch 000/059 | Loss: 109.2569\n",
      "Epoch: 357/513 Train Loss: 216.8307\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 358/513 | Batch 000/059 | Loss: 248.0132\n",
      "Epoch: 358/513 Train Loss: 212.7255\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 359/513 | Batch 000/059 | Loss: 302.5320\n",
      "Epoch: 359/513 Train Loss: 212.6965\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 360/513 | Batch 000/059 | Loss: 183.6238\n",
      "Epoch: 360/513 Train Loss: 215.8081\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 361/513 | Batch 000/059 | Loss: 204.2574\n",
      "Epoch: 361/513 Train Loss: 214.1687\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 362/513 | Batch 000/059 | Loss: 148.2311\n",
      "Epoch: 362/513 Train Loss: 213.5274\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 363/513 | Batch 000/059 | Loss: 167.6469\n",
      "Epoch: 363/513 Train Loss: 212.9791\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 364/513 | Batch 000/059 | Loss: 301.0847\n",
      "Epoch: 364/513 Train Loss: 212.6172\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 365/513 | Batch 000/059 | Loss: 248.3607\n",
      "Epoch: 365/513 Train Loss: 213.0908\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 366/513 | Batch 000/059 | Loss: 198.6051\n",
      "Epoch: 366/513 Train Loss: 213.4355\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 367/513 | Batch 000/059 | Loss: 184.0174\n",
      "Epoch: 367/513 Train Loss: 213.1543\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 368/513 | Batch 000/059 | Loss: 280.4138\n",
      "Epoch: 368/513 Train Loss: 216.3323\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 369/513 | Batch 000/059 | Loss: 219.9883\n",
      "Epoch: 369/513 Train Loss: 212.9152\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 370/513 | Batch 000/059 | Loss: 216.5070\n",
      "Epoch: 370/513 Train Loss: 212.7824\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 371/513 | Batch 000/059 | Loss: 179.4341\n",
      "Epoch: 371/513 Train Loss: 212.5682\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 372/513 | Batch 000/059 | Loss: 266.7453\n",
      "Epoch: 372/513 Train Loss: 215.0282\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 373/513 | Batch 000/059 | Loss: 137.7043\n",
      "Epoch: 373/513 Train Loss: 212.4284\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 374/513 | Batch 000/059 | Loss: 376.9378\n",
      "Epoch: 374/513 Train Loss: 213.4697\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 375/513 | Batch 000/059 | Loss: 271.9083\n",
      "Epoch: 375/513 Train Loss: 212.7892\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 376/513 | Batch 000/059 | Loss: 174.7423\n",
      "Epoch: 376/513 Train Loss: 212.4780\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 377/513 | Batch 000/059 | Loss: 291.1227\n",
      "Epoch: 377/513 Train Loss: 213.4761\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 378/513 | Batch 000/059 | Loss: 93.9484\n",
      "Epoch: 378/513 Train Loss: 212.5258\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 379/513 | Batch 000/059 | Loss: 196.1083\n",
      "Epoch: 379/513 Train Loss: 212.3424\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 380/513 | Batch 000/059 | Loss: 236.5035\n",
      "Epoch: 380/513 Train Loss: 213.3446\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 381/513 | Batch 000/059 | Loss: 257.4644\n",
      "Epoch: 381/513 Train Loss: 212.2221\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 382/513 | Batch 000/059 | Loss: 152.5842\n",
      "Epoch: 382/513 Train Loss: 212.4613\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 383/513 | Batch 000/059 | Loss: 283.8144\n",
      "Epoch: 383/513 Train Loss: 212.4444\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 384/513 | Batch 000/059 | Loss: 172.1564\n",
      "Epoch: 384/513 Train Loss: 212.8811\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 385/513 | Batch 000/059 | Loss: 187.7467\n",
      "Epoch: 385/513 Train Loss: 212.4990\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 386/513 | Batch 000/059 | Loss: 148.0733\n",
      "Epoch: 386/513 Train Loss: 217.8311\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 387/513 | Batch 000/059 | Loss: 298.3590\n",
      "Epoch: 387/513 Train Loss: 217.4952\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 388/513 | Batch 000/059 | Loss: 120.2983\n",
      "Epoch: 388/513 Train Loss: 212.1618\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 389/513 | Batch 000/059 | Loss: 199.1563\n",
      "Epoch: 389/513 Train Loss: 212.1818\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 390/513 | Batch 000/059 | Loss: 295.6852\n",
      "Epoch: 390/513 Train Loss: 212.6421\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 391/513 | Batch 000/059 | Loss: 243.7763\n",
      "Epoch: 391/513 Train Loss: 212.5166\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 392/513 | Batch 000/059 | Loss: 263.7015\n",
      "Epoch: 392/513 Train Loss: 212.1967\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 393/513 | Batch 000/059 | Loss: 243.5484\n",
      "Epoch: 393/513 Train Loss: 212.2485\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 394/513 | Batch 000/059 | Loss: 146.1790\n",
      "Epoch: 394/513 Train Loss: 213.5446\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 395/513 | Batch 000/059 | Loss: 257.9229\n",
      "Epoch: 395/513 Train Loss: 211.9031\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 396/513 | Batch 000/059 | Loss: 216.4046\n",
      "Epoch: 396/513 Train Loss: 214.2402\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 397/513 | Batch 000/059 | Loss: 189.4893\n",
      "Epoch: 397/513 Train Loss: 212.5939\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 398/513 | Batch 000/059 | Loss: 218.2334\n",
      "Epoch: 398/513 Train Loss: 212.0914\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 399/513 | Batch 000/059 | Loss: 243.9148\n",
      "Epoch: 399/513 Train Loss: 211.8420\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 400/513 | Batch 000/059 | Loss: 226.3755\n",
      "Epoch: 400/513 Train Loss: 212.2173\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 401/513 | Batch 000/059 | Loss: 245.6449\n",
      "Epoch: 401/513 Train Loss: 211.8469\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 402/513 | Batch 000/059 | Loss: 187.8322\n",
      "Epoch: 402/513 Train Loss: 211.8882\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 403/513 | Batch 000/059 | Loss: 195.6284\n",
      "Epoch: 403/513 Train Loss: 213.1591\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 404/513 | Batch 000/059 | Loss: 194.6214\n",
      "Epoch: 404/513 Train Loss: 211.7382\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 405/513 | Batch 000/059 | Loss: 214.8674\n",
      "Epoch: 405/513 Train Loss: 212.4392\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 406/513 | Batch 000/059 | Loss: 155.0437\n",
      "Epoch: 406/513 Train Loss: 211.9429\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 407/513 | Batch 000/059 | Loss: 188.3142\n",
      "Epoch: 407/513 Train Loss: 212.5165\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 408/513 | Batch 000/059 | Loss: 233.7025\n",
      "Epoch: 408/513 Train Loss: 213.1021\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 409/513 | Batch 000/059 | Loss: 170.0224\n",
      "Epoch: 409/513 Train Loss: 211.8922\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 410/513 | Batch 000/059 | Loss: 353.0053\n",
      "Epoch: 410/513 Train Loss: 211.8326\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 411/513 | Batch 000/059 | Loss: 291.4810\n",
      "Epoch: 411/513 Train Loss: 212.7715\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 412/513 | Batch 000/059 | Loss: 171.8961\n",
      "Epoch: 412/513 Train Loss: 211.7192\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 413/513 | Batch 000/059 | Loss: 232.1679\n",
      "Epoch: 413/513 Train Loss: 211.9916\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 414/513 | Batch 000/059 | Loss: 220.0540\n",
      "Epoch: 414/513 Train Loss: 213.1346\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 415/513 | Batch 000/059 | Loss: 149.5318\n",
      "Epoch: 415/513 Train Loss: 211.6951\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 416/513 | Batch 000/059 | Loss: 102.0078\n",
      "Epoch: 416/513 Train Loss: 211.9051\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 417/513 | Batch 000/059 | Loss: 342.7604\n",
      "Epoch: 417/513 Train Loss: 212.9530\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 418/513 | Batch 000/059 | Loss: 384.7379\n",
      "Epoch: 418/513 Train Loss: 212.6551\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 419/513 | Batch 000/059 | Loss: 198.2672\n",
      "Epoch: 419/513 Train Loss: 211.8297\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 420/513 | Batch 000/059 | Loss: 184.4679\n",
      "Epoch: 420/513 Train Loss: 214.7168\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 421/513 | Batch 000/059 | Loss: 332.3210\n",
      "Epoch: 421/513 Train Loss: 211.6068\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 422/513 | Batch 000/059 | Loss: 313.3398\n",
      "Epoch: 422/513 Train Loss: 211.9858\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 423/513 | Batch 000/059 | Loss: 217.1777\n",
      "Epoch: 423/513 Train Loss: 213.0837\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 424/513 | Batch 000/059 | Loss: 283.3365\n",
      "Epoch: 424/513 Train Loss: 211.4440\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 425/513 | Batch 000/059 | Loss: 172.8745\n",
      "Epoch: 425/513 Train Loss: 213.6775\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 426/513 | Batch 000/059 | Loss: 232.2825\n",
      "Epoch: 426/513 Train Loss: 211.7843\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 427/513 | Batch 000/059 | Loss: 157.2989\n",
      "Epoch: 427/513 Train Loss: 212.3485\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 428/513 | Batch 000/059 | Loss: 164.2356\n",
      "Epoch: 428/513 Train Loss: 214.8022\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 429/513 | Batch 000/059 | Loss: 283.0004\n",
      "Epoch: 429/513 Train Loss: 211.5624\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 430/513 | Batch 000/059 | Loss: 137.9927\n",
      "Epoch: 430/513 Train Loss: 212.2562\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 431/513 | Batch 000/059 | Loss: 165.0781\n",
      "Epoch: 431/513 Train Loss: 212.1814\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 432/513 | Batch 000/059 | Loss: 269.0670\n",
      "Epoch: 432/513 Train Loss: 212.8183\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 433/513 | Batch 000/059 | Loss: 259.2810\n",
      "Epoch: 433/513 Train Loss: 211.2647\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 434/513 | Batch 000/059 | Loss: 188.0719\n",
      "Epoch: 434/513 Train Loss: 211.6095\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 435/513 | Batch 000/059 | Loss: 214.6255\n",
      "Epoch: 435/513 Train Loss: 211.3243\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 436/513 | Batch 000/059 | Loss: 267.1002\n",
      "Epoch: 436/513 Train Loss: 211.2409\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 437/513 | Batch 000/059 | Loss: 287.3673\n",
      "Epoch: 437/513 Train Loss: 211.3253\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 438/513 | Batch 000/059 | Loss: 192.1918\n",
      "Epoch: 438/513 Train Loss: 211.6413\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 439/513 | Batch 000/059 | Loss: 256.8538\n",
      "Epoch: 439/513 Train Loss: 211.5316\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 440/513 | Batch 000/059 | Loss: 193.9990\n",
      "Epoch: 440/513 Train Loss: 211.5597\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 441/513 | Batch 000/059 | Loss: 306.8311\n",
      "Epoch: 441/513 Train Loss: 211.6496\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 442/513 | Batch 000/059 | Loss: 152.9971\n",
      "Epoch: 442/513 Train Loss: 211.6644\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 443/513 | Batch 000/059 | Loss: 172.6452\n",
      "Epoch: 443/513 Train Loss: 216.8843\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 444/513 | Batch 000/059 | Loss: 200.7195\n",
      "Epoch: 444/513 Train Loss: 211.5846\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 445/513 | Batch 000/059 | Loss: 277.4001\n",
      "Epoch: 445/513 Train Loss: 212.1754\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 446/513 | Batch 000/059 | Loss: 224.0691\n",
      "Epoch: 446/513 Train Loss: 212.8459\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 447/513 | Batch 000/059 | Loss: 211.4615\n",
      "Epoch: 447/513 Train Loss: 211.1476\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 448/513 | Batch 000/059 | Loss: 190.8716\n",
      "Epoch: 448/513 Train Loss: 211.2185\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 449/513 | Batch 000/059 | Loss: 133.2101\n",
      "Epoch: 449/513 Train Loss: 211.5844\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 450/513 | Batch 000/059 | Loss: 311.3155\n",
      "Epoch: 450/513 Train Loss: 211.3175\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 451/513 | Batch 000/059 | Loss: 250.1768\n",
      "Epoch: 451/513 Train Loss: 211.4538\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 452/513 | Batch 000/059 | Loss: 159.7139\n",
      "Epoch: 452/513 Train Loss: 211.5453\n",
      "Time elapsed: 1.19 min\n",
      "Epoch: 453/513 | Batch 000/059 | Loss: 171.6113\n",
      "Epoch: 453/513 Train Loss: 211.1792\n",
      "Time elapsed: 1.19 min\n",
      "Epoch: 454/513 | Batch 000/059 | Loss: 277.4387\n",
      "Epoch: 454/513 Train Loss: 211.0609\n",
      "Time elapsed: 1.19 min\n",
      "Epoch: 455/513 | Batch 000/059 | Loss: 180.6191\n",
      "Epoch: 455/513 Train Loss: 211.0672\n",
      "Time elapsed: 1.19 min\n",
      "Epoch: 456/513 | Batch 000/059 | Loss: 279.0722\n",
      "Epoch: 456/513 Train Loss: 212.5273\n",
      "Time elapsed: 1.20 min\n",
      "Epoch: 457/513 | Batch 000/059 | Loss: 221.8424\n",
      "Epoch: 457/513 Train Loss: 212.4589\n",
      "Time elapsed: 1.20 min\n",
      "Epoch: 458/513 | Batch 000/059 | Loss: 166.8973\n",
      "Epoch: 458/513 Train Loss: 211.1856\n",
      "Time elapsed: 1.20 min\n",
      "Epoch: 459/513 | Batch 000/059 | Loss: 301.5081\n",
      "Epoch: 459/513 Train Loss: 211.7531\n",
      "Time elapsed: 1.20 min\n",
      "Epoch: 460/513 | Batch 000/059 | Loss: 211.4447\n",
      "Epoch: 460/513 Train Loss: 211.3838\n",
      "Time elapsed: 1.21 min\n",
      "Epoch: 461/513 | Batch 000/059 | Loss: 262.9214\n",
      "Epoch: 461/513 Train Loss: 211.3118\n",
      "Time elapsed: 1.21 min\n",
      "Epoch: 462/513 | Batch 000/059 | Loss: 213.4056\n",
      "Epoch: 462/513 Train Loss: 212.7125\n",
      "Time elapsed: 1.21 min\n",
      "Epoch: 463/513 | Batch 000/059 | Loss: 282.5606\n",
      "Epoch: 463/513 Train Loss: 212.2094\n",
      "Time elapsed: 1.22 min\n",
      "Epoch: 464/513 | Batch 000/059 | Loss: 292.1059\n",
      "Epoch: 464/513 Train Loss: 211.3715\n",
      "Time elapsed: 1.22 min\n",
      "Epoch: 465/513 | Batch 000/059 | Loss: 212.5554\n",
      "Epoch: 465/513 Train Loss: 211.0498\n",
      "Time elapsed: 1.22 min\n",
      "Epoch: 466/513 | Batch 000/059 | Loss: 228.9216\n",
      "Epoch: 466/513 Train Loss: 212.0545\n",
      "Time elapsed: 1.22 min\n",
      "Epoch: 467/513 | Batch 000/059 | Loss: 111.1992\n",
      "Epoch: 467/513 Train Loss: 211.1790\n",
      "Time elapsed: 1.23 min\n",
      "Epoch: 468/513 | Batch 000/059 | Loss: 163.1583\n",
      "Epoch: 468/513 Train Loss: 210.9740\n",
      "Time elapsed: 1.23 min\n",
      "Epoch: 469/513 | Batch 000/059 | Loss: 223.4426\n",
      "Epoch: 469/513 Train Loss: 210.8299\n",
      "Time elapsed: 1.23 min\n",
      "Epoch: 470/513 | Batch 000/059 | Loss: 412.6033\n",
      "Epoch: 470/513 Train Loss: 211.0868\n",
      "Time elapsed: 1.23 min\n",
      "Epoch: 471/513 | Batch 000/059 | Loss: 255.5823\n",
      "Epoch: 471/513 Train Loss: 211.6260\n",
      "Time elapsed: 1.24 min\n",
      "Epoch: 472/513 | Batch 000/059 | Loss: 218.7193\n",
      "Epoch: 472/513 Train Loss: 211.1975\n",
      "Time elapsed: 1.24 min\n",
      "Epoch: 473/513 | Batch 000/059 | Loss: 236.8972\n",
      "Epoch: 473/513 Train Loss: 211.1908\n",
      "Time elapsed: 1.24 min\n",
      "Epoch: 474/513 | Batch 000/059 | Loss: 232.1443\n",
      "Epoch: 474/513 Train Loss: 211.1439\n",
      "Time elapsed: 1.24 min\n",
      "Epoch: 475/513 | Batch 000/059 | Loss: 191.9855\n",
      "Epoch: 475/513 Train Loss: 210.9370\n",
      "Time elapsed: 1.25 min\n",
      "Epoch: 476/513 | Batch 000/059 | Loss: 200.4959\n",
      "Epoch: 476/513 Train Loss: 218.2069\n",
      "Time elapsed: 1.25 min\n",
      "Epoch: 477/513 | Batch 000/059 | Loss: 180.9284\n",
      "Epoch: 477/513 Train Loss: 211.1075\n",
      "Time elapsed: 1.25 min\n",
      "Epoch: 478/513 | Batch 000/059 | Loss: 216.1162\n",
      "Epoch: 478/513 Train Loss: 212.2974\n",
      "Time elapsed: 1.25 min\n",
      "Epoch: 479/513 | Batch 000/059 | Loss: 123.3427\n",
      "Epoch: 479/513 Train Loss: 213.5780\n",
      "Time elapsed: 1.26 min\n",
      "Epoch: 480/513 | Batch 000/059 | Loss: 171.0513\n",
      "Epoch: 480/513 Train Loss: 213.0124\n",
      "Time elapsed: 1.26 min\n",
      "Epoch: 481/513 | Batch 000/059 | Loss: 212.4396\n",
      "Epoch: 481/513 Train Loss: 210.7844\n",
      "Time elapsed: 1.26 min\n",
      "Epoch: 482/513 | Batch 000/059 | Loss: 286.0244\n",
      "Epoch: 482/513 Train Loss: 210.8358\n",
      "Time elapsed: 1.26 min\n",
      "Epoch: 483/513 | Batch 000/059 | Loss: 202.5704\n",
      "Epoch: 483/513 Train Loss: 212.0352\n",
      "Time elapsed: 1.27 min\n",
      "Epoch: 484/513 | Batch 000/059 | Loss: 237.3793\n",
      "Epoch: 484/513 Train Loss: 210.5990\n",
      "Time elapsed: 1.27 min\n",
      "Epoch: 485/513 | Batch 000/059 | Loss: 300.2855\n",
      "Epoch: 485/513 Train Loss: 210.9513\n",
      "Time elapsed: 1.27 min\n",
      "Epoch: 486/513 | Batch 000/059 | Loss: 136.4809\n",
      "Epoch: 486/513 Train Loss: 210.6270\n",
      "Time elapsed: 1.27 min\n",
      "Epoch: 487/513 | Batch 000/059 | Loss: 234.2287\n",
      "Epoch: 487/513 Train Loss: 211.0565\n",
      "Time elapsed: 1.28 min\n",
      "Epoch: 488/513 | Batch 000/059 | Loss: 215.1193\n",
      "Epoch: 488/513 Train Loss: 210.7033\n",
      "Time elapsed: 1.28 min\n",
      "Epoch: 489/513 | Batch 000/059 | Loss: 225.9148\n",
      "Epoch: 489/513 Train Loss: 210.6638\n",
      "Time elapsed: 1.28 min\n",
      "Epoch: 490/513 | Batch 000/059 | Loss: 96.8515\n",
      "Epoch: 490/513 Train Loss: 211.3305\n",
      "Time elapsed: 1.29 min\n",
      "Epoch: 491/513 | Batch 000/059 | Loss: 120.0625\n",
      "Epoch: 491/513 Train Loss: 212.4674\n",
      "Time elapsed: 1.29 min\n",
      "Epoch: 492/513 | Batch 000/059 | Loss: 379.1055\n",
      "Epoch: 492/513 Train Loss: 210.5791\n",
      "Time elapsed: 1.29 min\n",
      "Epoch: 493/513 | Batch 000/059 | Loss: 325.4587\n",
      "Epoch: 493/513 Train Loss: 210.6604\n",
      "Time elapsed: 1.29 min\n",
      "Epoch: 494/513 | Batch 000/059 | Loss: 313.8267\n",
      "Epoch: 494/513 Train Loss: 210.6341\n",
      "Time elapsed: 1.30 min\n",
      "Epoch: 495/513 | Batch 000/059 | Loss: 203.9049\n",
      "Epoch: 495/513 Train Loss: 213.7879\n",
      "Time elapsed: 1.30 min\n",
      "Epoch: 496/513 | Batch 000/059 | Loss: 230.5733\n",
      "Epoch: 496/513 Train Loss: 210.8588\n",
      "Time elapsed: 1.30 min\n",
      "Epoch: 497/513 | Batch 000/059 | Loss: 238.4487\n",
      "Epoch: 497/513 Train Loss: 210.4715\n",
      "Time elapsed: 1.30 min\n",
      "Epoch: 498/513 | Batch 000/059 | Loss: 204.2870\n",
      "Epoch: 498/513 Train Loss: 211.0996\n",
      "Time elapsed: 1.31 min\n",
      "Epoch: 499/513 | Batch 000/059 | Loss: 281.8438\n",
      "Epoch: 499/513 Train Loss: 211.4255\n",
      "Time elapsed: 1.31 min\n",
      "Epoch: 500/513 | Batch 000/059 | Loss: 167.5729\n",
      "Epoch: 500/513 Train Loss: 212.4715\n",
      "Time elapsed: 1.31 min\n",
      "Epoch: 501/513 | Batch 000/059 | Loss: 158.9115\n",
      "Epoch: 501/513 Train Loss: 211.3311\n",
      "Time elapsed: 1.31 min\n",
      "Epoch: 502/513 | Batch 000/059 | Loss: 302.5247\n",
      "Epoch: 502/513 Train Loss: 210.6077\n",
      "Time elapsed: 1.32 min\n",
      "Epoch: 503/513 | Batch 000/059 | Loss: 181.6511\n",
      "Epoch: 503/513 Train Loss: 213.5719\n",
      "Time elapsed: 1.32 min\n",
      "Epoch: 504/513 | Batch 000/059 | Loss: 204.0672\n",
      "Epoch: 504/513 Train Loss: 210.9447\n",
      "Time elapsed: 1.32 min\n",
      "Epoch: 505/513 | Batch 000/059 | Loss: 203.7706\n",
      "Epoch: 505/513 Train Loss: 212.2823\n",
      "Time elapsed: 1.32 min\n",
      "Epoch: 506/513 | Batch 000/059 | Loss: 422.4506\n",
      "Epoch: 506/513 Train Loss: 210.4368\n",
      "Time elapsed: 1.33 min\n",
      "Epoch: 507/513 | Batch 000/059 | Loss: 198.0331\n",
      "Epoch: 507/513 Train Loss: 211.5949\n",
      "Time elapsed: 1.33 min\n",
      "Epoch: 508/513 | Batch 000/059 | Loss: 329.9519\n",
      "Epoch: 508/513 Train Loss: 214.9451\n",
      "Time elapsed: 1.33 min\n",
      "Epoch: 509/513 | Batch 000/059 | Loss: 263.1879\n",
      "Epoch: 509/513 Train Loss: 210.7239\n",
      "Time elapsed: 1.33 min\n",
      "Epoch: 510/513 | Batch 000/059 | Loss: 295.2443\n",
      "Epoch: 510/513 Train Loss: 213.7419\n",
      "Time elapsed: 1.34 min\n",
      "Epoch: 511/513 | Batch 000/059 | Loss: 282.6091\n",
      "Epoch: 511/513 Train Loss: 212.5672\n",
      "Time elapsed: 1.34 min\n",
      "Epoch: 512/513 | Batch 000/059 | Loss: 176.8170\n",
      "Epoch: 512/513 Train Loss: 212.9560\n",
      "Time elapsed: 1.34 min\n",
      "Epoch: 513/513 | Batch 000/059 | Loss: 321.3940\n",
      "Epoch: 513/513 Train Loss: 210.4330\n",
      "Time elapsed: 1.35 min\n",
      "Total Training Time: 1.35 min\n",
      "Training Loss: 210.43\n",
      "Test Loss: 220.31\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "minibatch_cost = []\n",
    "epoch_cost = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(device)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        outputs = model(features)\n",
    "        \n",
    "        cost = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        minibatch_cost.append(cost)\n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 1000:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "       \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        cost = compute_loss(model, train_loader)\n",
    "        epoch_cost.append(cost)\n",
    "        print('Epoch: %03d/%03d Train Loss: %.4f' % (\n",
    "                epoch+1, num_epochs, cost))\n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "        \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "    print('Training Loss: %.2f' % compute_loss(model, train_loader))\n",
    "    print('Test Loss: %.2f' % compute_loss(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "se_q-tBWsVR3",
    "outputId": "5906a9d7-8ceb-408a-860d-6dd688d81195"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the loss curve\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(len(epoch_cost)), epoch_cost)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross entropy loss\")\n",
    "plt.title(\"Loss of training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a481kCYNO9of"
   },
   "source": [
    "## 8. RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjKjUYTWsxwW"
   },
   "source": [
    "### Model B: 1 Hidden Layer Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TO8ZyqJcsxwW"
   },
   "source": [
    "### Create model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VLn-c5CQsxwX",
    "outputId": "1c91bcf9-be68-4067-b9f2-3f5d1d1f5ff5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "n_iters = 30000\n",
    "num_epochs = n_iters / (len(train_data) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "print(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "id": "fjXfo8E4sxwa"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # create your dataloader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "id": "yflYszlKsxwa"
   },
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, output_dim):\n",
    "        super(FNN, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear4 = nn.Linear(hidden_dim1, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.linear4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "id": "LfwnODvBsxwa"
   },
   "outputs": [],
   "source": [
    "input_dim = tensor_x.shape[1]\n",
    "output_dim = 1\n",
    "hidden_dim1 = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "id": "4z5Hyz29sxwa"
   },
   "outputs": [],
   "source": [
    "model = FNN(input_dim, hidden_dim1, output_dim)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5OJKThv5sxwa",
    "outputId": "ab9a0939-4fdc-4b30-cb86-cd06c398dba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 256]           2,816\n",
      "           Dropout-2               [-1, 1, 256]               0\n",
      "              ReLU-3               [-1, 1, 256]               0\n",
      "            Linear-4                 [-1, 1, 1]             257\n",
      "================================================================\n",
      "Total params: 3,073\n",
      "Trainable params: 3,073\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1,tensor_x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "id": "vQAnlWC2sxwb"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 0.0001\n",
    "LAMBDA = 0.05\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=LAMBDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zELRVqZBsxwb"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zzfUgf2sxwb",
    "outputId": "d94ed2c2-d65e-44d9-c87c-a91593c0dcc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/513 | Batch 000/059 | Loss: 7323.1660\n",
      "Epoch: 001/513 Train Loss: 6512.9612\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 002/513 | Batch 000/059 | Loss: 6360.2568\n",
      "Epoch: 002/513 Train Loss: 6147.2818\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 003/513 | Batch 000/059 | Loss: 5195.2656\n",
      "Epoch: 003/513 Train Loss: 5803.2204\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 004/513 | Batch 000/059 | Loss: 6268.8418\n",
      "Epoch: 004/513 Train Loss: 5452.6751\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 005/513 | Batch 000/059 | Loss: 5142.9868\n",
      "Epoch: 005/513 Train Loss: 5087.6417\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 006/513 | Batch 000/059 | Loss: 5538.0869\n",
      "Epoch: 006/513 Train Loss: 4707.6977\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 007/513 | Batch 000/059 | Loss: 4698.6978\n",
      "Epoch: 007/513 Train Loss: 4316.3904\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 008/513 | Batch 000/059 | Loss: 3664.9292\n",
      "Epoch: 008/513 Train Loss: 3917.9313\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 009/513 | Batch 000/059 | Loss: 3865.5667\n",
      "Epoch: 009/513 Train Loss: 3515.5321\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 010/513 | Batch 000/059 | Loss: 3918.9587\n",
      "Epoch: 010/513 Train Loss: 3115.1676\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 011/513 | Batch 000/059 | Loss: 3012.4077\n",
      "Epoch: 011/513 Train Loss: 2723.2473\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 012/513 | Batch 000/059 | Loss: 2608.3032\n",
      "Epoch: 012/513 Train Loss: 2345.4106\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 013/513 | Batch 000/059 | Loss: 1928.0298\n",
      "Epoch: 013/513 Train Loss: 1985.3870\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 014/513 | Batch 000/059 | Loss: 2135.5002\n",
      "Epoch: 014/513 Train Loss: 1652.1642\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 015/513 | Batch 000/059 | Loss: 1748.8574\n",
      "Epoch: 015/513 Train Loss: 1352.8478\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 016/513 | Batch 000/059 | Loss: 1221.4711\n",
      "Epoch: 016/513 Train Loss: 1091.9676\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 017/513 | Batch 000/059 | Loss: 609.2225\n",
      "Epoch: 017/513 Train Loss: 872.8741\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 018/513 | Batch 000/059 | Loss: 1198.0631\n",
      "Epoch: 018/513 Train Loss: 699.7790\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 019/513 | Batch 000/059 | Loss: 555.7133\n",
      "Epoch: 019/513 Train Loss: 570.1360\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 020/513 | Batch 000/059 | Loss: 314.2251\n",
      "Epoch: 020/513 Train Loss: 480.5832\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 021/513 | Batch 000/059 | Loss: 433.8889\n",
      "Epoch: 021/513 Train Loss: 426.5330\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 022/513 | Batch 000/059 | Loss: 467.9279\n",
      "Epoch: 022/513 Train Loss: 394.7145\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 023/513 | Batch 000/059 | Loss: 369.4811\n",
      "Epoch: 023/513 Train Loss: 378.3998\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 024/513 | Batch 000/059 | Loss: 360.7600\n",
      "Epoch: 024/513 Train Loss: 370.1302\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 025/513 | Batch 000/059 | Loss: 706.2403\n",
      "Epoch: 025/513 Train Loss: 364.7884\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 026/513 | Batch 000/059 | Loss: 438.2854\n",
      "Epoch: 026/513 Train Loss: 361.3992\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 027/513 | Batch 000/059 | Loss: 308.6320\n",
      "Epoch: 027/513 Train Loss: 357.7885\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 028/513 | Batch 000/059 | Loss: 509.7815\n",
      "Epoch: 028/513 Train Loss: 354.8551\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 029/513 | Batch 000/059 | Loss: 259.3863\n",
      "Epoch: 029/513 Train Loss: 352.4672\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 030/513 | Batch 000/059 | Loss: 328.5133\n",
      "Epoch: 030/513 Train Loss: 350.5236\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 031/513 | Batch 000/059 | Loss: 623.7805\n",
      "Epoch: 031/513 Train Loss: 348.9489\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 032/513 | Batch 000/059 | Loss: 266.3593\n",
      "Epoch: 032/513 Train Loss: 347.5531\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 033/513 | Batch 000/059 | Loss: 352.4275\n",
      "Epoch: 033/513 Train Loss: 346.2776\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 034/513 | Batch 000/059 | Loss: 410.7833\n",
      "Epoch: 034/513 Train Loss: 345.3381\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 035/513 | Batch 000/059 | Loss: 169.2891\n",
      "Epoch: 035/513 Train Loss: 344.5814\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 036/513 | Batch 000/059 | Loss: 499.3637\n",
      "Epoch: 036/513 Train Loss: 343.9739\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 037/513 | Batch 000/059 | Loss: 307.4681\n",
      "Epoch: 037/513 Train Loss: 343.3110\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 038/513 | Batch 000/059 | Loss: 233.6259\n",
      "Epoch: 038/513 Train Loss: 342.3651\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 039/513 | Batch 000/059 | Loss: 360.7957\n",
      "Epoch: 039/513 Train Loss: 341.6830\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 040/513 | Batch 000/059 | Loss: 284.9063\n",
      "Epoch: 040/513 Train Loss: 341.2000\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 041/513 | Batch 000/059 | Loss: 239.7182\n",
      "Epoch: 041/513 Train Loss: 340.5577\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 042/513 | Batch 000/059 | Loss: 459.0267\n",
      "Epoch: 042/513 Train Loss: 339.9031\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 043/513 | Batch 000/059 | Loss: 288.0456\n",
      "Epoch: 043/513 Train Loss: 339.3441\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 044/513 | Batch 000/059 | Loss: 260.7934\n",
      "Epoch: 044/513 Train Loss: 338.7961\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 045/513 | Batch 000/059 | Loss: 272.1291\n",
      "Epoch: 045/513 Train Loss: 338.3512\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 046/513 | Batch 000/059 | Loss: 335.2525\n",
      "Epoch: 046/513 Train Loss: 337.8049\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 047/513 | Batch 000/059 | Loss: 436.9568\n",
      "Epoch: 047/513 Train Loss: 337.4500\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 048/513 | Batch 000/059 | Loss: 406.3563\n",
      "Epoch: 048/513 Train Loss: 337.0640\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 049/513 | Batch 000/059 | Loss: 509.4869\n",
      "Epoch: 049/513 Train Loss: 336.6185\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 050/513 | Batch 000/059 | Loss: 708.1263\n",
      "Epoch: 050/513 Train Loss: 335.7813\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 051/513 | Batch 000/059 | Loss: 456.2054\n",
      "Epoch: 051/513 Train Loss: 335.4874\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 052/513 | Batch 000/059 | Loss: 248.1746\n",
      "Epoch: 052/513 Train Loss: 334.9096\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 053/513 | Batch 000/059 | Loss: 308.3850\n",
      "Epoch: 053/513 Train Loss: 334.4371\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 054/513 | Batch 000/059 | Loss: 384.2705\n",
      "Epoch: 054/513 Train Loss: 333.9997\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 055/513 | Batch 000/059 | Loss: 265.7054\n",
      "Epoch: 055/513 Train Loss: 333.5649\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 056/513 | Batch 000/059 | Loss: 476.2908\n",
      "Epoch: 056/513 Train Loss: 333.0327\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 057/513 | Batch 000/059 | Loss: 271.3096\n",
      "Epoch: 057/513 Train Loss: 332.5506\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 058/513 | Batch 000/059 | Loss: 514.5463\n",
      "Epoch: 058/513 Train Loss: 332.1850\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 059/513 | Batch 000/059 | Loss: 465.3350\n",
      "Epoch: 059/513 Train Loss: 331.7016\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 060/513 | Batch 000/059 | Loss: 255.6998\n",
      "Epoch: 060/513 Train Loss: 331.2813\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 061/513 | Batch 000/059 | Loss: 324.3683\n",
      "Epoch: 061/513 Train Loss: 331.0352\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 062/513 | Batch 000/059 | Loss: 191.4186\n",
      "Epoch: 062/513 Train Loss: 330.4135\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 063/513 | Batch 000/059 | Loss: 340.9977\n",
      "Epoch: 063/513 Train Loss: 330.0725\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 064/513 | Batch 000/059 | Loss: 496.0289\n",
      "Epoch: 064/513 Train Loss: 329.6741\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 065/513 | Batch 000/059 | Loss: 313.8849\n",
      "Epoch: 065/513 Train Loss: 329.2704\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 066/513 | Batch 000/059 | Loss: 268.1419\n",
      "Epoch: 066/513 Train Loss: 328.6809\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 067/513 | Batch 000/059 | Loss: 601.3104\n",
      "Epoch: 067/513 Train Loss: 328.3606\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 068/513 | Batch 000/059 | Loss: 314.2390\n",
      "Epoch: 068/513 Train Loss: 328.0084\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 069/513 | Batch 000/059 | Loss: 250.9038\n",
      "Epoch: 069/513 Train Loss: 327.4907\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 070/513 | Batch 000/059 | Loss: 187.5668\n",
      "Epoch: 070/513 Train Loss: 327.0328\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 071/513 | Batch 000/059 | Loss: 260.3615\n",
      "Epoch: 071/513 Train Loss: 326.6139\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 072/513 | Batch 000/059 | Loss: 189.7476\n",
      "Epoch: 072/513 Train Loss: 326.2112\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 073/513 | Batch 000/059 | Loss: 414.9334\n",
      "Epoch: 073/513 Train Loss: 325.8213\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 074/513 | Batch 000/059 | Loss: 267.9239\n",
      "Epoch: 074/513 Train Loss: 325.8522\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 075/513 | Batch 000/059 | Loss: 319.4265\n",
      "Epoch: 075/513 Train Loss: 325.1663\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 076/513 | Batch 000/059 | Loss: 415.8256\n",
      "Epoch: 076/513 Train Loss: 324.7016\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 077/513 | Batch 000/059 | Loss: 200.2924\n",
      "Epoch: 077/513 Train Loss: 324.2672\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 078/513 | Batch 000/059 | Loss: 393.8207\n",
      "Epoch: 078/513 Train Loss: 323.9028\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 079/513 | Batch 000/059 | Loss: 406.8472\n",
      "Epoch: 079/513 Train Loss: 323.5074\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 080/513 | Batch 000/059 | Loss: 283.6068\n",
      "Epoch: 080/513 Train Loss: 323.1127\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 081/513 | Batch 000/059 | Loss: 300.2652\n",
      "Epoch: 081/513 Train Loss: 322.9795\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 082/513 | Batch 000/059 | Loss: 254.6657\n",
      "Epoch: 082/513 Train Loss: 322.3132\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 083/513 | Batch 000/059 | Loss: 413.7776\n",
      "Epoch: 083/513 Train Loss: 322.0095\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 084/513 | Batch 000/059 | Loss: 257.3735\n",
      "Epoch: 084/513 Train Loss: 321.7839\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 085/513 | Batch 000/059 | Loss: 445.4098\n",
      "Epoch: 085/513 Train Loss: 321.3815\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 086/513 | Batch 000/059 | Loss: 228.1484\n",
      "Epoch: 086/513 Train Loss: 320.7971\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 087/513 | Batch 000/059 | Loss: 157.1188\n",
      "Epoch: 087/513 Train Loss: 320.5414\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 088/513 | Batch 000/059 | Loss: 349.3728\n",
      "Epoch: 088/513 Train Loss: 320.3840\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 089/513 | Batch 000/059 | Loss: 347.2281\n",
      "Epoch: 089/513 Train Loss: 319.5874\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 090/513 | Batch 000/059 | Loss: 222.8014\n",
      "Epoch: 090/513 Train Loss: 319.1803\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 091/513 | Batch 000/059 | Loss: 437.4090\n",
      "Epoch: 091/513 Train Loss: 318.9537\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 092/513 | Batch 000/059 | Loss: 252.9635\n",
      "Epoch: 092/513 Train Loss: 318.4221\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 093/513 | Batch 000/059 | Loss: 296.5991\n",
      "Epoch: 093/513 Train Loss: 317.9661\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 094/513 | Batch 000/059 | Loss: 481.1197\n",
      "Epoch: 094/513 Train Loss: 317.5579\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 095/513 | Batch 000/059 | Loss: 133.9233\n",
      "Epoch: 095/513 Train Loss: 317.1549\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 096/513 | Batch 000/059 | Loss: 318.2949\n",
      "Epoch: 096/513 Train Loss: 316.9974\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 097/513 | Batch 000/059 | Loss: 432.7335\n",
      "Epoch: 097/513 Train Loss: 316.3621\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 098/513 | Batch 000/059 | Loss: 256.1324\n",
      "Epoch: 098/513 Train Loss: 315.9812\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 099/513 | Batch 000/059 | Loss: 408.5389\n",
      "Epoch: 099/513 Train Loss: 315.5493\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 100/513 | Batch 000/059 | Loss: 642.4688\n",
      "Epoch: 100/513 Train Loss: 315.1673\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 101/513 | Batch 000/059 | Loss: 321.7401\n",
      "Epoch: 101/513 Train Loss: 314.7036\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 102/513 | Batch 000/059 | Loss: 353.7050\n",
      "Epoch: 102/513 Train Loss: 314.4704\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 103/513 | Batch 000/059 | Loss: 423.4315\n",
      "Epoch: 103/513 Train Loss: 313.9434\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 104/513 | Batch 000/059 | Loss: 373.6568\n",
      "Epoch: 104/513 Train Loss: 313.6831\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 105/513 | Batch 000/059 | Loss: 319.2574\n",
      "Epoch: 105/513 Train Loss: 313.1695\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 106/513 | Batch 000/059 | Loss: 300.3588\n",
      "Epoch: 106/513 Train Loss: 312.6705\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 107/513 | Batch 000/059 | Loss: 187.7592\n",
      "Epoch: 107/513 Train Loss: 312.3845\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 108/513 | Batch 000/059 | Loss: 262.7585\n",
      "Epoch: 108/513 Train Loss: 311.9139\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 109/513 | Batch 000/059 | Loss: 295.9597\n",
      "Epoch: 109/513 Train Loss: 311.5105\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 110/513 | Batch 000/059 | Loss: 252.8920\n",
      "Epoch: 110/513 Train Loss: 311.2147\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 111/513 | Batch 000/059 | Loss: 299.2198\n",
      "Epoch: 111/513 Train Loss: 310.8874\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 112/513 | Batch 000/059 | Loss: 272.8323\n",
      "Epoch: 112/513 Train Loss: 310.2941\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 113/513 | Batch 000/059 | Loss: 248.8620\n",
      "Epoch: 113/513 Train Loss: 309.9147\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 114/513 | Batch 000/059 | Loss: 296.5592\n",
      "Epoch: 114/513 Train Loss: 309.9101\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 115/513 | Batch 000/059 | Loss: 257.5746\n",
      "Epoch: 115/513 Train Loss: 309.3524\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 116/513 | Batch 000/059 | Loss: 271.3747\n",
      "Epoch: 116/513 Train Loss: 308.7574\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 117/513 | Batch 000/059 | Loss: 356.7855\n",
      "Epoch: 117/513 Train Loss: 308.2853\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 118/513 | Batch 000/059 | Loss: 218.4563\n",
      "Epoch: 118/513 Train Loss: 307.8631\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 119/513 | Batch 000/059 | Loss: 151.2008\n",
      "Epoch: 119/513 Train Loss: 307.8022\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 120/513 | Batch 000/059 | Loss: 260.0245\n",
      "Epoch: 120/513 Train Loss: 307.0268\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 121/513 | Batch 000/059 | Loss: 674.7582\n",
      "Epoch: 121/513 Train Loss: 306.6893\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 122/513 | Batch 000/059 | Loss: 407.9501\n",
      "Epoch: 122/513 Train Loss: 306.2129\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 123/513 | Batch 000/059 | Loss: 461.6047\n",
      "Epoch: 123/513 Train Loss: 305.9061\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 124/513 | Batch 000/059 | Loss: 249.8941\n",
      "Epoch: 124/513 Train Loss: 305.3063\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 125/513 | Batch 000/059 | Loss: 305.5527\n",
      "Epoch: 125/513 Train Loss: 304.9747\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 126/513 | Batch 000/059 | Loss: 175.6565\n",
      "Epoch: 126/513 Train Loss: 304.5598\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 127/513 | Batch 000/059 | Loss: 415.8664\n",
      "Epoch: 127/513 Train Loss: 304.1258\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 128/513 | Batch 000/059 | Loss: 358.6823\n",
      "Epoch: 128/513 Train Loss: 303.7259\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 129/513 | Batch 000/059 | Loss: 301.2249\n",
      "Epoch: 129/513 Train Loss: 303.3338\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 130/513 | Batch 000/059 | Loss: 216.1926\n",
      "Epoch: 130/513 Train Loss: 302.9287\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 131/513 | Batch 000/059 | Loss: 484.7058\n",
      "Epoch: 131/513 Train Loss: 302.3876\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 132/513 | Batch 000/059 | Loss: 347.1730\n",
      "Epoch: 132/513 Train Loss: 301.9610\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 133/513 | Batch 000/059 | Loss: 393.5952\n",
      "Epoch: 133/513 Train Loss: 301.5290\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 134/513 | Batch 000/059 | Loss: 256.3945\n",
      "Epoch: 134/513 Train Loss: 301.0918\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 135/513 | Batch 000/059 | Loss: 273.9149\n",
      "Epoch: 135/513 Train Loss: 300.7503\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 136/513 | Batch 000/059 | Loss: 260.3228\n",
      "Epoch: 136/513 Train Loss: 300.2413\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 137/513 | Batch 000/059 | Loss: 374.1284\n",
      "Epoch: 137/513 Train Loss: 299.8216\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 138/513 | Batch 000/059 | Loss: 195.9878\n",
      "Epoch: 138/513 Train Loss: 299.3755\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 139/513 | Batch 000/059 | Loss: 203.6213\n",
      "Epoch: 139/513 Train Loss: 298.9787\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 140/513 | Batch 000/059 | Loss: 338.1467\n",
      "Epoch: 140/513 Train Loss: 298.5346\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 141/513 | Batch 000/059 | Loss: 427.7567\n",
      "Epoch: 141/513 Train Loss: 298.4369\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 142/513 | Batch 000/059 | Loss: 288.4346\n",
      "Epoch: 142/513 Train Loss: 297.7364\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 143/513 | Batch 000/059 | Loss: 382.4084\n",
      "Epoch: 143/513 Train Loss: 297.3045\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 144/513 | Batch 000/059 | Loss: 200.8010\n",
      "Epoch: 144/513 Train Loss: 296.8779\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 145/513 | Batch 000/059 | Loss: 213.8460\n",
      "Epoch: 145/513 Train Loss: 296.4398\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 146/513 | Batch 000/059 | Loss: 312.4935\n",
      "Epoch: 146/513 Train Loss: 296.0661\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 147/513 | Batch 000/059 | Loss: 301.7068\n",
      "Epoch: 147/513 Train Loss: 295.6338\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 148/513 | Batch 000/059 | Loss: 224.2127\n",
      "Epoch: 148/513 Train Loss: 295.2197\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 149/513 | Batch 000/059 | Loss: 343.0526\n",
      "Epoch: 149/513 Train Loss: 294.9460\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 150/513 | Batch 000/059 | Loss: 322.2398\n",
      "Epoch: 150/513 Train Loss: 294.4791\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 151/513 | Batch 000/059 | Loss: 298.0483\n",
      "Epoch: 151/513 Train Loss: 294.3962\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 152/513 | Batch 000/059 | Loss: 286.4130\n",
      "Epoch: 152/513 Train Loss: 293.6972\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 153/513 | Batch 000/059 | Loss: 423.9508\n",
      "Epoch: 153/513 Train Loss: 293.4211\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 154/513 | Batch 000/059 | Loss: 478.4330\n",
      "Epoch: 154/513 Train Loss: 292.8067\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 155/513 | Batch 000/059 | Loss: 236.0122\n",
      "Epoch: 155/513 Train Loss: 292.3904\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 156/513 | Batch 000/059 | Loss: 363.8038\n",
      "Epoch: 156/513 Train Loss: 292.0898\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 157/513 | Batch 000/059 | Loss: 376.4943\n",
      "Epoch: 157/513 Train Loss: 291.6528\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 158/513 | Batch 000/059 | Loss: 311.8484\n",
      "Epoch: 158/513 Train Loss: 291.1815\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 159/513 | Batch 000/059 | Loss: 275.1476\n",
      "Epoch: 159/513 Train Loss: 290.7872\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 160/513 | Batch 000/059 | Loss: 328.7433\n",
      "Epoch: 160/513 Train Loss: 290.4330\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 161/513 | Batch 000/059 | Loss: 306.8099\n",
      "Epoch: 161/513 Train Loss: 290.4754\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 162/513 | Batch 000/059 | Loss: 337.1669\n",
      "Epoch: 162/513 Train Loss: 289.5289\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 163/513 | Batch 000/059 | Loss: 241.6180\n",
      "Epoch: 163/513 Train Loss: 289.1485\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 164/513 | Batch 000/059 | Loss: 372.6095\n",
      "Epoch: 164/513 Train Loss: 288.6859\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 165/513 | Batch 000/059 | Loss: 335.1202\n",
      "Epoch: 165/513 Train Loss: 288.2429\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 166/513 | Batch 000/059 | Loss: 223.6418\n",
      "Epoch: 166/513 Train Loss: 287.8137\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 167/513 | Batch 000/059 | Loss: 274.7983\n",
      "Epoch: 167/513 Train Loss: 287.5075\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 168/513 | Batch 000/059 | Loss: 371.0607\n",
      "Epoch: 168/513 Train Loss: 287.0882\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 169/513 | Batch 000/059 | Loss: 273.5433\n",
      "Epoch: 169/513 Train Loss: 286.7027\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 170/513 | Batch 000/059 | Loss: 401.8603\n",
      "Epoch: 170/513 Train Loss: 286.2071\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 171/513 | Batch 000/059 | Loss: 187.6260\n",
      "Epoch: 171/513 Train Loss: 285.8524\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 172/513 | Batch 000/059 | Loss: 247.9752\n",
      "Epoch: 172/513 Train Loss: 285.4315\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 173/513 | Batch 000/059 | Loss: 416.2093\n",
      "Epoch: 173/513 Train Loss: 284.9779\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 174/513 | Batch 000/059 | Loss: 402.7961\n",
      "Epoch: 174/513 Train Loss: 284.5353\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 175/513 | Batch 000/059 | Loss: 160.3114\n",
      "Epoch: 175/513 Train Loss: 284.2973\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 176/513 | Batch 000/059 | Loss: 307.5400\n",
      "Epoch: 176/513 Train Loss: 283.8652\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 177/513 | Batch 000/059 | Loss: 265.5919\n",
      "Epoch: 177/513 Train Loss: 283.3036\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 178/513 | Batch 000/059 | Loss: 254.3132\n",
      "Epoch: 178/513 Train Loss: 283.0173\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 179/513 | Batch 000/059 | Loss: 290.5343\n",
      "Epoch: 179/513 Train Loss: 282.6126\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 180/513 | Batch 000/059 | Loss: 209.3474\n",
      "Epoch: 180/513 Train Loss: 282.0701\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 181/513 | Batch 000/059 | Loss: 308.9246\n",
      "Epoch: 181/513 Train Loss: 281.7721\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 182/513 | Batch 000/059 | Loss: 301.6961\n",
      "Epoch: 182/513 Train Loss: 281.2074\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 183/513 | Batch 000/059 | Loss: 294.4668\n",
      "Epoch: 183/513 Train Loss: 280.8258\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 184/513 | Batch 000/059 | Loss: 248.9520\n",
      "Epoch: 184/513 Train Loss: 280.3444\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 185/513 | Batch 000/059 | Loss: 155.7456\n",
      "Epoch: 185/513 Train Loss: 280.1411\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 186/513 | Batch 000/059 | Loss: 283.9744\n",
      "Epoch: 186/513 Train Loss: 279.5993\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 187/513 | Batch 000/059 | Loss: 508.9983\n",
      "Epoch: 187/513 Train Loss: 279.1521\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 188/513 | Batch 000/059 | Loss: 180.6422\n",
      "Epoch: 188/513 Train Loss: 278.8480\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 189/513 | Batch 000/059 | Loss: 245.1457\n",
      "Epoch: 189/513 Train Loss: 278.4274\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 190/513 | Batch 000/059 | Loss: 484.6432\n",
      "Epoch: 190/513 Train Loss: 278.0278\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 191/513 | Batch 000/059 | Loss: 257.7146\n",
      "Epoch: 191/513 Train Loss: 277.9116\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 192/513 | Batch 000/059 | Loss: 238.0360\n",
      "Epoch: 192/513 Train Loss: 277.2493\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 193/513 | Batch 000/059 | Loss: 227.1144\n",
      "Epoch: 193/513 Train Loss: 277.0905\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 194/513 | Batch 000/059 | Loss: 341.3521\n",
      "Epoch: 194/513 Train Loss: 276.4714\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 195/513 | Batch 000/059 | Loss: 311.8184\n",
      "Epoch: 195/513 Train Loss: 275.9900\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 196/513 | Batch 000/059 | Loss: 205.3129\n",
      "Epoch: 196/513 Train Loss: 275.5890\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 197/513 | Batch 000/059 | Loss: 241.7829\n",
      "Epoch: 197/513 Train Loss: 275.1950\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 198/513 | Batch 000/059 | Loss: 225.1735\n",
      "Epoch: 198/513 Train Loss: 274.7563\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 199/513 | Batch 000/059 | Loss: 298.1815\n",
      "Epoch: 199/513 Train Loss: 274.4377\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 200/513 | Batch 000/059 | Loss: 296.6669\n",
      "Epoch: 200/513 Train Loss: 273.9730\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 201/513 | Batch 000/059 | Loss: 304.9005\n",
      "Epoch: 201/513 Train Loss: 273.5933\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 202/513 | Batch 000/059 | Loss: 266.4237\n",
      "Epoch: 202/513 Train Loss: 273.2851\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 203/513 | Batch 000/059 | Loss: 273.0493\n",
      "Epoch: 203/513 Train Loss: 272.8781\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 204/513 | Batch 000/059 | Loss: 233.1927\n",
      "Epoch: 204/513 Train Loss: 272.5451\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 205/513 | Batch 000/059 | Loss: 309.2917\n",
      "Epoch: 205/513 Train Loss: 272.1362\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 206/513 | Batch 000/059 | Loss: 425.8317\n",
      "Epoch: 206/513 Train Loss: 271.7916\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 207/513 | Batch 000/059 | Loss: 366.5125\n",
      "Epoch: 207/513 Train Loss: 271.4897\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 208/513 | Batch 000/059 | Loss: 564.2877\n",
      "Epoch: 208/513 Train Loss: 271.0958\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 209/513 | Batch 000/059 | Loss: 321.9843\n",
      "Epoch: 209/513 Train Loss: 270.7394\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 210/513 | Batch 000/059 | Loss: 396.0676\n",
      "Epoch: 210/513 Train Loss: 270.2956\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 211/513 | Batch 000/059 | Loss: 243.4160\n",
      "Epoch: 211/513 Train Loss: 269.9246\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 212/513 | Batch 000/059 | Loss: 341.9094\n",
      "Epoch: 212/513 Train Loss: 269.5722\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 213/513 | Batch 000/059 | Loss: 282.2345\n",
      "Epoch: 213/513 Train Loss: 269.3486\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 214/513 | Batch 000/059 | Loss: 362.1128\n",
      "Epoch: 214/513 Train Loss: 268.8222\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 215/513 | Batch 000/059 | Loss: 210.3106\n",
      "Epoch: 215/513 Train Loss: 268.6825\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 216/513 | Batch 000/059 | Loss: 182.9146\n",
      "Epoch: 216/513 Train Loss: 268.0956\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 217/513 | Batch 000/059 | Loss: 292.6306\n",
      "Epoch: 217/513 Train Loss: 267.8596\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 218/513 | Batch 000/059 | Loss: 182.2875\n",
      "Epoch: 218/513 Train Loss: 267.3824\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 219/513 | Batch 000/059 | Loss: 197.2883\n",
      "Epoch: 219/513 Train Loss: 267.2740\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 220/513 | Batch 000/059 | Loss: 166.6556\n",
      "Epoch: 220/513 Train Loss: 266.8791\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 221/513 | Batch 000/059 | Loss: 203.3311\n",
      "Epoch: 221/513 Train Loss: 266.5764\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 222/513 | Batch 000/059 | Loss: 375.7283\n",
      "Epoch: 222/513 Train Loss: 266.3034\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 223/513 | Batch 000/059 | Loss: 204.9118\n",
      "Epoch: 223/513 Train Loss: 265.7872\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 224/513 | Batch 000/059 | Loss: 235.3518\n",
      "Epoch: 224/513 Train Loss: 265.4193\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 225/513 | Batch 000/059 | Loss: 320.9905\n",
      "Epoch: 225/513 Train Loss: 265.0111\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 226/513 | Batch 000/059 | Loss: 274.4011\n",
      "Epoch: 226/513 Train Loss: 264.7401\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 227/513 | Batch 000/059 | Loss: 219.4976\n",
      "Epoch: 227/513 Train Loss: 264.3896\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 228/513 | Batch 000/059 | Loss: 383.3052\n",
      "Epoch: 228/513 Train Loss: 264.0798\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 229/513 | Batch 000/059 | Loss: 356.6724\n",
      "Epoch: 229/513 Train Loss: 263.7684\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 230/513 | Batch 000/059 | Loss: 241.9143\n",
      "Epoch: 230/513 Train Loss: 263.3007\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 231/513 | Batch 000/059 | Loss: 475.6631\n",
      "Epoch: 231/513 Train Loss: 263.1343\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 232/513 | Batch 000/059 | Loss: 308.2675\n",
      "Epoch: 232/513 Train Loss: 263.0210\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 233/513 | Batch 000/059 | Loss: 468.5951\n",
      "Epoch: 233/513 Train Loss: 262.2563\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 234/513 | Batch 000/059 | Loss: 213.2652\n",
      "Epoch: 234/513 Train Loss: 261.9693\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 235/513 | Batch 000/059 | Loss: 226.3852\n",
      "Epoch: 235/513 Train Loss: 261.7143\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 236/513 | Batch 000/059 | Loss: 248.6273\n",
      "Epoch: 236/513 Train Loss: 261.3093\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 237/513 | Batch 000/059 | Loss: 345.7121\n",
      "Epoch: 237/513 Train Loss: 261.1664\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 238/513 | Batch 000/059 | Loss: 277.9355\n",
      "Epoch: 238/513 Train Loss: 260.6385\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 239/513 | Batch 000/059 | Loss: 234.3476\n",
      "Epoch: 239/513 Train Loss: 260.4742\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 240/513 | Batch 000/059 | Loss: 231.9185\n",
      "Epoch: 240/513 Train Loss: 260.1029\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 241/513 | Batch 000/059 | Loss: 315.4637\n",
      "Epoch: 241/513 Train Loss: 259.8399\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 242/513 | Batch 000/059 | Loss: 395.5651\n",
      "Epoch: 242/513 Train Loss: 259.7375\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 243/513 | Batch 000/059 | Loss: 142.8710\n",
      "Epoch: 243/513 Train Loss: 259.1804\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 244/513 | Batch 000/059 | Loss: 201.2923\n",
      "Epoch: 244/513 Train Loss: 258.8194\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 245/513 | Batch 000/059 | Loss: 474.6436\n",
      "Epoch: 245/513 Train Loss: 258.5937\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 246/513 | Batch 000/059 | Loss: 278.8746\n",
      "Epoch: 246/513 Train Loss: 258.1553\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 247/513 | Batch 000/059 | Loss: 356.5442\n",
      "Epoch: 247/513 Train Loss: 257.7881\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 248/513 | Batch 000/059 | Loss: 420.9251\n",
      "Epoch: 248/513 Train Loss: 257.4949\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 249/513 | Batch 000/059 | Loss: 265.2883\n",
      "Epoch: 249/513 Train Loss: 257.3424\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 250/513 | Batch 000/059 | Loss: 217.3058\n",
      "Epoch: 250/513 Train Loss: 256.9197\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 251/513 | Batch 000/059 | Loss: 365.9989\n",
      "Epoch: 251/513 Train Loss: 256.6532\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 252/513 | Batch 000/059 | Loss: 272.4793\n",
      "Epoch: 252/513 Train Loss: 256.3892\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 253/513 | Batch 000/059 | Loss: 217.4711\n",
      "Epoch: 253/513 Train Loss: 256.1316\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 254/513 | Batch 000/059 | Loss: 224.3878\n",
      "Epoch: 254/513 Train Loss: 255.7940\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 255/513 | Batch 000/059 | Loss: 315.2798\n",
      "Epoch: 255/513 Train Loss: 255.4911\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 256/513 | Batch 000/059 | Loss: 154.6518\n",
      "Epoch: 256/513 Train Loss: 255.5250\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 257/513 | Batch 000/059 | Loss: 447.0246\n",
      "Epoch: 257/513 Train Loss: 254.9099\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 258/513 | Batch 000/059 | Loss: 253.7081\n",
      "Epoch: 258/513 Train Loss: 255.1829\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 259/513 | Batch 000/059 | Loss: 168.1847\n",
      "Epoch: 259/513 Train Loss: 254.4909\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 260/513 | Batch 000/059 | Loss: 281.5676\n",
      "Epoch: 260/513 Train Loss: 254.1565\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 261/513 | Batch 000/059 | Loss: 203.5216\n",
      "Epoch: 261/513 Train Loss: 253.9594\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 262/513 | Batch 000/059 | Loss: 293.2800\n",
      "Epoch: 262/513 Train Loss: 253.6106\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 263/513 | Batch 000/059 | Loss: 273.8193\n",
      "Epoch: 263/513 Train Loss: 253.5090\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 264/513 | Batch 000/059 | Loss: 234.0490\n",
      "Epoch: 264/513 Train Loss: 253.1231\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 265/513 | Batch 000/059 | Loss: 174.0912\n",
      "Epoch: 265/513 Train Loss: 252.9989\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 266/513 | Batch 000/059 | Loss: 254.5625\n",
      "Epoch: 266/513 Train Loss: 252.6459\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 267/513 | Batch 000/059 | Loss: 336.7971\n",
      "Epoch: 267/513 Train Loss: 252.3294\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 268/513 | Batch 000/059 | Loss: 289.4811\n",
      "Epoch: 268/513 Train Loss: 252.3365\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 269/513 | Batch 000/059 | Loss: 293.2976\n",
      "Epoch: 269/513 Train Loss: 251.8250\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 270/513 | Batch 000/059 | Loss: 210.8678\n",
      "Epoch: 270/513 Train Loss: 251.7697\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 271/513 | Batch 000/059 | Loss: 228.3112\n",
      "Epoch: 271/513 Train Loss: 251.3635\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 272/513 | Batch 000/059 | Loss: 267.4281\n",
      "Epoch: 272/513 Train Loss: 250.9951\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 273/513 | Batch 000/059 | Loss: 184.1221\n",
      "Epoch: 273/513 Train Loss: 250.7223\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 274/513 | Batch 000/059 | Loss: 304.2263\n",
      "Epoch: 274/513 Train Loss: 250.4533\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 275/513 | Batch 000/059 | Loss: 318.6480\n",
      "Epoch: 275/513 Train Loss: 250.2729\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 276/513 | Batch 000/059 | Loss: 264.9985\n",
      "Epoch: 276/513 Train Loss: 250.3923\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 277/513 | Batch 000/059 | Loss: 220.7126\n",
      "Epoch: 277/513 Train Loss: 250.0817\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 278/513 | Batch 000/059 | Loss: 300.5369\n",
      "Epoch: 278/513 Train Loss: 249.7477\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 279/513 | Batch 000/059 | Loss: 117.9430\n",
      "Epoch: 279/513 Train Loss: 249.2992\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 280/513 | Batch 000/059 | Loss: 162.7597\n",
      "Epoch: 280/513 Train Loss: 249.2679\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 281/513 | Batch 000/059 | Loss: 176.3817\n",
      "Epoch: 281/513 Train Loss: 249.0806\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 282/513 | Batch 000/059 | Loss: 354.2217\n",
      "Epoch: 282/513 Train Loss: 248.7832\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 283/513 | Batch 000/059 | Loss: 361.4404\n",
      "Epoch: 283/513 Train Loss: 248.4188\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 284/513 | Batch 000/059 | Loss: 195.0305\n",
      "Epoch: 284/513 Train Loss: 248.6311\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 285/513 | Batch 000/059 | Loss: 293.8682\n",
      "Epoch: 285/513 Train Loss: 247.9946\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 286/513 | Batch 000/059 | Loss: 350.7168\n",
      "Epoch: 286/513 Train Loss: 247.8194\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 287/513 | Batch 000/059 | Loss: 224.6410\n",
      "Epoch: 287/513 Train Loss: 247.5698\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 288/513 | Batch 000/059 | Loss: 216.7454\n",
      "Epoch: 288/513 Train Loss: 247.3682\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 289/513 | Batch 000/059 | Loss: 166.7568\n",
      "Epoch: 289/513 Train Loss: 247.1258\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 290/513 | Batch 000/059 | Loss: 268.9359\n",
      "Epoch: 290/513 Train Loss: 247.4505\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 291/513 | Batch 000/059 | Loss: 223.0603\n",
      "Epoch: 291/513 Train Loss: 246.7797\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 292/513 | Batch 000/059 | Loss: 281.6266\n",
      "Epoch: 292/513 Train Loss: 246.8086\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 293/513 | Batch 000/059 | Loss: 252.2187\n",
      "Epoch: 293/513 Train Loss: 246.3215\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 294/513 | Batch 000/059 | Loss: 184.1742\n",
      "Epoch: 294/513 Train Loss: 246.0913\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 295/513 | Batch 000/059 | Loss: 232.3358\n",
      "Epoch: 295/513 Train Loss: 246.1547\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 296/513 | Batch 000/059 | Loss: 176.7253\n",
      "Epoch: 296/513 Train Loss: 245.8393\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 297/513 | Batch 000/059 | Loss: 156.1248\n",
      "Epoch: 297/513 Train Loss: 245.7745\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 298/513 | Batch 000/059 | Loss: 174.1263\n",
      "Epoch: 298/513 Train Loss: 245.4632\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 299/513 | Batch 000/059 | Loss: 179.0257\n",
      "Epoch: 299/513 Train Loss: 245.0799\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 300/513 | Batch 000/059 | Loss: 478.9547\n",
      "Epoch: 300/513 Train Loss: 244.9059\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 301/513 | Batch 000/059 | Loss: 397.8220\n",
      "Epoch: 301/513 Train Loss: 244.7888\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 302/513 | Batch 000/059 | Loss: 305.4999\n",
      "Epoch: 302/513 Train Loss: 244.8312\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 303/513 | Batch 000/059 | Loss: 379.3067\n",
      "Epoch: 303/513 Train Loss: 244.4711\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 304/513 | Batch 000/059 | Loss: 370.8440\n",
      "Epoch: 304/513 Train Loss: 244.2255\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 305/513 | Batch 000/059 | Loss: 227.8218\n",
      "Epoch: 305/513 Train Loss: 244.0787\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 306/513 | Batch 000/059 | Loss: 270.9345\n",
      "Epoch: 306/513 Train Loss: 243.8641\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 307/513 | Batch 000/059 | Loss: 296.9712\n",
      "Epoch: 307/513 Train Loss: 243.7154\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 308/513 | Batch 000/059 | Loss: 240.8351\n",
      "Epoch: 308/513 Train Loss: 243.5581\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 309/513 | Batch 000/059 | Loss: 302.9090\n",
      "Epoch: 309/513 Train Loss: 243.3931\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 310/513 | Batch 000/059 | Loss: 221.9556\n",
      "Epoch: 310/513 Train Loss: 243.5646\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 311/513 | Batch 000/059 | Loss: 451.5046\n",
      "Epoch: 311/513 Train Loss: 243.0249\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 312/513 | Batch 000/059 | Loss: 221.7120\n",
      "Epoch: 312/513 Train Loss: 242.8625\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 313/513 | Batch 000/059 | Loss: 274.7841\n",
      "Epoch: 313/513 Train Loss: 242.7742\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 314/513 | Batch 000/059 | Loss: 187.7415\n",
      "Epoch: 314/513 Train Loss: 242.5249\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 315/513 | Batch 000/059 | Loss: 202.7108\n",
      "Epoch: 315/513 Train Loss: 242.5366\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 316/513 | Batch 000/059 | Loss: 235.3759\n",
      "Epoch: 316/513 Train Loss: 242.2748\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 317/513 | Batch 000/059 | Loss: 250.4825\n",
      "Epoch: 317/513 Train Loss: 242.1710\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 318/513 | Batch 000/059 | Loss: 299.2990\n",
      "Epoch: 318/513 Train Loss: 242.1754\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 319/513 | Batch 000/059 | Loss: 337.2957\n",
      "Epoch: 319/513 Train Loss: 241.8318\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 320/513 | Batch 000/059 | Loss: 218.7359\n",
      "Epoch: 320/513 Train Loss: 241.6971\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 321/513 | Batch 000/059 | Loss: 253.4402\n",
      "Epoch: 321/513 Train Loss: 241.6007\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 322/513 | Batch 000/059 | Loss: 303.5876\n",
      "Epoch: 322/513 Train Loss: 241.8453\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 323/513 | Batch 000/059 | Loss: 234.7024\n",
      "Epoch: 323/513 Train Loss: 241.1409\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 324/513 | Batch 000/059 | Loss: 184.3463\n",
      "Epoch: 324/513 Train Loss: 241.0560\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 325/513 | Batch 000/059 | Loss: 184.7834\n",
      "Epoch: 325/513 Train Loss: 240.8810\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 326/513 | Batch 000/059 | Loss: 120.2449\n",
      "Epoch: 326/513 Train Loss: 240.7074\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 327/513 | Batch 000/059 | Loss: 268.2491\n",
      "Epoch: 327/513 Train Loss: 240.5972\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 328/513 | Batch 000/059 | Loss: 208.2218\n",
      "Epoch: 328/513 Train Loss: 240.4956\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 329/513 | Batch 000/059 | Loss: 162.9969\n",
      "Epoch: 329/513 Train Loss: 240.4093\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 330/513 | Batch 000/059 | Loss: 500.3944\n",
      "Epoch: 330/513 Train Loss: 240.2572\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 331/513 | Batch 000/059 | Loss: 323.6234\n",
      "Epoch: 331/513 Train Loss: 240.0224\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 332/513 | Batch 000/059 | Loss: 145.4417\n",
      "Epoch: 332/513 Train Loss: 239.9191\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 333/513 | Batch 000/059 | Loss: 149.2594\n",
      "Epoch: 333/513 Train Loss: 239.8091\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 334/513 | Batch 000/059 | Loss: 338.6662\n",
      "Epoch: 334/513 Train Loss: 239.6506\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 335/513 | Batch 000/059 | Loss: 198.9190\n",
      "Epoch: 335/513 Train Loss: 239.5616\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 336/513 | Batch 000/059 | Loss: 161.7050\n",
      "Epoch: 336/513 Train Loss: 239.5201\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 337/513 | Batch 000/059 | Loss: 270.6685\n",
      "Epoch: 337/513 Train Loss: 239.6865\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 338/513 | Batch 000/059 | Loss: 234.4734\n",
      "Epoch: 338/513 Train Loss: 239.1665\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 339/513 | Batch 000/059 | Loss: 295.1955\n",
      "Epoch: 339/513 Train Loss: 239.0027\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 340/513 | Batch 000/059 | Loss: 178.7153\n",
      "Epoch: 340/513 Train Loss: 238.9318\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 341/513 | Batch 000/059 | Loss: 200.6151\n",
      "Epoch: 341/513 Train Loss: 238.7844\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 342/513 | Batch 000/059 | Loss: 157.1364\n",
      "Epoch: 342/513 Train Loss: 238.6818\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 343/513 | Batch 000/059 | Loss: 340.2114\n",
      "Epoch: 343/513 Train Loss: 238.6190\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 344/513 | Batch 000/059 | Loss: 277.9614\n",
      "Epoch: 344/513 Train Loss: 238.4286\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 345/513 | Batch 000/059 | Loss: 184.9678\n",
      "Epoch: 345/513 Train Loss: 238.3496\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 346/513 | Batch 000/059 | Loss: 199.3512\n",
      "Epoch: 346/513 Train Loss: 238.3574\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 347/513 | Batch 000/059 | Loss: 385.1945\n",
      "Epoch: 347/513 Train Loss: 238.0933\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 348/513 | Batch 000/059 | Loss: 166.4341\n",
      "Epoch: 348/513 Train Loss: 237.9967\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 349/513 | Batch 000/059 | Loss: 182.4673\n",
      "Epoch: 349/513 Train Loss: 238.3702\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 350/513 | Batch 000/059 | Loss: 240.6057\n",
      "Epoch: 350/513 Train Loss: 237.8458\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 351/513 | Batch 000/059 | Loss: 458.5877\n",
      "Epoch: 351/513 Train Loss: 237.7235\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 352/513 | Batch 000/059 | Loss: 410.6525\n",
      "Epoch: 352/513 Train Loss: 237.6323\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 353/513 | Batch 000/059 | Loss: 276.8819\n",
      "Epoch: 353/513 Train Loss: 237.5168\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 354/513 | Batch 000/059 | Loss: 452.2232\n",
      "Epoch: 354/513 Train Loss: 237.4305\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 355/513 | Batch 000/059 | Loss: 212.8248\n",
      "Epoch: 355/513 Train Loss: 237.5642\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 356/513 | Batch 000/059 | Loss: 318.5016\n",
      "Epoch: 356/513 Train Loss: 237.2354\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 357/513 | Batch 000/059 | Loss: 228.4435\n",
      "Epoch: 357/513 Train Loss: 237.1509\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 358/513 | Batch 000/059 | Loss: 216.2862\n",
      "Epoch: 358/513 Train Loss: 237.1001\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 359/513 | Batch 000/059 | Loss: 282.5112\n",
      "Epoch: 359/513 Train Loss: 236.9676\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 360/513 | Batch 000/059 | Loss: 386.6760\n",
      "Epoch: 360/513 Train Loss: 236.9725\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 361/513 | Batch 000/059 | Loss: 295.1171\n",
      "Epoch: 361/513 Train Loss: 236.7613\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 362/513 | Batch 000/059 | Loss: 251.4747\n",
      "Epoch: 362/513 Train Loss: 236.8746\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 363/513 | Batch 000/059 | Loss: 164.7085\n",
      "Epoch: 363/513 Train Loss: 236.7816\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 364/513 | Batch 000/059 | Loss: 204.3912\n",
      "Epoch: 364/513 Train Loss: 236.6986\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 365/513 | Batch 000/059 | Loss: 148.3647\n",
      "Epoch: 365/513 Train Loss: 236.5441\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 366/513 | Batch 000/059 | Loss: 229.8003\n",
      "Epoch: 366/513 Train Loss: 236.3587\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 367/513 | Batch 000/059 | Loss: 164.3188\n",
      "Epoch: 367/513 Train Loss: 236.2901\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 368/513 | Batch 000/059 | Loss: 467.9707\n",
      "Epoch: 368/513 Train Loss: 236.4649\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 369/513 | Batch 000/059 | Loss: 226.9118\n",
      "Epoch: 369/513 Train Loss: 236.1130\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 370/513 | Batch 000/059 | Loss: 327.8452\n",
      "Epoch: 370/513 Train Loss: 236.4737\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 371/513 | Batch 000/059 | Loss: 249.8135\n",
      "Epoch: 371/513 Train Loss: 236.1703\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 372/513 | Batch 000/059 | Loss: 123.3371\n",
      "Epoch: 372/513 Train Loss: 235.9270\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 373/513 | Batch 000/059 | Loss: 145.3144\n",
      "Epoch: 373/513 Train Loss: 235.8219\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 374/513 | Batch 000/059 | Loss: 322.2714\n",
      "Epoch: 374/513 Train Loss: 235.8822\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 375/513 | Batch 000/059 | Loss: 294.4585\n",
      "Epoch: 375/513 Train Loss: 235.5948\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 376/513 | Batch 000/059 | Loss: 214.5245\n",
      "Epoch: 376/513 Train Loss: 235.5108\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 377/513 | Batch 000/059 | Loss: 207.5462\n",
      "Epoch: 377/513 Train Loss: 235.4742\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 378/513 | Batch 000/059 | Loss: 287.1841\n",
      "Epoch: 378/513 Train Loss: 235.3903\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 379/513 | Batch 000/059 | Loss: 264.0125\n",
      "Epoch: 379/513 Train Loss: 235.6769\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 380/513 | Batch 000/059 | Loss: 305.8953\n",
      "Epoch: 380/513 Train Loss: 235.2321\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 381/513 | Batch 000/059 | Loss: 344.9265\n",
      "Epoch: 381/513 Train Loss: 235.1894\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 382/513 | Batch 000/059 | Loss: 230.0524\n",
      "Epoch: 382/513 Train Loss: 235.5121\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 383/513 | Batch 000/059 | Loss: 340.2855\n",
      "Epoch: 383/513 Train Loss: 235.1672\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 384/513 | Batch 000/059 | Loss: 203.1144\n",
      "Epoch: 384/513 Train Loss: 234.9277\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 385/513 | Batch 000/059 | Loss: 196.1681\n",
      "Epoch: 385/513 Train Loss: 234.9064\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 386/513 | Batch 000/059 | Loss: 177.0671\n",
      "Epoch: 386/513 Train Loss: 234.9702\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 387/513 | Batch 000/059 | Loss: 425.3088\n",
      "Epoch: 387/513 Train Loss: 234.7722\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 388/513 | Batch 000/059 | Loss: 336.9586\n",
      "Epoch: 388/513 Train Loss: 234.7424\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 389/513 | Batch 000/059 | Loss: 234.7032\n",
      "Epoch: 389/513 Train Loss: 234.6028\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 390/513 | Batch 000/059 | Loss: 196.8628\n",
      "Epoch: 390/513 Train Loss: 234.5834\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 391/513 | Batch 000/059 | Loss: 190.5606\n",
      "Epoch: 391/513 Train Loss: 234.5134\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 392/513 | Batch 000/059 | Loss: 350.2551\n",
      "Epoch: 392/513 Train Loss: 234.4417\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 393/513 | Batch 000/059 | Loss: 365.0360\n",
      "Epoch: 393/513 Train Loss: 234.7072\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 394/513 | Batch 000/059 | Loss: 362.1704\n",
      "Epoch: 394/513 Train Loss: 234.3353\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 395/513 | Batch 000/059 | Loss: 279.1956\n",
      "Epoch: 395/513 Train Loss: 234.2842\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 396/513 | Batch 000/059 | Loss: 260.7345\n",
      "Epoch: 396/513 Train Loss: 234.2632\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 397/513 | Batch 000/059 | Loss: 231.9611\n",
      "Epoch: 397/513 Train Loss: 234.1857\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 398/513 | Batch 000/059 | Loss: 225.8687\n",
      "Epoch: 398/513 Train Loss: 234.1377\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 399/513 | Batch 000/059 | Loss: 280.2631\n",
      "Epoch: 399/513 Train Loss: 234.2868\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 400/513 | Batch 000/059 | Loss: 260.2364\n",
      "Epoch: 400/513 Train Loss: 233.9992\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 401/513 | Batch 000/059 | Loss: 261.1036\n",
      "Epoch: 401/513 Train Loss: 234.0904\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 402/513 | Batch 000/059 | Loss: 330.0093\n",
      "Epoch: 402/513 Train Loss: 233.8802\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 403/513 | Batch 000/059 | Loss: 305.0083\n",
      "Epoch: 403/513 Train Loss: 234.2204\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 404/513 | Batch 000/059 | Loss: 301.1449\n",
      "Epoch: 404/513 Train Loss: 233.7596\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 405/513 | Batch 000/059 | Loss: 174.2443\n",
      "Epoch: 405/513 Train Loss: 233.7145\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 406/513 | Batch 000/059 | Loss: 317.9345\n",
      "Epoch: 406/513 Train Loss: 234.2348\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 407/513 | Batch 000/059 | Loss: 307.4101\n",
      "Epoch: 407/513 Train Loss: 233.5893\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 408/513 | Batch 000/059 | Loss: 236.1839\n",
      "Epoch: 408/513 Train Loss: 233.4989\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 409/513 | Batch 000/059 | Loss: 207.4125\n",
      "Epoch: 409/513 Train Loss: 233.7103\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 410/513 | Batch 000/059 | Loss: 388.2737\n",
      "Epoch: 410/513 Train Loss: 233.3851\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 411/513 | Batch 000/059 | Loss: 315.2049\n",
      "Epoch: 411/513 Train Loss: 233.5350\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 412/513 | Batch 000/059 | Loss: 226.9180\n",
      "Epoch: 412/513 Train Loss: 233.4691\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 413/513 | Batch 000/059 | Loss: 343.9429\n",
      "Epoch: 413/513 Train Loss: 233.6723\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 414/513 | Batch 000/059 | Loss: 196.1622\n",
      "Epoch: 414/513 Train Loss: 233.1343\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 415/513 | Batch 000/059 | Loss: 261.9061\n",
      "Epoch: 415/513 Train Loss: 233.5319\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 416/513 | Batch 000/059 | Loss: 127.4312\n",
      "Epoch: 416/513 Train Loss: 233.2810\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 417/513 | Batch 000/059 | Loss: 273.8763\n",
      "Epoch: 417/513 Train Loss: 233.0328\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 418/513 | Batch 000/059 | Loss: 262.2634\n",
      "Epoch: 418/513 Train Loss: 233.2047\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 419/513 | Batch 000/059 | Loss: 353.6110\n",
      "Epoch: 419/513 Train Loss: 232.9313\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 420/513 | Batch 000/059 | Loss: 239.6169\n",
      "Epoch: 420/513 Train Loss: 233.1277\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 421/513 | Batch 000/059 | Loss: 228.0396\n",
      "Epoch: 421/513 Train Loss: 232.8670\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 422/513 | Batch 000/059 | Loss: 323.7157\n",
      "Epoch: 422/513 Train Loss: 232.8508\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 423/513 | Batch 000/059 | Loss: 390.7961\n",
      "Epoch: 423/513 Train Loss: 233.0841\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 424/513 | Batch 000/059 | Loss: 370.0473\n",
      "Epoch: 424/513 Train Loss: 232.7291\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 425/513 | Batch 000/059 | Loss: 238.9878\n",
      "Epoch: 425/513 Train Loss: 232.6819\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 426/513 | Batch 000/059 | Loss: 175.7085\n",
      "Epoch: 426/513 Train Loss: 232.6513\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 427/513 | Batch 000/059 | Loss: 283.1626\n",
      "Epoch: 427/513 Train Loss: 232.6709\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 428/513 | Batch 000/059 | Loss: 237.7284\n",
      "Epoch: 428/513 Train Loss: 232.7613\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 429/513 | Batch 000/059 | Loss: 228.4225\n",
      "Epoch: 429/513 Train Loss: 232.5597\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 430/513 | Batch 000/059 | Loss: 266.8408\n",
      "Epoch: 430/513 Train Loss: 232.6121\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 431/513 | Batch 000/059 | Loss: 404.7090\n",
      "Epoch: 431/513 Train Loss: 232.4978\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 432/513 | Batch 000/059 | Loss: 295.1632\n",
      "Epoch: 432/513 Train Loss: 233.0552\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 433/513 | Batch 000/059 | Loss: 309.2996\n",
      "Epoch: 433/513 Train Loss: 232.4039\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 434/513 | Batch 000/059 | Loss: 347.7907\n",
      "Epoch: 434/513 Train Loss: 232.4874\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 435/513 | Batch 000/059 | Loss: 310.2151\n",
      "Epoch: 435/513 Train Loss: 232.3243\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 436/513 | Batch 000/059 | Loss: 259.9477\n",
      "Epoch: 436/513 Train Loss: 232.2998\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 437/513 | Batch 000/059 | Loss: 245.3922\n",
      "Epoch: 437/513 Train Loss: 232.3823\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 438/513 | Batch 000/059 | Loss: 252.2382\n",
      "Epoch: 438/513 Train Loss: 232.2708\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 439/513 | Batch 000/059 | Loss: 237.7650\n",
      "Epoch: 439/513 Train Loss: 232.1200\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 440/513 | Batch 000/059 | Loss: 171.4358\n",
      "Epoch: 440/513 Train Loss: 232.0629\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 441/513 | Batch 000/059 | Loss: 163.5125\n",
      "Epoch: 441/513 Train Loss: 232.0568\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 442/513 | Batch 000/059 | Loss: 383.0200\n",
      "Epoch: 442/513 Train Loss: 232.0401\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 443/513 | Batch 000/059 | Loss: 217.7368\n",
      "Epoch: 443/513 Train Loss: 232.4468\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 444/513 | Batch 000/059 | Loss: 227.4894\n",
      "Epoch: 444/513 Train Loss: 232.2526\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 445/513 | Batch 000/059 | Loss: 394.8777\n",
      "Epoch: 445/513 Train Loss: 232.0488\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 446/513 | Batch 000/059 | Loss: 305.7296\n",
      "Epoch: 446/513 Train Loss: 231.9157\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 447/513 | Batch 000/059 | Loss: 158.9819\n",
      "Epoch: 447/513 Train Loss: 232.0190\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 448/513 | Batch 000/059 | Loss: 200.8083\n",
      "Epoch: 448/513 Train Loss: 232.4856\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 449/513 | Batch 000/059 | Loss: 238.5636\n",
      "Epoch: 449/513 Train Loss: 231.8847\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 450/513 | Batch 000/059 | Loss: 235.6684\n",
      "Epoch: 450/513 Train Loss: 231.8430\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 451/513 | Batch 000/059 | Loss: 218.9788\n",
      "Epoch: 451/513 Train Loss: 232.0280\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 452/513 | Batch 000/059 | Loss: 213.9057\n",
      "Epoch: 452/513 Train Loss: 231.7543\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 453/513 | Batch 000/059 | Loss: 165.0430\n",
      "Epoch: 453/513 Train Loss: 231.7080\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 454/513 | Batch 000/059 | Loss: 151.2687\n",
      "Epoch: 454/513 Train Loss: 231.6949\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 455/513 | Batch 000/059 | Loss: 264.1724\n",
      "Epoch: 455/513 Train Loss: 231.7168\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 456/513 | Batch 000/059 | Loss: 212.4001\n",
      "Epoch: 456/513 Train Loss: 231.5820\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 457/513 | Batch 000/059 | Loss: 238.3421\n",
      "Epoch: 457/513 Train Loss: 231.5781\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 458/513 | Batch 000/059 | Loss: 222.3940\n",
      "Epoch: 458/513 Train Loss: 231.6662\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 459/513 | Batch 000/059 | Loss: 221.8755\n",
      "Epoch: 459/513 Train Loss: 231.7397\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 460/513 | Batch 000/059 | Loss: 322.1633\n",
      "Epoch: 460/513 Train Loss: 231.6217\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 461/513 | Batch 000/059 | Loss: 342.1826\n",
      "Epoch: 461/513 Train Loss: 231.5798\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 462/513 | Batch 000/059 | Loss: 336.4937\n",
      "Epoch: 462/513 Train Loss: 231.4045\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 463/513 | Batch 000/059 | Loss: 415.9024\n",
      "Epoch: 463/513 Train Loss: 231.6822\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 464/513 | Batch 000/059 | Loss: 292.2167\n",
      "Epoch: 464/513 Train Loss: 231.4074\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 465/513 | Batch 000/059 | Loss: 281.3945\n",
      "Epoch: 465/513 Train Loss: 231.4823\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 466/513 | Batch 000/059 | Loss: 186.2335\n",
      "Epoch: 466/513 Train Loss: 231.3369\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 467/513 | Batch 000/059 | Loss: 227.3187\n",
      "Epoch: 467/513 Train Loss: 231.4041\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 468/513 | Batch 000/059 | Loss: 317.3319\n",
      "Epoch: 468/513 Train Loss: 231.3708\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 469/513 | Batch 000/059 | Loss: 263.9022\n",
      "Epoch: 469/513 Train Loss: 231.3718\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 470/513 | Batch 000/059 | Loss: 398.4041\n",
      "Epoch: 470/513 Train Loss: 231.3699\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 471/513 | Batch 000/059 | Loss: 180.5972\n",
      "Epoch: 471/513 Train Loss: 231.5627\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 472/513 | Batch 000/059 | Loss: 172.1896\n",
      "Epoch: 472/513 Train Loss: 231.3621\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 473/513 | Batch 000/059 | Loss: 221.2523\n",
      "Epoch: 473/513 Train Loss: 231.1427\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 474/513 | Batch 000/059 | Loss: 392.2911\n",
      "Epoch: 474/513 Train Loss: 231.1459\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 475/513 | Batch 000/059 | Loss: 240.2632\n",
      "Epoch: 475/513 Train Loss: 231.1612\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 476/513 | Batch 000/059 | Loss: 238.4472\n",
      "Epoch: 476/513 Train Loss: 231.0732\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 477/513 | Batch 000/059 | Loss: 297.6306\n",
      "Epoch: 477/513 Train Loss: 231.4592\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 478/513 | Batch 000/059 | Loss: 187.7628\n",
      "Epoch: 478/513 Train Loss: 231.3299\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 479/513 | Batch 000/059 | Loss: 410.9723\n",
      "Epoch: 479/513 Train Loss: 231.0444\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 480/513 | Batch 000/059 | Loss: 251.0007\n",
      "Epoch: 480/513 Train Loss: 230.9864\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 481/513 | Batch 000/059 | Loss: 340.4669\n",
      "Epoch: 481/513 Train Loss: 230.9941\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 482/513 | Batch 000/059 | Loss: 355.2758\n",
      "Epoch: 482/513 Train Loss: 230.9214\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 483/513 | Batch 000/059 | Loss: 230.1873\n",
      "Epoch: 483/513 Train Loss: 230.9716\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 484/513 | Batch 000/059 | Loss: 171.7140\n",
      "Epoch: 484/513 Train Loss: 230.8929\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 485/513 | Batch 000/059 | Loss: 397.8159\n",
      "Epoch: 485/513 Train Loss: 230.8322\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 486/513 | Batch 000/059 | Loss: 156.4717\n",
      "Epoch: 486/513 Train Loss: 230.7922\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 487/513 | Batch 000/059 | Loss: 320.4809\n",
      "Epoch: 487/513 Train Loss: 230.7817\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 488/513 | Batch 000/059 | Loss: 209.2126\n",
      "Epoch: 488/513 Train Loss: 230.9643\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 489/513 | Batch 000/059 | Loss: 272.2792\n",
      "Epoch: 489/513 Train Loss: 231.0895\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 490/513 | Batch 000/059 | Loss: 202.7148\n",
      "Epoch: 490/513 Train Loss: 230.7405\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 491/513 | Batch 000/059 | Loss: 172.0221\n",
      "Epoch: 491/513 Train Loss: 230.9235\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 492/513 | Batch 000/059 | Loss: 218.9367\n",
      "Epoch: 492/513 Train Loss: 230.7048\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 493/513 | Batch 000/059 | Loss: 390.3746\n",
      "Epoch: 493/513 Train Loss: 231.0058\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 494/513 | Batch 000/059 | Loss: 179.9237\n",
      "Epoch: 494/513 Train Loss: 230.6758\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 495/513 | Batch 000/059 | Loss: 123.5525\n",
      "Epoch: 495/513 Train Loss: 230.5968\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 496/513 | Batch 000/059 | Loss: 184.8537\n",
      "Epoch: 496/513 Train Loss: 230.9443\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 497/513 | Batch 000/059 | Loss: 207.2560\n",
      "Epoch: 497/513 Train Loss: 230.6196\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 498/513 | Batch 000/059 | Loss: 236.1091\n",
      "Epoch: 498/513 Train Loss: 230.5640\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 499/513 | Batch 000/059 | Loss: 298.3519\n",
      "Epoch: 499/513 Train Loss: 230.6078\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 500/513 | Batch 000/059 | Loss: 224.3662\n",
      "Epoch: 500/513 Train Loss: 230.5197\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 501/513 | Batch 000/059 | Loss: 178.3264\n",
      "Epoch: 501/513 Train Loss: 230.6682\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 502/513 | Batch 000/059 | Loss: 171.7844\n",
      "Epoch: 502/513 Train Loss: 230.4711\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 503/513 | Batch 000/059 | Loss: 155.0662\n",
      "Epoch: 503/513 Train Loss: 230.4649\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 504/513 | Batch 000/059 | Loss: 154.7605\n",
      "Epoch: 504/513 Train Loss: 230.4019\n",
      "Time elapsed: 1.19 min\n",
      "Epoch: 505/513 | Batch 000/059 | Loss: 94.4789\n",
      "Epoch: 505/513 Train Loss: 230.4331\n",
      "Time elapsed: 1.19 min\n",
      "Epoch: 506/513 | Batch 000/059 | Loss: 193.1805\n",
      "Epoch: 506/513 Train Loss: 230.3693\n",
      "Time elapsed: 1.19 min\n",
      "Epoch: 507/513 | Batch 000/059 | Loss: 207.9336\n",
      "Epoch: 507/513 Train Loss: 230.4364\n",
      "Time elapsed: 1.19 min\n",
      "Epoch: 508/513 | Batch 000/059 | Loss: 195.4388\n",
      "Epoch: 508/513 Train Loss: 230.3757\n",
      "Time elapsed: 1.19 min\n",
      "Epoch: 509/513 | Batch 000/059 | Loss: 537.2280\n",
      "Epoch: 509/513 Train Loss: 230.5668\n",
      "Time elapsed: 1.20 min\n",
      "Epoch: 510/513 | Batch 000/059 | Loss: 138.8526\n",
      "Epoch: 510/513 Train Loss: 230.2508\n",
      "Time elapsed: 1.20 min\n",
      "Epoch: 511/513 | Batch 000/059 | Loss: 239.8244\n",
      "Epoch: 511/513 Train Loss: 230.3693\n",
      "Time elapsed: 1.20 min\n",
      "Epoch: 512/513 | Batch 000/059 | Loss: 158.4980\n",
      "Epoch: 512/513 Train Loss: 230.3775\n",
      "Time elapsed: 1.20 min\n",
      "Epoch: 513/513 | Batch 000/059 | Loss: 350.9737\n",
      "Epoch: 513/513 Train Loss: 230.1850\n",
      "Time elapsed: 1.21 min\n",
      "Total Training Time: 1.21 min\n",
      "Training Loss: 230.19\n",
      "Test Loss: 236.81\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "minibatch_cost = []\n",
    "epoch_cost = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(device)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        outputs = model(features)\n",
    "        \n",
    "        cost = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        minibatch_cost.append(cost)\n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 1000:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "       \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        cost = compute_loss(model, train_loader)\n",
    "        epoch_cost.append(cost)\n",
    "        print('Epoch: %03d/%03d Train Loss: %.4f' % (\n",
    "                epoch+1, num_epochs, cost))\n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "        \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "    print('Training Loss: %.2f' % compute_loss(model, train_loader))\n",
    "    print('Test Loss: %.2f' % compute_loss(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "BeGlxXnlsxwc",
    "outputId": "72c63552-6da9-4460-c0b8-3d21e30855db"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the loss curve\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(len(epoch_cost)), epoch_cost)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross entropy loss\")\n",
    "plt.title(\"Loss of training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PALWpQuHO9oh"
   },
   "source": [
    "## Batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kp-_zqf9uql4"
   },
   "source": [
    "### Model B: 1 Hidden Layer Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DIRGuLvuql5"
   },
   "source": [
    "### Create model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KtAh951Euql5",
    "outputId": "36e96712-dbaf-4972-f49b-bd85abb21cb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "n_iters = 30000\n",
    "num_epochs = n_iters / (len(train_data) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "print(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "id": "ZvhOYRW5uql9"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # create your dataloader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "id": "qYqlIgGFuql9"
   },
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, output_dim):\n",
    "        super(FNN, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.BatchNorm1d1=nn.BatchNorm1d(hidden_dim1)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear4 = nn.Linear(hidden_dim1, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.BatchNorm1d1(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.linear4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "id": "4VWAGuS9uql9"
   },
   "outputs": [],
   "source": [
    "input_dim = tensor_x.shape[1]\n",
    "output_dim = 1\n",
    "hidden_dim1 = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "id": "fTPt4OJMuql9"
   },
   "outputs": [],
   "source": [
    "model = FNN(input_dim, hidden_dim1, output_dim)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "id": "Gem5Jjliuql9"
   },
   "outputs": [],
   "source": [
    "#summary(model, (1,tensor_x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "id": "BlIlKGGzuql-"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 0.0001\n",
    "LAMBDA = 0.05\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate, weight_decay=LAMBDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpmpkOPbuql-"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S7AJxMHquql-",
    "outputId": "a66cc0ae-c593-423f-f7d6-86e34850df23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/513 | Batch 000/059 | Loss: 6294.1729\n",
      "Epoch: 001/513 Train Loss: 6434.2974\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 002/513 | Batch 000/059 | Loss: 6542.1777\n",
      "Epoch: 002/513 Train Loss: 5836.4483\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 003/513 | Batch 000/059 | Loss: 4830.3838\n",
      "Epoch: 003/513 Train Loss: 5350.1724\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 004/513 | Batch 000/059 | Loss: 5977.6367\n",
      "Epoch: 004/513 Train Loss: 4903.2834\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 005/513 | Batch 000/059 | Loss: 5142.8252\n",
      "Epoch: 005/513 Train Loss: 4485.1183\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 006/513 | Batch 000/059 | Loss: 5483.1250\n",
      "Epoch: 006/513 Train Loss: 4060.1115\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 007/513 | Batch 000/059 | Loss: 5114.7256\n",
      "Epoch: 007/513 Train Loss: 3647.1382\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 008/513 | Batch 000/059 | Loss: 3441.8293\n",
      "Epoch: 008/513 Train Loss: 3307.5264\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 009/513 | Batch 000/059 | Loss: 3131.9814\n",
      "Epoch: 009/513 Train Loss: 2975.7619\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 010/513 | Batch 000/059 | Loss: 3263.2690\n",
      "Epoch: 010/513 Train Loss: 2540.6409\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 011/513 | Batch 000/059 | Loss: 2061.2007\n",
      "Epoch: 011/513 Train Loss: 2334.2449\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 012/513 | Batch 000/059 | Loss: 1971.0760\n",
      "Epoch: 012/513 Train Loss: 2061.9124\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 013/513 | Batch 000/059 | Loss: 2329.1230\n",
      "Epoch: 013/513 Train Loss: 1751.2746\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 014/513 | Batch 000/059 | Loss: 1880.2734\n",
      "Epoch: 014/513 Train Loss: 1545.8746\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 015/513 | Batch 000/059 | Loss: 1432.8698\n",
      "Epoch: 015/513 Train Loss: 1302.9771\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 016/513 | Batch 000/059 | Loss: 1324.2255\n",
      "Epoch: 016/513 Train Loss: 1181.3691\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 017/513 | Batch 000/059 | Loss: 1168.8278\n",
      "Epoch: 017/513 Train Loss: 1105.4070\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 018/513 | Batch 000/059 | Loss: 1132.3862\n",
      "Epoch: 018/513 Train Loss: 913.9122\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 019/513 | Batch 000/059 | Loss: 882.4698\n",
      "Epoch: 019/513 Train Loss: 800.6783\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 020/513 | Batch 000/059 | Loss: 733.8258\n",
      "Epoch: 020/513 Train Loss: 697.0619\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 021/513 | Batch 000/059 | Loss: 688.4844\n",
      "Epoch: 021/513 Train Loss: 609.4775\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 022/513 | Batch 000/059 | Loss: 744.1129\n",
      "Epoch: 022/513 Train Loss: 566.5561\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 023/513 | Batch 000/059 | Loss: 356.3466\n",
      "Epoch: 023/513 Train Loss: 522.2995\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 024/513 | Batch 000/059 | Loss: 441.9508\n",
      "Epoch: 024/513 Train Loss: 470.8094\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 025/513 | Batch 000/059 | Loss: 492.2840\n",
      "Epoch: 025/513 Train Loss: 434.0146\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 026/513 | Batch 000/059 | Loss: 374.7024\n",
      "Epoch: 026/513 Train Loss: 428.3588\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 027/513 | Batch 000/059 | Loss: 482.0120\n",
      "Epoch: 027/513 Train Loss: 393.9922\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 028/513 | Batch 000/059 | Loss: 597.3549\n",
      "Epoch: 028/513 Train Loss: 381.4612\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 029/513 | Batch 000/059 | Loss: 396.9772\n",
      "Epoch: 029/513 Train Loss: 368.7220\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 030/513 | Batch 000/059 | Loss: 489.6411\n",
      "Epoch: 030/513 Train Loss: 361.8181\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 031/513 | Batch 000/059 | Loss: 286.3745\n",
      "Epoch: 031/513 Train Loss: 350.2813\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 032/513 | Batch 000/059 | Loss: 277.1463\n",
      "Epoch: 032/513 Train Loss: 343.4010\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 033/513 | Batch 000/059 | Loss: 404.8578\n",
      "Epoch: 033/513 Train Loss: 341.6249\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 034/513 | Batch 000/059 | Loss: 524.9916\n",
      "Epoch: 034/513 Train Loss: 331.3868\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 035/513 | Batch 000/059 | Loss: 817.2982\n",
      "Epoch: 035/513 Train Loss: 329.2738\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 036/513 | Batch 000/059 | Loss: 208.6765\n",
      "Epoch: 036/513 Train Loss: 323.3480\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 037/513 | Batch 000/059 | Loss: 531.0262\n",
      "Epoch: 037/513 Train Loss: 319.7237\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 038/513 | Batch 000/059 | Loss: 216.6039\n",
      "Epoch: 038/513 Train Loss: 314.7265\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 039/513 | Batch 000/059 | Loss: 351.8355\n",
      "Epoch: 039/513 Train Loss: 309.7882\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 040/513 | Batch 000/059 | Loss: 386.7333\n",
      "Epoch: 040/513 Train Loss: 306.7244\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 041/513 | Batch 000/059 | Loss: 333.8347\n",
      "Epoch: 041/513 Train Loss: 306.0194\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 042/513 | Batch 000/059 | Loss: 347.1029\n",
      "Epoch: 042/513 Train Loss: 301.4264\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 043/513 | Batch 000/059 | Loss: 320.7694\n",
      "Epoch: 043/513 Train Loss: 300.5036\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 044/513 | Batch 000/059 | Loss: 214.9332\n",
      "Epoch: 044/513 Train Loss: 293.2462\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 045/513 | Batch 000/059 | Loss: 340.7196\n",
      "Epoch: 045/513 Train Loss: 292.3042\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 046/513 | Batch 000/059 | Loss: 231.5871\n",
      "Epoch: 046/513 Train Loss: 294.8224\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 047/513 | Batch 000/059 | Loss: 403.0825\n",
      "Epoch: 047/513 Train Loss: 288.1838\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 048/513 | Batch 000/059 | Loss: 180.9901\n",
      "Epoch: 048/513 Train Loss: 283.1399\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 049/513 | Batch 000/059 | Loss: 571.2328\n",
      "Epoch: 049/513 Train Loss: 280.4105\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 050/513 | Batch 000/059 | Loss: 395.3933\n",
      "Epoch: 050/513 Train Loss: 278.9423\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 051/513 | Batch 000/059 | Loss: 220.0055\n",
      "Epoch: 051/513 Train Loss: 280.0240\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 052/513 | Batch 000/059 | Loss: 624.0513\n",
      "Epoch: 052/513 Train Loss: 277.1735\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 053/513 | Batch 000/059 | Loss: 365.7529\n",
      "Epoch: 053/513 Train Loss: 272.5772\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 054/513 | Batch 000/059 | Loss: 350.3398\n",
      "Epoch: 054/513 Train Loss: 274.3614\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 055/513 | Batch 000/059 | Loss: 305.0475\n",
      "Epoch: 055/513 Train Loss: 269.4300\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 056/513 | Batch 000/059 | Loss: 210.8202\n",
      "Epoch: 056/513 Train Loss: 267.2422\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 057/513 | Batch 000/059 | Loss: 205.1300\n",
      "Epoch: 057/513 Train Loss: 271.6821\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 058/513 | Batch 000/059 | Loss: 352.7620\n",
      "Epoch: 058/513 Train Loss: 265.2539\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 059/513 | Batch 000/059 | Loss: 247.7913\n",
      "Epoch: 059/513 Train Loss: 265.7812\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 060/513 | Batch 000/059 | Loss: 372.9858\n",
      "Epoch: 060/513 Train Loss: 266.0740\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 061/513 | Batch 000/059 | Loss: 394.4240\n",
      "Epoch: 061/513 Train Loss: 260.9390\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 062/513 | Batch 000/059 | Loss: 306.0654\n",
      "Epoch: 062/513 Train Loss: 257.7958\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 063/513 | Batch 000/059 | Loss: 312.2925\n",
      "Epoch: 063/513 Train Loss: 261.9429\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 064/513 | Batch 000/059 | Loss: 273.7334\n",
      "Epoch: 064/513 Train Loss: 269.2298\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 065/513 | Batch 000/059 | Loss: 247.5602\n",
      "Epoch: 065/513 Train Loss: 262.7352\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 066/513 | Batch 000/059 | Loss: 219.1658\n",
      "Epoch: 066/513 Train Loss: 254.1190\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 067/513 | Batch 000/059 | Loss: 506.4037\n",
      "Epoch: 067/513 Train Loss: 251.2006\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 068/513 | Batch 000/059 | Loss: 612.6587\n",
      "Epoch: 068/513 Train Loss: 248.7874\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 069/513 | Batch 000/059 | Loss: 272.3296\n",
      "Epoch: 069/513 Train Loss: 251.2717\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 070/513 | Batch 000/059 | Loss: 289.9814\n",
      "Epoch: 070/513 Train Loss: 251.1931\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 071/513 | Batch 000/059 | Loss: 568.3354\n",
      "Epoch: 071/513 Train Loss: 249.8488\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 072/513 | Batch 000/059 | Loss: 219.3923\n",
      "Epoch: 072/513 Train Loss: 249.0714\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 073/513 | Batch 000/059 | Loss: 354.4408\n",
      "Epoch: 073/513 Train Loss: 245.4412\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 074/513 | Batch 000/059 | Loss: 257.5172\n",
      "Epoch: 074/513 Train Loss: 244.6903\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 075/513 | Batch 000/059 | Loss: 291.3349\n",
      "Epoch: 075/513 Train Loss: 247.5389\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 076/513 | Batch 000/059 | Loss: 304.4103\n",
      "Epoch: 076/513 Train Loss: 242.9875\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 077/513 | Batch 000/059 | Loss: 404.8036\n",
      "Epoch: 077/513 Train Loss: 247.5436\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 078/513 | Batch 000/059 | Loss: 238.5350\n",
      "Epoch: 078/513 Train Loss: 240.5506\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 079/513 | Batch 000/059 | Loss: 355.7429\n",
      "Epoch: 079/513 Train Loss: 244.0637\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 080/513 | Batch 000/059 | Loss: 414.4843\n",
      "Epoch: 080/513 Train Loss: 241.7794\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 081/513 | Batch 000/059 | Loss: 332.2413\n",
      "Epoch: 081/513 Train Loss: 240.3487\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 082/513 | Batch 000/059 | Loss: 322.2323\n",
      "Epoch: 082/513 Train Loss: 238.4710\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 083/513 | Batch 000/059 | Loss: 351.2380\n",
      "Epoch: 083/513 Train Loss: 237.2296\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 084/513 | Batch 000/059 | Loss: 281.8825\n",
      "Epoch: 084/513 Train Loss: 238.3308\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 085/513 | Batch 000/059 | Loss: 306.9554\n",
      "Epoch: 085/513 Train Loss: 234.6442\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 086/513 | Batch 000/059 | Loss: 123.8361\n",
      "Epoch: 086/513 Train Loss: 235.1842\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 087/513 | Batch 000/059 | Loss: 213.5677\n",
      "Epoch: 087/513 Train Loss: 238.7329\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 088/513 | Batch 000/059 | Loss: 228.1027\n",
      "Epoch: 088/513 Train Loss: 236.0931\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 089/513 | Batch 000/059 | Loss: 333.1554\n",
      "Epoch: 089/513 Train Loss: 241.0271\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 090/513 | Batch 000/059 | Loss: 172.1494\n",
      "Epoch: 090/513 Train Loss: 234.5930\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 091/513 | Batch 000/059 | Loss: 412.9994\n",
      "Epoch: 091/513 Train Loss: 231.3402\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 092/513 | Batch 000/059 | Loss: 227.0005\n",
      "Epoch: 092/513 Train Loss: 235.0555\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 093/513 | Batch 000/059 | Loss: 230.0035\n",
      "Epoch: 093/513 Train Loss: 232.2134\n",
      "Time elapsed: 0.30 min\n",
      "Epoch: 094/513 | Batch 000/059 | Loss: 173.7240\n",
      "Epoch: 094/513 Train Loss: 230.9309\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 095/513 | Batch 000/059 | Loss: 323.1603\n",
      "Epoch: 095/513 Train Loss: 232.4543\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 096/513 | Batch 000/059 | Loss: 224.6091\n",
      "Epoch: 096/513 Train Loss: 229.9880\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 097/513 | Batch 000/059 | Loss: 218.4922\n",
      "Epoch: 097/513 Train Loss: 232.5066\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 098/513 | Batch 000/059 | Loss: 411.8697\n",
      "Epoch: 098/513 Train Loss: 232.7660\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 099/513 | Batch 000/059 | Loss: 365.4948\n",
      "Epoch: 099/513 Train Loss: 227.1206\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 100/513 | Batch 000/059 | Loss: 233.3314\n",
      "Epoch: 100/513 Train Loss: 229.3269\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 101/513 | Batch 000/059 | Loss: 302.0177\n",
      "Epoch: 101/513 Train Loss: 226.9406\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 102/513 | Batch 000/059 | Loss: 626.0420\n",
      "Epoch: 102/513 Train Loss: 230.4705\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 103/513 | Batch 000/059 | Loss: 350.8278\n",
      "Epoch: 103/513 Train Loss: 227.2011\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 104/513 | Batch 000/059 | Loss: 270.2331\n",
      "Epoch: 104/513 Train Loss: 226.3779\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 105/513 | Batch 000/059 | Loss: 204.9591\n",
      "Epoch: 105/513 Train Loss: 226.5904\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 106/513 | Batch 000/059 | Loss: 270.1377\n",
      "Epoch: 106/513 Train Loss: 228.4323\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 107/513 | Batch 000/059 | Loss: 251.7846\n",
      "Epoch: 107/513 Train Loss: 226.2406\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 108/513 | Batch 000/059 | Loss: 317.9399\n",
      "Epoch: 108/513 Train Loss: 225.0611\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 109/513 | Batch 000/059 | Loss: 322.2099\n",
      "Epoch: 109/513 Train Loss: 229.0196\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 110/513 | Batch 000/059 | Loss: 294.0375\n",
      "Epoch: 110/513 Train Loss: 228.2770\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 111/513 | Batch 000/059 | Loss: 222.7406\n",
      "Epoch: 111/513 Train Loss: 226.6204\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 112/513 | Batch 000/059 | Loss: 353.2734\n",
      "Epoch: 112/513 Train Loss: 224.4858\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 113/513 | Batch 000/059 | Loss: 222.9529\n",
      "Epoch: 113/513 Train Loss: 227.3058\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 114/513 | Batch 000/059 | Loss: 348.1159\n",
      "Epoch: 114/513 Train Loss: 224.5992\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 115/513 | Batch 000/059 | Loss: 135.3899\n",
      "Epoch: 115/513 Train Loss: 226.8801\n",
      "Time elapsed: 0.37 min\n",
      "Epoch: 116/513 | Batch 000/059 | Loss: 463.3047\n",
      "Epoch: 116/513 Train Loss: 225.2269\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 117/513 | Batch 000/059 | Loss: 306.8080\n",
      "Epoch: 117/513 Train Loss: 226.4204\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 118/513 | Batch 000/059 | Loss: 164.2702\n",
      "Epoch: 118/513 Train Loss: 226.1754\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 119/513 | Batch 000/059 | Loss: 453.5334\n",
      "Epoch: 119/513 Train Loss: 224.0677\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 120/513 | Batch 000/059 | Loss: 386.1416\n",
      "Epoch: 120/513 Train Loss: 224.5461\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 121/513 | Batch 000/059 | Loss: 207.5043\n",
      "Epoch: 121/513 Train Loss: 229.8018\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 122/513 | Batch 000/059 | Loss: 363.4549\n",
      "Epoch: 122/513 Train Loss: 224.3494\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 123/513 | Batch 000/059 | Loss: 275.3315\n",
      "Epoch: 123/513 Train Loss: 223.4834\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 124/513 | Batch 000/059 | Loss: 290.6134\n",
      "Epoch: 124/513 Train Loss: 224.3756\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 125/513 | Batch 000/059 | Loss: 193.4129\n",
      "Epoch: 125/513 Train Loss: 225.3726\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 126/513 | Batch 000/059 | Loss: 178.5266\n",
      "Epoch: 126/513 Train Loss: 223.6773\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 127/513 | Batch 000/059 | Loss: 220.3377\n",
      "Epoch: 127/513 Train Loss: 222.3585\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 128/513 | Batch 000/059 | Loss: 188.7373\n",
      "Epoch: 128/513 Train Loss: 222.2342\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 129/513 | Batch 000/059 | Loss: 369.8630\n",
      "Epoch: 129/513 Train Loss: 222.8747\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 130/513 | Batch 000/059 | Loss: 383.7773\n",
      "Epoch: 130/513 Train Loss: 229.3138\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 131/513 | Batch 000/059 | Loss: 282.8848\n",
      "Epoch: 131/513 Train Loss: 221.1992\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 132/513 | Batch 000/059 | Loss: 355.7535\n",
      "Epoch: 132/513 Train Loss: 224.5339\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 133/513 | Batch 000/059 | Loss: 314.3751\n",
      "Epoch: 133/513 Train Loss: 220.9795\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 134/513 | Batch 000/059 | Loss: 211.1016\n",
      "Epoch: 134/513 Train Loss: 221.7523\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 135/513 | Batch 000/059 | Loss: 201.6239\n",
      "Epoch: 135/513 Train Loss: 223.9034\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 136/513 | Batch 000/059 | Loss: 190.5917\n",
      "Epoch: 136/513 Train Loss: 221.2238\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 137/513 | Batch 000/059 | Loss: 180.0945\n",
      "Epoch: 137/513 Train Loss: 227.9849\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 138/513 | Batch 000/059 | Loss: 231.0431\n",
      "Epoch: 138/513 Train Loss: 222.1408\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 139/513 | Batch 000/059 | Loss: 269.3025\n",
      "Epoch: 139/513 Train Loss: 220.6346\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 140/513 | Batch 000/059 | Loss: 257.5506\n",
      "Epoch: 140/513 Train Loss: 220.5883\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 141/513 | Batch 000/059 | Loss: 260.1750\n",
      "Epoch: 141/513 Train Loss: 224.0823\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 142/513 | Batch 000/059 | Loss: 178.9659\n",
      "Epoch: 142/513 Train Loss: 220.5719\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 143/513 | Batch 000/059 | Loss: 311.5248\n",
      "Epoch: 143/513 Train Loss: 222.1917\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 144/513 | Batch 000/059 | Loss: 250.3837\n",
      "Epoch: 144/513 Train Loss: 222.0765\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 145/513 | Batch 000/059 | Loss: 678.4329\n",
      "Epoch: 145/513 Train Loss: 221.1889\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 146/513 | Batch 000/059 | Loss: 416.4383\n",
      "Epoch: 146/513 Train Loss: 219.8159\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 147/513 | Batch 000/059 | Loss: 343.7619\n",
      "Epoch: 147/513 Train Loss: 220.0023\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 148/513 | Batch 000/059 | Loss: 426.8974\n",
      "Epoch: 148/513 Train Loss: 224.0149\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 149/513 | Batch 000/059 | Loss: 412.4862\n",
      "Epoch: 149/513 Train Loss: 220.7911\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 150/513 | Batch 000/059 | Loss: 197.5793\n",
      "Epoch: 150/513 Train Loss: 223.5304\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 151/513 | Batch 000/059 | Loss: 382.4750\n",
      "Epoch: 151/513 Train Loss: 220.2812\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 152/513 | Batch 000/059 | Loss: 294.0638\n",
      "Epoch: 152/513 Train Loss: 221.0269\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 153/513 | Batch 000/059 | Loss: 309.4044\n",
      "Epoch: 153/513 Train Loss: 220.9721\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 154/513 | Batch 000/059 | Loss: 389.7043\n",
      "Epoch: 154/513 Train Loss: 218.7886\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 155/513 | Batch 000/059 | Loss: 240.9982\n",
      "Epoch: 155/513 Train Loss: 220.0166\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 156/513 | Batch 000/059 | Loss: 231.3276\n",
      "Epoch: 156/513 Train Loss: 219.2956\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 157/513 | Batch 000/059 | Loss: 215.1230\n",
      "Epoch: 157/513 Train Loss: 218.9826\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 158/513 | Batch 000/059 | Loss: 296.3296\n",
      "Epoch: 158/513 Train Loss: 220.7752\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 159/513 | Batch 000/059 | Loss: 184.0421\n",
      "Epoch: 159/513 Train Loss: 220.4977\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 160/513 | Batch 000/059 | Loss: 304.6192\n",
      "Epoch: 160/513 Train Loss: 219.0502\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 161/513 | Batch 000/059 | Loss: 395.0915\n",
      "Epoch: 161/513 Train Loss: 221.8314\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 162/513 | Batch 000/059 | Loss: 424.0166\n",
      "Epoch: 162/513 Train Loss: 219.0284\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 163/513 | Batch 000/059 | Loss: 496.3229\n",
      "Epoch: 163/513 Train Loss: 220.3190\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 164/513 | Batch 000/059 | Loss: 202.7162\n",
      "Epoch: 164/513 Train Loss: 218.8134\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 165/513 | Batch 000/059 | Loss: 155.9055\n",
      "Epoch: 165/513 Train Loss: 219.6837\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 166/513 | Batch 000/059 | Loss: 150.4538\n",
      "Epoch: 166/513 Train Loss: 221.8296\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 167/513 | Batch 000/059 | Loss: 309.6708\n",
      "Epoch: 167/513 Train Loss: 221.0067\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 168/513 | Batch 000/059 | Loss: 238.7505\n",
      "Epoch: 168/513 Train Loss: 218.4852\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 169/513 | Batch 000/059 | Loss: 342.2393\n",
      "Epoch: 169/513 Train Loss: 218.2595\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 170/513 | Batch 000/059 | Loss: 320.4355\n",
      "Epoch: 170/513 Train Loss: 224.3380\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 171/513 | Batch 000/059 | Loss: 281.9882\n",
      "Epoch: 171/513 Train Loss: 226.1418\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 172/513 | Batch 000/059 | Loss: 178.7066\n",
      "Epoch: 172/513 Train Loss: 216.8869\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 173/513 | Batch 000/059 | Loss: 260.0490\n",
      "Epoch: 173/513 Train Loss: 218.6758\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 174/513 | Batch 000/059 | Loss: 444.8488\n",
      "Epoch: 174/513 Train Loss: 217.6038\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 175/513 | Batch 000/059 | Loss: 338.9105\n",
      "Epoch: 175/513 Train Loss: 217.6019\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 176/513 | Batch 000/059 | Loss: 193.6562\n",
      "Epoch: 176/513 Train Loss: 219.3020\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 177/513 | Batch 000/059 | Loss: 191.4564\n",
      "Epoch: 177/513 Train Loss: 216.9822\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 178/513 | Batch 000/059 | Loss: 187.1034\n",
      "Epoch: 178/513 Train Loss: 218.9801\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 179/513 | Batch 000/059 | Loss: 328.4328\n",
      "Epoch: 179/513 Train Loss: 218.4516\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 180/513 | Batch 000/059 | Loss: 253.6867\n",
      "Epoch: 180/513 Train Loss: 221.2018\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 181/513 | Batch 000/059 | Loss: 222.6572\n",
      "Epoch: 181/513 Train Loss: 217.5527\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 182/513 | Batch 000/059 | Loss: 215.9695\n",
      "Epoch: 182/513 Train Loss: 219.0140\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 183/513 | Batch 000/059 | Loss: 139.0182\n",
      "Epoch: 183/513 Train Loss: 217.9751\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 184/513 | Batch 000/059 | Loss: 229.8725\n",
      "Epoch: 184/513 Train Loss: 217.1544\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 185/513 | Batch 000/059 | Loss: 310.3867\n",
      "Epoch: 185/513 Train Loss: 217.2310\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 186/513 | Batch 000/059 | Loss: 171.9193\n",
      "Epoch: 186/513 Train Loss: 217.8844\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 187/513 | Batch 000/059 | Loss: 296.5559\n",
      "Epoch: 187/513 Train Loss: 218.2479\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 188/513 | Batch 000/059 | Loss: 315.9132\n",
      "Epoch: 188/513 Train Loss: 219.5698\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 189/513 | Batch 000/059 | Loss: 345.6964\n",
      "Epoch: 189/513 Train Loss: 219.0189\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 190/513 | Batch 000/059 | Loss: 254.2156\n",
      "Epoch: 190/513 Train Loss: 217.2401\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 191/513 | Batch 000/059 | Loss: 270.1591\n",
      "Epoch: 191/513 Train Loss: 218.4805\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 192/513 | Batch 000/059 | Loss: 167.9965\n",
      "Epoch: 192/513 Train Loss: 217.8510\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 193/513 | Batch 000/059 | Loss: 203.6339\n",
      "Epoch: 193/513 Train Loss: 218.6824\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 194/513 | Batch 000/059 | Loss: 269.0680\n",
      "Epoch: 194/513 Train Loss: 223.7194\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 195/513 | Batch 000/059 | Loss: 259.7423\n",
      "Epoch: 195/513 Train Loss: 216.5248\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 196/513 | Batch 000/059 | Loss: 226.4569\n",
      "Epoch: 196/513 Train Loss: 217.2231\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 197/513 | Batch 000/059 | Loss: 200.1265\n",
      "Epoch: 197/513 Train Loss: 219.3704\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 198/513 | Batch 000/059 | Loss: 213.3457\n",
      "Epoch: 198/513 Train Loss: 216.0710\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 199/513 | Batch 000/059 | Loss: 184.0533\n",
      "Epoch: 199/513 Train Loss: 218.2741\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 200/513 | Batch 000/059 | Loss: 215.6461\n",
      "Epoch: 200/513 Train Loss: 215.7886\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 201/513 | Batch 000/059 | Loss: 326.2858\n",
      "Epoch: 201/513 Train Loss: 216.7573\n",
      "Time elapsed: 0.65 min\n",
      "Epoch: 202/513 | Batch 000/059 | Loss: 217.7598\n",
      "Epoch: 202/513 Train Loss: 215.9604\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 203/513 | Batch 000/059 | Loss: 338.5157\n",
      "Epoch: 203/513 Train Loss: 217.4988\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 204/513 | Batch 000/059 | Loss: 343.6577\n",
      "Epoch: 204/513 Train Loss: 216.5645\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 205/513 | Batch 000/059 | Loss: 259.5763\n",
      "Epoch: 205/513 Train Loss: 218.9182\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 206/513 | Batch 000/059 | Loss: 302.2289\n",
      "Epoch: 206/513 Train Loss: 220.1754\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 207/513 | Batch 000/059 | Loss: 401.6722\n",
      "Epoch: 207/513 Train Loss: 216.8214\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 208/513 | Batch 000/059 | Loss: 220.6847\n",
      "Epoch: 208/513 Train Loss: 217.0401\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 209/513 | Batch 000/059 | Loss: 269.6636\n",
      "Epoch: 209/513 Train Loss: 215.2149\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 210/513 | Batch 000/059 | Loss: 260.3735\n",
      "Epoch: 210/513 Train Loss: 215.3282\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 211/513 | Batch 000/059 | Loss: 436.0410\n",
      "Epoch: 211/513 Train Loss: 215.9258\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 212/513 | Batch 000/059 | Loss: 258.7944\n",
      "Epoch: 212/513 Train Loss: 220.2505\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 213/513 | Batch 000/059 | Loss: 242.9046\n",
      "Epoch: 213/513 Train Loss: 216.1675\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 214/513 | Batch 000/059 | Loss: 289.1397\n",
      "Epoch: 214/513 Train Loss: 216.7733\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 215/513 | Batch 000/059 | Loss: 163.4886\n",
      "Epoch: 215/513 Train Loss: 215.3922\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 216/513 | Batch 000/059 | Loss: 307.8367\n",
      "Epoch: 216/513 Train Loss: 216.5297\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 217/513 | Batch 000/059 | Loss: 327.7842\n",
      "Epoch: 217/513 Train Loss: 215.3734\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 218/513 | Batch 000/059 | Loss: 359.7983\n",
      "Epoch: 218/513 Train Loss: 216.9946\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 219/513 | Batch 000/059 | Loss: 245.5076\n",
      "Epoch: 219/513 Train Loss: 215.7001\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 220/513 | Batch 000/059 | Loss: 198.5817\n",
      "Epoch: 220/513 Train Loss: 216.6105\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 221/513 | Batch 000/059 | Loss: 252.2249\n",
      "Epoch: 221/513 Train Loss: 215.9617\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 222/513 | Batch 000/059 | Loss: 274.8403\n",
      "Epoch: 222/513 Train Loss: 216.6818\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 223/513 | Batch 000/059 | Loss: 350.1709\n",
      "Epoch: 223/513 Train Loss: 215.2359\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 224/513 | Batch 000/059 | Loss: 324.2744\n",
      "Epoch: 224/513 Train Loss: 215.4018\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 225/513 | Batch 000/059 | Loss: 224.0124\n",
      "Epoch: 225/513 Train Loss: 215.0830\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 226/513 | Batch 000/059 | Loss: 230.3023\n",
      "Epoch: 226/513 Train Loss: 220.6580\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 227/513 | Batch 000/059 | Loss: 188.3515\n",
      "Epoch: 227/513 Train Loss: 215.7560\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 228/513 | Batch 000/059 | Loss: 300.4364\n",
      "Epoch: 228/513 Train Loss: 215.9220\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 229/513 | Batch 000/059 | Loss: 316.6653\n",
      "Epoch: 229/513 Train Loss: 214.7588\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 230/513 | Batch 000/059 | Loss: 154.7683\n",
      "Epoch: 230/513 Train Loss: 215.8909\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 231/513 | Batch 000/059 | Loss: 205.3025\n",
      "Epoch: 231/513 Train Loss: 220.9830\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 232/513 | Batch 000/059 | Loss: 676.5435\n",
      "Epoch: 232/513 Train Loss: 217.5088\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 233/513 | Batch 000/059 | Loss: 239.7327\n",
      "Epoch: 233/513 Train Loss: 214.7988\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 234/513 | Batch 000/059 | Loss: 367.4732\n",
      "Epoch: 234/513 Train Loss: 215.8132\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 235/513 | Batch 000/059 | Loss: 384.8560\n",
      "Epoch: 235/513 Train Loss: 216.0303\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 236/513 | Batch 000/059 | Loss: 345.1061\n",
      "Epoch: 236/513 Train Loss: 217.8349\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 237/513 | Batch 000/059 | Loss: 302.4544\n",
      "Epoch: 237/513 Train Loss: 216.5188\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 238/513 | Batch 000/059 | Loss: 328.4247\n",
      "Epoch: 238/513 Train Loss: 215.5623\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 239/513 | Batch 000/059 | Loss: 243.4370\n",
      "Epoch: 239/513 Train Loss: 216.5602\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 240/513 | Batch 000/059 | Loss: 485.1905\n",
      "Epoch: 240/513 Train Loss: 216.1624\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 241/513 | Batch 000/059 | Loss: 239.4838\n",
      "Epoch: 241/513 Train Loss: 215.9808\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 242/513 | Batch 000/059 | Loss: 233.7556\n",
      "Epoch: 242/513 Train Loss: 215.1950\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 243/513 | Batch 000/059 | Loss: 352.1449\n",
      "Epoch: 243/513 Train Loss: 215.2290\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 244/513 | Batch 000/059 | Loss: 194.4959\n",
      "Epoch: 244/513 Train Loss: 216.5064\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 245/513 | Batch 000/059 | Loss: 225.9254\n",
      "Epoch: 245/513 Train Loss: 214.3892\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 246/513 | Batch 000/059 | Loss: 176.0935\n",
      "Epoch: 246/513 Train Loss: 217.9124\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 247/513 | Batch 000/059 | Loss: 287.2756\n",
      "Epoch: 247/513 Train Loss: 214.0388\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 248/513 | Batch 000/059 | Loss: 366.0382\n",
      "Epoch: 248/513 Train Loss: 217.2616\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 249/513 | Batch 000/059 | Loss: 249.8155\n",
      "Epoch: 249/513 Train Loss: 216.0035\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 250/513 | Batch 000/059 | Loss: 294.6803\n",
      "Epoch: 250/513 Train Loss: 215.7532\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 251/513 | Batch 000/059 | Loss: 403.7577\n",
      "Epoch: 251/513 Train Loss: 216.2289\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 252/513 | Batch 000/059 | Loss: 257.4578\n",
      "Epoch: 252/513 Train Loss: 214.1764\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 253/513 | Batch 000/059 | Loss: 289.7980\n",
      "Epoch: 253/513 Train Loss: 213.7233\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 254/513 | Batch 000/059 | Loss: 229.8715\n",
      "Epoch: 254/513 Train Loss: 214.5086\n",
      "Time elapsed: 0.82 min\n",
      "Epoch: 255/513 | Batch 000/059 | Loss: 338.5201\n",
      "Epoch: 255/513 Train Loss: 214.5101\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 256/513 | Batch 000/059 | Loss: 263.0644\n",
      "Epoch: 256/513 Train Loss: 224.2559\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 257/513 | Batch 000/059 | Loss: 157.8161\n",
      "Epoch: 257/513 Train Loss: 216.3168\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 258/513 | Batch 000/059 | Loss: 224.5118\n",
      "Epoch: 258/513 Train Loss: 214.6899\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 259/513 | Batch 000/059 | Loss: 319.7328\n",
      "Epoch: 259/513 Train Loss: 217.1531\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 260/513 | Batch 000/059 | Loss: 281.6940\n",
      "Epoch: 260/513 Train Loss: 216.2419\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 261/513 | Batch 000/059 | Loss: 345.2083\n",
      "Epoch: 261/513 Train Loss: 214.6892\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 262/513 | Batch 000/059 | Loss: 255.3998\n",
      "Epoch: 262/513 Train Loss: 213.8037\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 263/513 | Batch 000/059 | Loss: 278.3650\n",
      "Epoch: 263/513 Train Loss: 216.7535\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 264/513 | Batch 000/059 | Loss: 198.0893\n",
      "Epoch: 264/513 Train Loss: 214.0838\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 265/513 | Batch 000/059 | Loss: 345.0342\n",
      "Epoch: 265/513 Train Loss: 213.7773\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 266/513 | Batch 000/059 | Loss: 205.5998\n",
      "Epoch: 266/513 Train Loss: 214.5962\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 267/513 | Batch 000/059 | Loss: 208.0581\n",
      "Epoch: 267/513 Train Loss: 217.7591\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 268/513 | Batch 000/059 | Loss: 338.0330\n",
      "Epoch: 268/513 Train Loss: 218.1721\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 269/513 | Batch 000/059 | Loss: 253.2596\n",
      "Epoch: 269/513 Train Loss: 221.7750\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 270/513 | Batch 000/059 | Loss: 353.5978\n",
      "Epoch: 270/513 Train Loss: 220.9992\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 271/513 | Batch 000/059 | Loss: 227.0477\n",
      "Epoch: 271/513 Train Loss: 217.5774\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 272/513 | Batch 000/059 | Loss: 621.4853\n",
      "Epoch: 272/513 Train Loss: 214.1886\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 273/513 | Batch 000/059 | Loss: 272.2122\n",
      "Epoch: 273/513 Train Loss: 214.5443\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 274/513 | Batch 000/059 | Loss: 212.4828\n",
      "Epoch: 274/513 Train Loss: 213.8220\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 275/513 | Batch 000/059 | Loss: 361.3254\n",
      "Epoch: 275/513 Train Loss: 213.5795\n",
      "Time elapsed: 0.89 min\n",
      "Epoch: 276/513 | Batch 000/059 | Loss: 317.1620\n",
      "Epoch: 276/513 Train Loss: 213.2196\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 277/513 | Batch 000/059 | Loss: 282.7409\n",
      "Epoch: 277/513 Train Loss: 215.3021\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 278/513 | Batch 000/059 | Loss: 296.4193\n",
      "Epoch: 278/513 Train Loss: 215.0867\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 279/513 | Batch 000/059 | Loss: 254.5529\n",
      "Epoch: 279/513 Train Loss: 214.4576\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 280/513 | Batch 000/059 | Loss: 289.4565\n",
      "Epoch: 280/513 Train Loss: 217.2689\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 281/513 | Batch 000/059 | Loss: 226.8598\n",
      "Epoch: 281/513 Train Loss: 216.1861\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 282/513 | Batch 000/059 | Loss: 181.5859\n",
      "Epoch: 282/513 Train Loss: 213.8581\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 283/513 | Batch 000/059 | Loss: 264.4865\n",
      "Epoch: 283/513 Train Loss: 213.6577\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 284/513 | Batch 000/059 | Loss: 236.9865\n",
      "Epoch: 284/513 Train Loss: 214.4362\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 285/513 | Batch 000/059 | Loss: 308.4175\n",
      "Epoch: 285/513 Train Loss: 215.4854\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 286/513 | Batch 000/059 | Loss: 232.3843\n",
      "Epoch: 286/513 Train Loss: 213.7133\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 287/513 | Batch 000/059 | Loss: 263.1416\n",
      "Epoch: 287/513 Train Loss: 213.3077\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 288/513 | Batch 000/059 | Loss: 407.7818\n",
      "Epoch: 288/513 Train Loss: 214.1879\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 289/513 | Batch 000/059 | Loss: 284.9377\n",
      "Epoch: 289/513 Train Loss: 214.5376\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 290/513 | Batch 000/059 | Loss: 291.0991\n",
      "Epoch: 290/513 Train Loss: 213.8833\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 291/513 | Batch 000/059 | Loss: 395.5409\n",
      "Epoch: 291/513 Train Loss: 214.7263\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 292/513 | Batch 000/059 | Loss: 166.3824\n",
      "Epoch: 292/513 Train Loss: 214.7430\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 293/513 | Batch 000/059 | Loss: 467.2388\n",
      "Epoch: 293/513 Train Loss: 213.7380\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 294/513 | Batch 000/059 | Loss: 444.1022\n",
      "Epoch: 294/513 Train Loss: 213.8039\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 295/513 | Batch 000/059 | Loss: 164.7280\n",
      "Epoch: 295/513 Train Loss: 212.8781\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 296/513 | Batch 000/059 | Loss: 367.7964\n",
      "Epoch: 296/513 Train Loss: 212.9612\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 297/513 | Batch 000/059 | Loss: 320.8458\n",
      "Epoch: 297/513 Train Loss: 213.9355\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 298/513 | Batch 000/059 | Loss: 271.7876\n",
      "Epoch: 298/513 Train Loss: 214.1603\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 299/513 | Batch 000/059 | Loss: 221.1771\n",
      "Epoch: 299/513 Train Loss: 214.2939\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 300/513 | Batch 000/059 | Loss: 280.5933\n",
      "Epoch: 300/513 Train Loss: 215.1336\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 301/513 | Batch 000/059 | Loss: 210.7758\n",
      "Epoch: 301/513 Train Loss: 214.8055\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 302/513 | Batch 000/059 | Loss: 232.1431\n",
      "Epoch: 302/513 Train Loss: 213.1954\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 303/513 | Batch 000/059 | Loss: 273.8613\n",
      "Epoch: 303/513 Train Loss: 212.7683\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 304/513 | Batch 000/059 | Loss: 399.2572\n",
      "Epoch: 304/513 Train Loss: 215.1591\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 305/513 | Batch 000/059 | Loss: 245.3876\n",
      "Epoch: 305/513 Train Loss: 212.6116\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 306/513 | Batch 000/059 | Loss: 237.6898\n",
      "Epoch: 306/513 Train Loss: 216.1588\n",
      "Time elapsed: 0.99 min\n",
      "Epoch: 307/513 | Batch 000/059 | Loss: 249.0662\n",
      "Epoch: 307/513 Train Loss: 213.4588\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 308/513 | Batch 000/059 | Loss: 283.4040\n",
      "Epoch: 308/513 Train Loss: 213.3759\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 309/513 | Batch 000/059 | Loss: 258.2913\n",
      "Epoch: 309/513 Train Loss: 213.5237\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 310/513 | Batch 000/059 | Loss: 300.9058\n",
      "Epoch: 310/513 Train Loss: 214.1342\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 311/513 | Batch 000/059 | Loss: 341.2798\n",
      "Epoch: 311/513 Train Loss: 214.8796\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 312/513 | Batch 000/059 | Loss: 191.7435\n",
      "Epoch: 312/513 Train Loss: 222.0106\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 313/513 | Batch 000/059 | Loss: 407.6245\n",
      "Epoch: 313/513 Train Loss: 214.0490\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 314/513 | Batch 000/059 | Loss: 219.8461\n",
      "Epoch: 314/513 Train Loss: 213.1843\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 315/513 | Batch 000/059 | Loss: 205.1636\n",
      "Epoch: 315/513 Train Loss: 213.4554\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 316/513 | Batch 000/059 | Loss: 171.9474\n",
      "Epoch: 316/513 Train Loss: 213.0401\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 317/513 | Batch 000/059 | Loss: 222.0472\n",
      "Epoch: 317/513 Train Loss: 212.8522\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 318/513 | Batch 000/059 | Loss: 220.4556\n",
      "Epoch: 318/513 Train Loss: 213.7134\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 319/513 | Batch 000/059 | Loss: 142.1613\n",
      "Epoch: 319/513 Train Loss: 212.5408\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 320/513 | Batch 000/059 | Loss: 282.7199\n",
      "Epoch: 320/513 Train Loss: 215.8107\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 321/513 | Batch 000/059 | Loss: 356.2258\n",
      "Epoch: 321/513 Train Loss: 212.7212\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 322/513 | Batch 000/059 | Loss: 290.7551\n",
      "Epoch: 322/513 Train Loss: 214.5194\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 323/513 | Batch 000/059 | Loss: 196.9160\n",
      "Epoch: 323/513 Train Loss: 214.0419\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 324/513 | Batch 000/059 | Loss: 350.0012\n",
      "Epoch: 324/513 Train Loss: 213.0893\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 325/513 | Batch 000/059 | Loss: 238.5815\n",
      "Epoch: 325/513 Train Loss: 214.0482\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 326/513 | Batch 000/059 | Loss: 247.4438\n",
      "Epoch: 326/513 Train Loss: 213.0205\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 327/513 | Batch 000/059 | Loss: 265.7328\n",
      "Epoch: 327/513 Train Loss: 213.0149\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 328/513 | Batch 000/059 | Loss: 248.1818\n",
      "Epoch: 328/513 Train Loss: 211.8525\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 329/513 | Batch 000/059 | Loss: 368.5295\n",
      "Epoch: 329/513 Train Loss: 213.1483\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 330/513 | Batch 000/059 | Loss: 312.5760\n",
      "Epoch: 330/513 Train Loss: 212.8627\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 331/513 | Batch 000/059 | Loss: 223.2164\n",
      "Epoch: 331/513 Train Loss: 216.0824\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 332/513 | Batch 000/059 | Loss: 264.2800\n",
      "Epoch: 332/513 Train Loss: 215.2497\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 333/513 | Batch 000/059 | Loss: 328.8917\n",
      "Epoch: 333/513 Train Loss: 214.0730\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 334/513 | Batch 000/059 | Loss: 229.7668\n",
      "Epoch: 334/513 Train Loss: 213.2446\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 335/513 | Batch 000/059 | Loss: 405.3027\n",
      "Epoch: 335/513 Train Loss: 212.6478\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 336/513 | Batch 000/059 | Loss: 180.0114\n",
      "Epoch: 336/513 Train Loss: 213.8059\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 337/513 | Batch 000/059 | Loss: 328.1638\n",
      "Epoch: 337/513 Train Loss: 212.4407\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 338/513 | Batch 000/059 | Loss: 152.9936\n",
      "Epoch: 338/513 Train Loss: 214.1909\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 339/513 | Batch 000/059 | Loss: 279.9974\n",
      "Epoch: 339/513 Train Loss: 212.8108\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 340/513 | Batch 000/059 | Loss: 211.3147\n",
      "Epoch: 340/513 Train Loss: 214.4565\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 341/513 | Batch 000/059 | Loss: 318.5851\n",
      "Epoch: 341/513 Train Loss: 212.0510\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 342/513 | Batch 000/059 | Loss: 177.9476\n",
      "Epoch: 342/513 Train Loss: 213.5439\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 343/513 | Batch 000/059 | Loss: 202.5034\n",
      "Epoch: 343/513 Train Loss: 216.2452\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 344/513 | Batch 000/059 | Loss: 238.6184\n",
      "Epoch: 344/513 Train Loss: 215.8736\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 345/513 | Batch 000/059 | Loss: 332.8191\n",
      "Epoch: 345/513 Train Loss: 212.4519\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 346/513 | Batch 000/059 | Loss: 191.0995\n",
      "Epoch: 346/513 Train Loss: 213.3812\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 347/513 | Batch 000/059 | Loss: 326.0945\n",
      "Epoch: 347/513 Train Loss: 213.8763\n",
      "Time elapsed: 1.12 min\n",
      "Epoch: 348/513 | Batch 000/059 | Loss: 196.8236\n",
      "Epoch: 348/513 Train Loss: 212.8747\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 349/513 | Batch 000/059 | Loss: 355.9698\n",
      "Epoch: 349/513 Train Loss: 213.9395\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 350/513 | Batch 000/059 | Loss: 430.9390\n",
      "Epoch: 350/513 Train Loss: 212.8102\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 351/513 | Batch 000/059 | Loss: 236.5433\n",
      "Epoch: 351/513 Train Loss: 214.0676\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 352/513 | Batch 000/059 | Loss: 374.3674\n",
      "Epoch: 352/513 Train Loss: 212.0493\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 353/513 | Batch 000/059 | Loss: 231.2802\n",
      "Epoch: 353/513 Train Loss: 212.5873\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 354/513 | Batch 000/059 | Loss: 270.4462\n",
      "Epoch: 354/513 Train Loss: 212.1880\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 355/513 | Batch 000/059 | Loss: 254.0094\n",
      "Epoch: 355/513 Train Loss: 215.6917\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 356/513 | Batch 000/059 | Loss: 155.0068\n",
      "Epoch: 356/513 Train Loss: 216.3895\n",
      "Time elapsed: 1.15 min\n",
      "Epoch: 357/513 | Batch 000/059 | Loss: 368.2609\n",
      "Epoch: 357/513 Train Loss: 212.0046\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 358/513 | Batch 000/059 | Loss: 345.4357\n",
      "Epoch: 358/513 Train Loss: 212.9582\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 359/513 | Batch 000/059 | Loss: 191.1900\n",
      "Epoch: 359/513 Train Loss: 212.5903\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 360/513 | Batch 000/059 | Loss: 278.4576\n",
      "Epoch: 360/513 Train Loss: 211.5941\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 361/513 | Batch 000/059 | Loss: 291.9919\n",
      "Epoch: 361/513 Train Loss: 212.6865\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 362/513 | Batch 000/059 | Loss: 206.7580\n",
      "Epoch: 362/513 Train Loss: 214.7843\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 363/513 | Batch 000/059 | Loss: 285.6152\n",
      "Epoch: 363/513 Train Loss: 212.9368\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 364/513 | Batch 000/059 | Loss: 317.2206\n",
      "Epoch: 364/513 Train Loss: 211.7134\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 365/513 | Batch 000/059 | Loss: 251.0400\n",
      "Epoch: 365/513 Train Loss: 212.2197\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 366/513 | Batch 000/059 | Loss: 363.5979\n",
      "Epoch: 366/513 Train Loss: 216.5491\n",
      "Time elapsed: 1.19 min\n",
      "Epoch: 367/513 | Batch 000/059 | Loss: 296.3704\n",
      "Epoch: 367/513 Train Loss: 215.5136\n",
      "Time elapsed: 1.19 min\n",
      "Epoch: 368/513 | Batch 000/059 | Loss: 335.6996\n",
      "Epoch: 368/513 Train Loss: 215.8259\n",
      "Time elapsed: 1.19 min\n",
      "Epoch: 369/513 | Batch 000/059 | Loss: 442.7567\n",
      "Epoch: 369/513 Train Loss: 212.0699\n",
      "Time elapsed: 1.20 min\n",
      "Epoch: 370/513 | Batch 000/059 | Loss: 198.0664\n",
      "Epoch: 370/513 Train Loss: 211.2543\n",
      "Time elapsed: 1.20 min\n",
      "Epoch: 371/513 | Batch 000/059 | Loss: 168.1735\n",
      "Epoch: 371/513 Train Loss: 216.2440\n",
      "Time elapsed: 1.20 min\n",
      "Epoch: 372/513 | Batch 000/059 | Loss: 176.0281\n",
      "Epoch: 372/513 Train Loss: 212.4790\n",
      "Time elapsed: 1.21 min\n",
      "Epoch: 373/513 | Batch 000/059 | Loss: 267.0799\n",
      "Epoch: 373/513 Train Loss: 211.4095\n",
      "Time elapsed: 1.21 min\n",
      "Epoch: 374/513 | Batch 000/059 | Loss: 360.0912\n",
      "Epoch: 374/513 Train Loss: 213.5651\n",
      "Time elapsed: 1.21 min\n",
      "Epoch: 375/513 | Batch 000/059 | Loss: 263.9194\n",
      "Epoch: 375/513 Train Loss: 212.1866\n",
      "Time elapsed: 1.21 min\n",
      "Epoch: 376/513 | Batch 000/059 | Loss: 262.3215\n",
      "Epoch: 376/513 Train Loss: 213.1157\n",
      "Time elapsed: 1.22 min\n",
      "Epoch: 377/513 | Batch 000/059 | Loss: 295.9269\n",
      "Epoch: 377/513 Train Loss: 213.1665\n",
      "Time elapsed: 1.22 min\n",
      "Epoch: 378/513 | Batch 000/059 | Loss: 357.1948\n",
      "Epoch: 378/513 Train Loss: 212.5839\n",
      "Time elapsed: 1.22 min\n",
      "Epoch: 379/513 | Batch 000/059 | Loss: 179.9998\n",
      "Epoch: 379/513 Train Loss: 215.5502\n",
      "Time elapsed: 1.23 min\n",
      "Epoch: 380/513 | Batch 000/059 | Loss: 230.8443\n",
      "Epoch: 380/513 Train Loss: 211.8263\n",
      "Time elapsed: 1.23 min\n",
      "Epoch: 381/513 | Batch 000/059 | Loss: 295.7377\n",
      "Epoch: 381/513 Train Loss: 212.5418\n",
      "Time elapsed: 1.23 min\n",
      "Epoch: 382/513 | Batch 000/059 | Loss: 355.5602\n",
      "Epoch: 382/513 Train Loss: 211.9989\n",
      "Time elapsed: 1.24 min\n",
      "Epoch: 383/513 | Batch 000/059 | Loss: 205.5247\n",
      "Epoch: 383/513 Train Loss: 212.0214\n",
      "Time elapsed: 1.24 min\n",
      "Epoch: 384/513 | Batch 000/059 | Loss: 331.4704\n",
      "Epoch: 384/513 Train Loss: 211.6745\n",
      "Time elapsed: 1.24 min\n",
      "Epoch: 385/513 | Batch 000/059 | Loss: 210.0021\n",
      "Epoch: 385/513 Train Loss: 213.2162\n",
      "Time elapsed: 1.25 min\n",
      "Epoch: 386/513 | Batch 000/059 | Loss: 191.8338\n",
      "Epoch: 386/513 Train Loss: 213.7252\n",
      "Time elapsed: 1.25 min\n",
      "Epoch: 387/513 | Batch 000/059 | Loss: 308.7794\n",
      "Epoch: 387/513 Train Loss: 211.6096\n",
      "Time elapsed: 1.25 min\n",
      "Epoch: 388/513 | Batch 000/059 | Loss: 345.1022\n",
      "Epoch: 388/513 Train Loss: 211.4666\n",
      "Time elapsed: 1.26 min\n",
      "Epoch: 389/513 | Batch 000/059 | Loss: 405.7172\n",
      "Epoch: 389/513 Train Loss: 211.4328\n",
      "Time elapsed: 1.26 min\n",
      "Epoch: 390/513 | Batch 000/059 | Loss: 203.1804\n",
      "Epoch: 390/513 Train Loss: 213.2865\n",
      "Time elapsed: 1.26 min\n",
      "Epoch: 391/513 | Batch 000/059 | Loss: 230.3980\n",
      "Epoch: 391/513 Train Loss: 211.2344\n",
      "Time elapsed: 1.27 min\n",
      "Epoch: 392/513 | Batch 000/059 | Loss: 283.4480\n",
      "Epoch: 392/513 Train Loss: 211.0142\n",
      "Time elapsed: 1.27 min\n",
      "Epoch: 393/513 | Batch 000/059 | Loss: 249.9802\n",
      "Epoch: 393/513 Train Loss: 210.9853\n",
      "Time elapsed: 1.27 min\n",
      "Epoch: 394/513 | Batch 000/059 | Loss: 286.0816\n",
      "Epoch: 394/513 Train Loss: 211.5081\n",
      "Time elapsed: 1.28 min\n",
      "Epoch: 395/513 | Batch 000/059 | Loss: 271.8829\n",
      "Epoch: 395/513 Train Loss: 211.6980\n",
      "Time elapsed: 1.28 min\n",
      "Epoch: 396/513 | Batch 000/059 | Loss: 213.9865\n",
      "Epoch: 396/513 Train Loss: 211.3552\n",
      "Time elapsed: 1.28 min\n",
      "Epoch: 397/513 | Batch 000/059 | Loss: 303.5920\n",
      "Epoch: 397/513 Train Loss: 212.0827\n",
      "Time elapsed: 1.29 min\n",
      "Epoch: 398/513 | Batch 000/059 | Loss: 434.0910\n",
      "Epoch: 398/513 Train Loss: 211.1090\n",
      "Time elapsed: 1.29 min\n",
      "Epoch: 399/513 | Batch 000/059 | Loss: 224.5850\n",
      "Epoch: 399/513 Train Loss: 214.1332\n",
      "Time elapsed: 1.29 min\n",
      "Epoch: 400/513 | Batch 000/059 | Loss: 271.8568\n",
      "Epoch: 400/513 Train Loss: 211.8100\n",
      "Time elapsed: 1.29 min\n",
      "Epoch: 401/513 | Batch 000/059 | Loss: 550.2121\n",
      "Epoch: 401/513 Train Loss: 211.4634\n",
      "Time elapsed: 1.30 min\n",
      "Epoch: 402/513 | Batch 000/059 | Loss: 380.1926\n",
      "Epoch: 402/513 Train Loss: 211.5810\n",
      "Time elapsed: 1.30 min\n",
      "Epoch: 403/513 | Batch 000/059 | Loss: 207.0061\n",
      "Epoch: 403/513 Train Loss: 217.1705\n",
      "Time elapsed: 1.30 min\n",
      "Epoch: 404/513 | Batch 000/059 | Loss: 318.1343\n",
      "Epoch: 404/513 Train Loss: 210.8670\n",
      "Time elapsed: 1.31 min\n",
      "Epoch: 405/513 | Batch 000/059 | Loss: 189.8659\n",
      "Epoch: 405/513 Train Loss: 217.7152\n",
      "Time elapsed: 1.31 min\n",
      "Epoch: 406/513 | Batch 000/059 | Loss: 168.5619\n",
      "Epoch: 406/513 Train Loss: 211.5649\n",
      "Time elapsed: 1.31 min\n",
      "Epoch: 407/513 | Batch 000/059 | Loss: 586.3735\n",
      "Epoch: 407/513 Train Loss: 213.4756\n",
      "Time elapsed: 1.32 min\n",
      "Epoch: 408/513 | Batch 000/059 | Loss: 392.0987\n",
      "Epoch: 408/513 Train Loss: 214.0735\n",
      "Time elapsed: 1.32 min\n",
      "Epoch: 409/513 | Batch 000/059 | Loss: 198.3726\n",
      "Epoch: 409/513 Train Loss: 213.0052\n",
      "Time elapsed: 1.32 min\n",
      "Epoch: 410/513 | Batch 000/059 | Loss: 235.6862\n",
      "Epoch: 410/513 Train Loss: 213.2095\n",
      "Time elapsed: 1.33 min\n",
      "Epoch: 411/513 | Batch 000/059 | Loss: 208.7577\n",
      "Epoch: 411/513 Train Loss: 213.2129\n",
      "Time elapsed: 1.33 min\n",
      "Epoch: 412/513 | Batch 000/059 | Loss: 218.2886\n",
      "Epoch: 412/513 Train Loss: 215.2547\n",
      "Time elapsed: 1.33 min\n",
      "Epoch: 413/513 | Batch 000/059 | Loss: 353.4243\n",
      "Epoch: 413/513 Train Loss: 210.9193\n",
      "Time elapsed: 1.34 min\n",
      "Epoch: 414/513 | Batch 000/059 | Loss: 293.8112\n",
      "Epoch: 414/513 Train Loss: 210.6063\n",
      "Time elapsed: 1.34 min\n",
      "Epoch: 415/513 | Batch 000/059 | Loss: 245.8387\n",
      "Epoch: 415/513 Train Loss: 213.4619\n",
      "Time elapsed: 1.34 min\n",
      "Epoch: 416/513 | Batch 000/059 | Loss: 248.7249\n",
      "Epoch: 416/513 Train Loss: 212.3514\n",
      "Time elapsed: 1.35 min\n",
      "Epoch: 417/513 | Batch 000/059 | Loss: 271.0365\n",
      "Epoch: 417/513 Train Loss: 212.1992\n",
      "Time elapsed: 1.35 min\n",
      "Epoch: 418/513 | Batch 000/059 | Loss: 411.1215\n",
      "Epoch: 418/513 Train Loss: 212.1932\n",
      "Time elapsed: 1.35 min\n",
      "Epoch: 419/513 | Batch 000/059 | Loss: 311.0056\n",
      "Epoch: 419/513 Train Loss: 211.2427\n",
      "Time elapsed: 1.36 min\n",
      "Epoch: 420/513 | Batch 000/059 | Loss: 267.9734\n",
      "Epoch: 420/513 Train Loss: 213.1229\n",
      "Time elapsed: 1.36 min\n",
      "Epoch: 421/513 | Batch 000/059 | Loss: 228.8434\n",
      "Epoch: 421/513 Train Loss: 211.9204\n",
      "Time elapsed: 1.36 min\n",
      "Epoch: 422/513 | Batch 000/059 | Loss: 285.7721\n",
      "Epoch: 422/513 Train Loss: 211.0930\n",
      "Time elapsed: 1.37 min\n",
      "Epoch: 423/513 | Batch 000/059 | Loss: 501.2273\n",
      "Epoch: 423/513 Train Loss: 210.9969\n",
      "Time elapsed: 1.37 min\n",
      "Epoch: 424/513 | Batch 000/059 | Loss: 218.0193\n",
      "Epoch: 424/513 Train Loss: 210.6666\n",
      "Time elapsed: 1.37 min\n",
      "Epoch: 425/513 | Batch 000/059 | Loss: 284.3417\n",
      "Epoch: 425/513 Train Loss: 211.0336\n",
      "Time elapsed: 1.38 min\n",
      "Epoch: 426/513 | Batch 000/059 | Loss: 275.7668\n",
      "Epoch: 426/513 Train Loss: 211.9068\n",
      "Time elapsed: 1.38 min\n",
      "Epoch: 427/513 | Batch 000/059 | Loss: 255.1810\n",
      "Epoch: 427/513 Train Loss: 210.6596\n",
      "Time elapsed: 1.38 min\n",
      "Epoch: 428/513 | Batch 000/059 | Loss: 322.9871\n",
      "Epoch: 428/513 Train Loss: 213.3989\n",
      "Time elapsed: 1.39 min\n",
      "Epoch: 429/513 | Batch 000/059 | Loss: 342.5125\n",
      "Epoch: 429/513 Train Loss: 211.2532\n",
      "Time elapsed: 1.39 min\n",
      "Epoch: 430/513 | Batch 000/059 | Loss: 235.6455\n",
      "Epoch: 430/513 Train Loss: 210.6296\n",
      "Time elapsed: 1.39 min\n",
      "Epoch: 431/513 | Batch 000/059 | Loss: 196.6770\n",
      "Epoch: 431/513 Train Loss: 210.9867\n",
      "Time elapsed: 1.39 min\n",
      "Epoch: 432/513 | Batch 000/059 | Loss: 256.3882\n",
      "Epoch: 432/513 Train Loss: 215.0024\n",
      "Time elapsed: 1.40 min\n",
      "Epoch: 433/513 | Batch 000/059 | Loss: 205.8372\n",
      "Epoch: 433/513 Train Loss: 210.6797\n",
      "Time elapsed: 1.40 min\n",
      "Epoch: 434/513 | Batch 000/059 | Loss: 283.0314\n",
      "Epoch: 434/513 Train Loss: 214.5197\n",
      "Time elapsed: 1.40 min\n",
      "Epoch: 435/513 | Batch 000/059 | Loss: 346.8593\n",
      "Epoch: 435/513 Train Loss: 210.6806\n",
      "Time elapsed: 1.41 min\n",
      "Epoch: 436/513 | Batch 000/059 | Loss: 305.6046\n",
      "Epoch: 436/513 Train Loss: 213.1883\n",
      "Time elapsed: 1.41 min\n",
      "Epoch: 437/513 | Batch 000/059 | Loss: 257.8773\n",
      "Epoch: 437/513 Train Loss: 211.4029\n",
      "Time elapsed: 1.41 min\n",
      "Epoch: 438/513 | Batch 000/059 | Loss: 344.2202\n",
      "Epoch: 438/513 Train Loss: 211.6420\n",
      "Time elapsed: 1.42 min\n",
      "Epoch: 439/513 | Batch 000/059 | Loss: 573.9908\n",
      "Epoch: 439/513 Train Loss: 211.0364\n",
      "Time elapsed: 1.42 min\n",
      "Epoch: 440/513 | Batch 000/059 | Loss: 272.3127\n",
      "Epoch: 440/513 Train Loss: 211.4830\n",
      "Time elapsed: 1.42 min\n",
      "Epoch: 441/513 | Batch 000/059 | Loss: 406.0062\n",
      "Epoch: 441/513 Train Loss: 211.3611\n",
      "Time elapsed: 1.43 min\n",
      "Epoch: 442/513 | Batch 000/059 | Loss: 268.3485\n",
      "Epoch: 442/513 Train Loss: 212.1564\n",
      "Time elapsed: 1.43 min\n",
      "Epoch: 443/513 | Batch 000/059 | Loss: 350.5657\n",
      "Epoch: 443/513 Train Loss: 211.3012\n",
      "Time elapsed: 1.43 min\n",
      "Epoch: 444/513 | Batch 000/059 | Loss: 219.0219\n",
      "Epoch: 444/513 Train Loss: 215.5277\n",
      "Time elapsed: 1.44 min\n",
      "Epoch: 445/513 | Batch 000/059 | Loss: 320.6393\n",
      "Epoch: 445/513 Train Loss: 211.8025\n",
      "Time elapsed: 1.44 min\n",
      "Epoch: 446/513 | Batch 000/059 | Loss: 174.8468\n",
      "Epoch: 446/513 Train Loss: 212.7392\n",
      "Time elapsed: 1.44 min\n",
      "Epoch: 447/513 | Batch 000/059 | Loss: 385.7984\n",
      "Epoch: 447/513 Train Loss: 214.5479\n",
      "Time elapsed: 1.45 min\n",
      "Epoch: 448/513 | Batch 000/059 | Loss: 159.7520\n",
      "Epoch: 448/513 Train Loss: 212.3090\n",
      "Time elapsed: 1.45 min\n",
      "Epoch: 449/513 | Batch 000/059 | Loss: 223.9771\n",
      "Epoch: 449/513 Train Loss: 210.4117\n",
      "Time elapsed: 1.45 min\n",
      "Epoch: 450/513 | Batch 000/059 | Loss: 240.0687\n",
      "Epoch: 450/513 Train Loss: 211.3059\n",
      "Time elapsed: 1.46 min\n",
      "Epoch: 451/513 | Batch 000/059 | Loss: 296.2436\n",
      "Epoch: 451/513 Train Loss: 210.7539\n",
      "Time elapsed: 1.46 min\n",
      "Epoch: 452/513 | Batch 000/059 | Loss: 327.6310\n",
      "Epoch: 452/513 Train Loss: 212.6848\n",
      "Time elapsed: 1.46 min\n",
      "Epoch: 453/513 | Batch 000/059 | Loss: 292.5712\n",
      "Epoch: 453/513 Train Loss: 211.9702\n",
      "Time elapsed: 1.47 min\n",
      "Epoch: 454/513 | Batch 000/059 | Loss: 327.9847\n",
      "Epoch: 454/513 Train Loss: 210.3186\n",
      "Time elapsed: 1.47 min\n",
      "Epoch: 455/513 | Batch 000/059 | Loss: 175.8050\n",
      "Epoch: 455/513 Train Loss: 216.3199\n",
      "Time elapsed: 1.47 min\n",
      "Epoch: 456/513 | Batch 000/059 | Loss: 267.3195\n",
      "Epoch: 456/513 Train Loss: 213.4797\n",
      "Time elapsed: 1.48 min\n",
      "Epoch: 457/513 | Batch 000/059 | Loss: 215.8446\n",
      "Epoch: 457/513 Train Loss: 211.7330\n",
      "Time elapsed: 1.48 min\n",
      "Epoch: 458/513 | Batch 000/059 | Loss: 367.2174\n",
      "Epoch: 458/513 Train Loss: 210.8244\n",
      "Time elapsed: 1.48 min\n",
      "Epoch: 459/513 | Batch 000/059 | Loss: 277.1240\n",
      "Epoch: 459/513 Train Loss: 212.8414\n",
      "Time elapsed: 1.48 min\n",
      "Epoch: 460/513 | Batch 000/059 | Loss: 218.1923\n",
      "Epoch: 460/513 Train Loss: 212.2113\n",
      "Time elapsed: 1.49 min\n",
      "Epoch: 461/513 | Batch 000/059 | Loss: 315.0076\n",
      "Epoch: 461/513 Train Loss: 210.8574\n",
      "Time elapsed: 1.49 min\n",
      "Epoch: 462/513 | Batch 000/059 | Loss: 277.9307\n",
      "Epoch: 462/513 Train Loss: 211.9550\n",
      "Time elapsed: 1.49 min\n",
      "Epoch: 463/513 | Batch 000/059 | Loss: 311.1736\n",
      "Epoch: 463/513 Train Loss: 210.4031\n",
      "Time elapsed: 1.50 min\n",
      "Epoch: 464/513 | Batch 000/059 | Loss: 220.3030\n",
      "Epoch: 464/513 Train Loss: 211.1946\n",
      "Time elapsed: 1.50 min\n",
      "Epoch: 465/513 | Batch 000/059 | Loss: 334.3221\n",
      "Epoch: 465/513 Train Loss: 213.3660\n",
      "Time elapsed: 1.50 min\n",
      "Epoch: 466/513 | Batch 000/059 | Loss: 311.9556\n",
      "Epoch: 466/513 Train Loss: 211.8588\n",
      "Time elapsed: 1.51 min\n",
      "Epoch: 467/513 | Batch 000/059 | Loss: 214.8335\n",
      "Epoch: 467/513 Train Loss: 210.4348\n",
      "Time elapsed: 1.51 min\n",
      "Epoch: 468/513 | Batch 000/059 | Loss: 144.1626\n",
      "Epoch: 468/513 Train Loss: 212.2558\n",
      "Time elapsed: 1.51 min\n",
      "Epoch: 469/513 | Batch 000/059 | Loss: 166.3774\n",
      "Epoch: 469/513 Train Loss: 210.5962\n",
      "Time elapsed: 1.52 min\n",
      "Epoch: 470/513 | Batch 000/059 | Loss: 204.8426\n",
      "Epoch: 470/513 Train Loss: 210.2775\n",
      "Time elapsed: 1.52 min\n",
      "Epoch: 471/513 | Batch 000/059 | Loss: 436.0302\n",
      "Epoch: 471/513 Train Loss: 214.3038\n",
      "Time elapsed: 1.52 min\n",
      "Epoch: 472/513 | Batch 000/059 | Loss: 197.4178\n",
      "Epoch: 472/513 Train Loss: 211.3483\n",
      "Time elapsed: 1.53 min\n",
      "Epoch: 473/513 | Batch 000/059 | Loss: 397.5421\n",
      "Epoch: 473/513 Train Loss: 210.9074\n",
      "Time elapsed: 1.53 min\n",
      "Epoch: 474/513 | Batch 000/059 | Loss: 241.5585\n",
      "Epoch: 474/513 Train Loss: 210.7849\n",
      "Time elapsed: 1.53 min\n",
      "Epoch: 475/513 | Batch 000/059 | Loss: 307.1756\n",
      "Epoch: 475/513 Train Loss: 210.4348\n",
      "Time elapsed: 1.54 min\n",
      "Epoch: 476/513 | Batch 000/059 | Loss: 219.3497\n",
      "Epoch: 476/513 Train Loss: 210.4182\n",
      "Time elapsed: 1.54 min\n",
      "Epoch: 477/513 | Batch 000/059 | Loss: 196.5066\n",
      "Epoch: 477/513 Train Loss: 210.6924\n",
      "Time elapsed: 1.54 min\n",
      "Epoch: 478/513 | Batch 000/059 | Loss: 367.1038\n",
      "Epoch: 478/513 Train Loss: 211.7078\n",
      "Time elapsed: 1.55 min\n",
      "Epoch: 479/513 | Batch 000/059 | Loss: 360.9088\n",
      "Epoch: 479/513 Train Loss: 211.6192\n",
      "Time elapsed: 1.55 min\n",
      "Epoch: 480/513 | Batch 000/059 | Loss: 321.5996\n",
      "Epoch: 480/513 Train Loss: 212.5288\n",
      "Time elapsed: 1.55 min\n",
      "Epoch: 481/513 | Batch 000/059 | Loss: 247.9869\n",
      "Epoch: 481/513 Train Loss: 211.0150\n",
      "Time elapsed: 1.56 min\n",
      "Epoch: 482/513 | Batch 000/059 | Loss: 289.5503\n",
      "Epoch: 482/513 Train Loss: 210.5554\n",
      "Time elapsed: 1.56 min\n",
      "Epoch: 483/513 | Batch 000/059 | Loss: 362.3069\n",
      "Epoch: 483/513 Train Loss: 211.8595\n",
      "Time elapsed: 1.56 min\n",
      "Epoch: 484/513 | Batch 000/059 | Loss: 88.8500\n",
      "Epoch: 484/513 Train Loss: 215.3040\n",
      "Time elapsed: 1.57 min\n",
      "Epoch: 485/513 | Batch 000/059 | Loss: 295.8082\n",
      "Epoch: 485/513 Train Loss: 210.6976\n",
      "Time elapsed: 1.57 min\n",
      "Epoch: 486/513 | Batch 000/059 | Loss: 394.3792\n",
      "Epoch: 486/513 Train Loss: 210.6292\n",
      "Time elapsed: 1.57 min\n",
      "Epoch: 487/513 | Batch 000/059 | Loss: 316.9581\n",
      "Epoch: 487/513 Train Loss: 210.3428\n",
      "Time elapsed: 1.58 min\n",
      "Epoch: 488/513 | Batch 000/059 | Loss: 174.4447\n",
      "Epoch: 488/513 Train Loss: 211.3487\n",
      "Time elapsed: 1.58 min\n",
      "Epoch: 489/513 | Batch 000/059 | Loss: 406.5988\n",
      "Epoch: 489/513 Train Loss: 210.4076\n",
      "Time elapsed: 1.58 min\n",
      "Epoch: 490/513 | Batch 000/059 | Loss: 241.1362\n",
      "Epoch: 490/513 Train Loss: 212.2805\n",
      "Time elapsed: 1.58 min\n",
      "Epoch: 491/513 | Batch 000/059 | Loss: 281.2769\n",
      "Epoch: 491/513 Train Loss: 210.2146\n",
      "Time elapsed: 1.59 min\n",
      "Epoch: 492/513 | Batch 000/059 | Loss: 249.7956\n",
      "Epoch: 492/513 Train Loss: 211.1090\n",
      "Time elapsed: 1.59 min\n",
      "Epoch: 493/513 | Batch 000/059 | Loss: 286.5318\n",
      "Epoch: 493/513 Train Loss: 210.6416\n",
      "Time elapsed: 1.59 min\n",
      "Epoch: 494/513 | Batch 000/059 | Loss: 281.7558\n",
      "Epoch: 494/513 Train Loss: 211.7277\n",
      "Time elapsed: 1.60 min\n",
      "Epoch: 495/513 | Batch 000/059 | Loss: 277.3472\n",
      "Epoch: 495/513 Train Loss: 210.2430\n",
      "Time elapsed: 1.60 min\n",
      "Epoch: 496/513 | Batch 000/059 | Loss: 141.9500\n",
      "Epoch: 496/513 Train Loss: 214.1103\n",
      "Time elapsed: 1.60 min\n",
      "Epoch: 497/513 | Batch 000/059 | Loss: 250.1743\n",
      "Epoch: 497/513 Train Loss: 213.9679\n",
      "Time elapsed: 1.61 min\n",
      "Epoch: 498/513 | Batch 000/059 | Loss: 382.0021\n",
      "Epoch: 498/513 Train Loss: 210.8193\n",
      "Time elapsed: 1.61 min\n",
      "Epoch: 499/513 | Batch 000/059 | Loss: 274.5110\n",
      "Epoch: 499/513 Train Loss: 212.3656\n",
      "Time elapsed: 1.61 min\n",
      "Epoch: 500/513 | Batch 000/059 | Loss: 265.0830\n",
      "Epoch: 500/513 Train Loss: 211.3909\n",
      "Time elapsed: 1.62 min\n",
      "Epoch: 501/513 | Batch 000/059 | Loss: 246.2603\n",
      "Epoch: 501/513 Train Loss: 210.9019\n",
      "Time elapsed: 1.62 min\n",
      "Epoch: 502/513 | Batch 000/059 | Loss: 251.9500\n",
      "Epoch: 502/513 Train Loss: 210.8920\n",
      "Time elapsed: 1.62 min\n",
      "Epoch: 503/513 | Batch 000/059 | Loss: 310.1783\n",
      "Epoch: 503/513 Train Loss: 212.4874\n",
      "Time elapsed: 1.63 min\n",
      "Epoch: 504/513 | Batch 000/059 | Loss: 266.0725\n",
      "Epoch: 504/513 Train Loss: 210.6000\n",
      "Time elapsed: 1.63 min\n",
      "Epoch: 505/513 | Batch 000/059 | Loss: 206.0431\n",
      "Epoch: 505/513 Train Loss: 210.8162\n",
      "Time elapsed: 1.63 min\n",
      "Epoch: 506/513 | Batch 000/059 | Loss: 231.1746\n",
      "Epoch: 506/513 Train Loss: 211.7778\n",
      "Time elapsed: 1.64 min\n",
      "Epoch: 507/513 | Batch 000/059 | Loss: 329.7188\n",
      "Epoch: 507/513 Train Loss: 209.8286\n",
      "Time elapsed: 1.64 min\n",
      "Epoch: 508/513 | Batch 000/059 | Loss: 157.3177\n",
      "Epoch: 508/513 Train Loss: 210.5990\n",
      "Time elapsed: 1.64 min\n",
      "Epoch: 509/513 | Batch 000/059 | Loss: 226.3390\n",
      "Epoch: 509/513 Train Loss: 210.4053\n",
      "Time elapsed: 1.65 min\n",
      "Epoch: 510/513 | Batch 000/059 | Loss: 294.5050\n",
      "Epoch: 510/513 Train Loss: 210.2842\n",
      "Time elapsed: 1.65 min\n",
      "Epoch: 511/513 | Batch 000/059 | Loss: 136.6377\n",
      "Epoch: 511/513 Train Loss: 210.3133\n",
      "Time elapsed: 1.65 min\n",
      "Epoch: 512/513 | Batch 000/059 | Loss: 269.8553\n",
      "Epoch: 512/513 Train Loss: 211.5987\n",
      "Time elapsed: 1.66 min\n",
      "Epoch: 513/513 | Batch 000/059 | Loss: 319.7964\n",
      "Epoch: 513/513 Train Loss: 216.2646\n",
      "Time elapsed: 1.66 min\n",
      "Total Training Time: 1.66 min\n",
      "Training Loss: 216.26\n",
      "Test Loss: 230.41\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "minibatch_cost = []\n",
    "epoch_cost = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(device)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        outputs = model(features)\n",
    "        \n",
    "        cost = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        minibatch_cost.append(cost)\n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 1000:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "       \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        cost = compute_loss(model, train_loader)\n",
    "        epoch_cost.append(cost)\n",
    "        print('Epoch: %03d/%03d Train Loss: %.4f' % (\n",
    "                epoch+1, num_epochs, cost))\n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "        \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "    print('Training Loss: %.2f' % compute_loss(model, train_loader))\n",
    "    print('Test Loss: %.2f' % compute_loss(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "qXJ86ptBuql_",
    "outputId": "20f6ba99-1dfa-4cc4-8c73-81d22618ba69"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the loss curve\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(len(epoch_cost)), epoch_cost)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross entropy loss\")\n",
    "plt.title(\"Loss of training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIExIurTxJT8"
   },
   "source": [
    "## Results of regularizations\n",
    "\n",
    "| Model Name                    | Training Loss | Test Loss | Training Loss(Dropout) | Test Loss(Dropout) | \n",
    "|--------------------------------|----------|----------|----------|----------|\n",
    "| Model A |  220.02  |  231.08 |  217.57   |   229.48  |\n",
    "| Model B |  217.62  |  225.09  |  212.96   |   224.75  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5JR8yQLxJh6"
   },
   "source": [
    "## Results of optimizations (Test loss)\n",
    "- With dropouts from the regularization part\n",
    "\n",
    "| Optimizer                    | Model B |\n",
    "|--------------------------------|----------|\n",
    "| SGD |  224.20  |\n",
    "| SGD Momentum |  221.33  | \n",
    "| SGD Nesterov |  228.44  | \n",
    "| Adam |  238.04  |\n",
    "| Adagrad |  237.06  |\n",
    "| Adadelta |  223.24  | \n",
    "| **Adamax** |  **220.31**  |\n",
    "| RMSProp |  236.81  | \n",
    "\n",
    "\n",
    "### Batch normalozation\n",
    "- With dropouts from the regularization part\n",
    "- We employed best optimizer for the model from the previous table\n",
    "\n",
    "| Model Name | Optimizer| Training Loss | Test Loss |\n",
    "|------------|----------|----------|----------|\n",
    "| Model B |  Adamax  |  216.26  | 230.41  |\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Some_regularizations_on_Abalone.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
