{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN) with PyTorch (on MNIST)\n",
    "By [Zahra Taheri](https://github.com/zata213), September 11, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Networks Transition to Recurrent Neural Networks\n",
    "\n",
    "### RNN is essentially an FNN\n",
    "\n",
    "![alt text](fnn.png)\n",
    "\n",
    "![alt text](rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](fnn-2.png)\n",
    "\n",
    "![alt text](rnn-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing the number of hidden layers in RNN\n",
    "\n",
    "- In RNN, whenever you see a non-linear output, it is needed to put a linear function to ensure that information passes through time steps.\n",
    "- RNN is essencially repeating FNN, but information get passed through with your previous non-linear output passing through a linear function to your current hidden state.\n",
    "\n",
    "![alt text](rnn-3.png)\n",
    "\n",
    "![alt text](rnn-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Recurrent Neural Networks with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model A: 1 hidden layer (ReLU)\n",
    "- Unroll 28 time steps\n",
    "    - In each step input size is 28x1 and output size is 10\n",
    "    - Total per unroll: 28x28\n",
    "        - FNN input size is 28x28\n",
    "- 1 hidden layer\n",
    "- ReLU activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps\n",
    "- Step 1: Load dataset\n",
    "- Step 2: Make dataset iterable\n",
    "- Step 3: Create model class\n",
    "- Step 4: Instantiate model class\n",
    "- Step 5: Instantiate loss class\n",
    "- Step 6: Instantiate optimizer class\n",
    "- Step 7: Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 1: Load dataset\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='.\\data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='.\\data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "Step 2: Make dataset iterable\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs) # the number of times we go through the whole dataset\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "Step 3: Create model class\n",
    "'''\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Hidden dimension\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # RNN\n",
    "        # ** batch_first=True causes input/output tensors to be of shape (batch_dim, seq_dim, input_dim) \n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Initialize hidden state with zeros (layer_dim, batch_size, hidden_dim)\n",
    "        h_0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)) # x.size(o)=batch_size because of part **\n",
    "        \n",
    "        out, h_n = self.rnn(x, h_0)\n",
    "        \n",
    "        # Index hidden state of last time step\n",
    "        # out.size()->100,28,100\n",
    "        # out[:,-1,:]-> 100,100 -> just want last time step hidden states\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        #out.size()-> 100,10 (batch_size=100. 10 prediction for every image in 100 images)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One forward can return all 28 time steps but we only want the last time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 28 time steps\n",
    "    - In each time step: input dimension=28. It means that in each time step, we only fit 28 pixels and after 28 time steps, all 28x28 pixels are fitted. Therefore, we only want the prediction of the last time step.\n",
    "- 1 hidden layer\n",
    "- MNIST 1-9 digits $\\rightarrow$ output dimension=10\n",
    "- Cross Entropy Loss is used for RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 4: Instantiate model class\n",
    "'''\n",
    "\n",
    "input_dim = 28 # not 28x28 as FNN\n",
    "hidden_dim = 100\n",
    "layer_dim = 1\n",
    "output_dim = 10\n",
    "\n",
    "\n",
    "model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "'''\n",
    "Step 5: Instantiate loss class\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "Step 6: Instantiate optimizer class\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters In-Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](params.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input to hidden (A1)\n",
    "list(model.parameters())[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input to hidden bias (B1)\n",
    "list(model.parameters())[2].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hidden to hidden (A3). Linear function between time steps\n",
    "list(model.parameters())[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hidden to hidden bias (B3)\n",
    "list(model.parameters())[3].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hidden to readout (A2)\n",
    "list(model.parameters())[4].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hidden to readout bias (B2)\n",
    "list(model.parameters())[5].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input sizes:\n",
    "\n",
    "- RNN input size: (1, 28)\n",
    "- CNN input size: (1, 28, 28)\n",
    "- FNN input size: (1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 1.6357725858688354. Accuracy: 53\n",
      "Iteration: 1000. Loss: 0.5974587798118591. Accuracy: 66\n",
      "Iteration: 1500. Loss: 0.9172778129577637. Accuracy: 72\n",
      "Iteration: 2000. Loss: 0.5771211981773376. Accuracy: 84\n",
      "Iteration: 2500. Loss: 0.3714955151081085. Accuracy: 90\n",
      "Iteration: 3000. Loss: 0.2064978927373886. Accuracy: 89\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Step 7: Train the model\n",
    "'''\n",
    "\n",
    "# number of steps to unroll\n",
    "seq_dim = 28\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # Load images as variables\n",
    "        # resize images to (batch_size, seq_dim, input_dim) because when we create our model class, we had the argument \n",
    "        # batch_first=True\n",
    "        images = Variable(images.view(-1, seq_dim, input_dim))  \n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        # outputs.size()->100, 10\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, seq_dim, input_dim)) \n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct // total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model B: 2 hidden layer (ReLU)\n",
    "- Unroll 28 time steps\n",
    "    - In each step input size is 28x1 and output size is 10\n",
    "    - Total per unroll: 28x28\n",
    "        - FNN input size is 28x28\n",
    "- 2 hidden layer\n",
    "- ReLU activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps\n",
    "- Step 1: Load dataset\n",
    "- Step 2: Make dataset iterable\n",
    "- Step 3: Create model class\n",
    "- **Step 4: Instantiate model class**\n",
    "- Step 5: Instantiate loss class\n",
    "- Step 6: Instantiate optimizer class\n",
    "- Step 7: Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 1: Load dataset\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='.\\data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='.\\data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "Step 2: Make dataset iterable\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs) # the number of times we go through the whole dataset\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "Step 3: Create model class\n",
    "'''\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Hidden dimension\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # RNN\n",
    "        # ** batch_first=True causes input/output tensors to be of shape (batch_dim, seq_dim, input_dim) \n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Initialize hidden state with zeros (layer_dim, batch_size, hidden_dim)\n",
    "        h_0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)) # x.size(o)=batch_size because of part **\n",
    "        \n",
    "        out, h_n = self.rnn(x, h_0)\n",
    "        \n",
    "        # Index hidden state of last time step\n",
    "        # out.size()->100,28,100\n",
    "        # out[:,-1,:]-> 100,100 -> just want last time step hidden states\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        #out.size()-> 100,10 (batch_size=100. 10 prediction for every image in 100 images)\n",
    "        return out\n",
    "\n",
    "'''\n",
    "Step 4: Instantiate model class\n",
    "'''\n",
    "\n",
    "input_dim = 28 # not 28x28 as FNN\n",
    "hidden_dim = 100\n",
    "layer_dim = 2 # the only change is here\n",
    "output_dim = 10\n",
    "\n",
    "\n",
    "model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "'''\n",
    "Step 5: Instantiate loss class\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "Step 6: Instantiate optimizer class\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters In-Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (rnn): RNN(28, 100, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "10\n",
      "torch.Size([100, 28])\n",
      "torch.Size([100, 100])\n",
      "torch.Size([100])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 100])\n",
      "torch.Size([100, 100])\n",
      "torch.Size([100])\n",
      "torch.Size([100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(model) \n",
    "print(len(list(model.parameters())))\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](params-b.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 1.1144779920578003. Accuracy: 58\n",
      "Iteration: 1000. Loss: 0.6000416278839111. Accuracy: 81\n",
      "Iteration: 1500. Loss: 0.35051703453063965. Accuracy: 91\n",
      "Iteration: 2000. Loss: 0.43590569496154785. Accuracy: 90\n",
      "Iteration: 2500. Loss: 0.19808748364448547. Accuracy: 95\n",
      "Iteration: 3000. Loss: 0.031059807166457176. Accuracy: 95\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Step 7: Train the model\n",
    "'''\n",
    "\n",
    "# number of steps to unroll\n",
    "seq_dim = 28\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # Load images as variables\n",
    "        # resize images to (batch_size, seq_dim, input_dim) because when we create our model class, we had the argument \n",
    "        # batch_first=True\n",
    "        images = Variable(images.view(-1, seq_dim, input_dim))  \n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        # outputs.size()->100, 10\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, seq_dim, input_dim)) \n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct // total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model C: 2 hidden layer (Tanh)\n",
    "- Unroll 28 time steps\n",
    "    - In each step input size is 28x1 and output size is 10\n",
    "    - Total per unroll: 28x28\n",
    "        - FNN input size is 28x28\n",
    "- 2 hidden layer\n",
    "- Tanh activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps\n",
    "- Step 1: Load dataset\n",
    "- Step 2: Make dataset iterable\n",
    "- **Step 3: Create model class**\n",
    "- Step 4: Instantiate model class\n",
    "- Step 5: Instantiate loss class\n",
    "- Step 6: Instantiate optimizer class\n",
    "- Step 7: Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 1: Load dataset\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='.\\data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='.\\data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "Step 2: Make dataset iterable\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs) # the number of times we go through the whole dataset\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "Step 3: Create model class\n",
    "'''\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Hidden dimension\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # RNN\n",
    "        # ** batch_first=True causes input/output tensors to be of shape (batch_dim, seq_dim, input_dim) \n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='tanh')\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Initialize hidden state with zeros (layer_dim, batch_size, hidden_dim)\n",
    "        h_0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)) # x.size(o)=batch_size because of part **\n",
    "        \n",
    "        out, h_n = self.rnn(x, h_0)\n",
    "        \n",
    "        # Index hidden state of last time step\n",
    "        # out.size()->100,28,100\n",
    "        # out[:,-1,:]-> 100,100 -> just want last time step hidden states\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        #out.size()-> 100,10 (batch_size=100. 10 prediction for every image in 100 images)\n",
    "        return out\n",
    "\n",
    "'''\n",
    "Step 4: Instantiate model class\n",
    "'''\n",
    "\n",
    "input_dim = 28 # not 28x28 as FNN\n",
    "hidden_dim = 100\n",
    "layer_dim = 2 # the only change is here\n",
    "output_dim = 10\n",
    "\n",
    "\n",
    "model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "'''\n",
    "Step 5: Instantiate loss class\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "Step 6: Instantiate optimizer class\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters In-Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (rnn): RNN(28, 100, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "10\n",
      "torch.Size([100, 28])\n",
      "torch.Size([100, 100])\n",
      "torch.Size([100])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 100])\n",
      "torch.Size([100, 100])\n",
      "torch.Size([100])\n",
      "torch.Size([100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(model) \n",
    "print(len(list(model.parameters())))\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](params-b.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.6841119527816772. Accuracy: 77\n",
      "Iteration: 1000. Loss: 0.2851320505142212. Accuracy: 92\n",
      "Iteration: 1500. Loss: 2.4408063888549805. Accuracy: 54\n",
      "Iteration: 2000. Loss: 0.2851850986480713. Accuracy: 92\n",
      "Iteration: 2500. Loss: 0.2369268536567688. Accuracy: 95\n",
      "Iteration: 3000. Loss: 0.25919097661972046. Accuracy: 94\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Step 7: Train the model\n",
    "'''\n",
    "\n",
    "# number of steps to unroll\n",
    "seq_dim = 28\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # Load images as variables\n",
    "        # resize images to (batch_size, seq_dim, input_dim) because when we create our model class, we had the argument \n",
    "        # batch_first=True\n",
    "        images = Variable(images.view(-1, seq_dim, input_dim))  \n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        # outputs.size()->100, 10\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, seq_dim, input_dim)) \n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct // total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN From CPU to GPU in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model C: 2 hidden layer (Tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps\n",
    "- Step 1: Load dataset\n",
    "- Step 2: Make dataset iterable\n",
    "- **Step 3: Create model class**\n",
    "- **Step 4: Instantiate model class**\n",
    "- Step 5: Instantiate loss class\n",
    "- Step 6: Instantiate optimizer class\n",
    "- **Step 7: Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.49409088492393494. Accuracy: 83\n",
      "Iteration: 1000. Loss: 0.25837576389312744. Accuracy: 91\n",
      "Iteration: 1500. Loss: 0.16369344294071198. Accuracy: 94\n",
      "Iteration: 2000. Loss: 0.8283212184906006. Accuracy: 89\n",
      "Iteration: 2500. Loss: 0.07316825538873672. Accuracy: 94\n",
      "Iteration: 3000. Loss: 0.15746740996837616. Accuracy: 96\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "'''\n",
    "Step 1: Load dataset\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='.\\data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='.\\data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "Step 2: Make dataset iterable\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs) # the number of times we go through the whole dataset\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "Step 3: Create model class\n",
    "'''\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Hidden dimension\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # RNN\n",
    "        # ** batch_first=True causes input/output tensors to be of shape (batch_dim, seq_dim, input_dim) \n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='tanh')\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Initialize hidden state with zeros\n",
    "        #######################\n",
    "        #  USE GPU FOR MODEL  #\n",
    "        #######################\n",
    "        if torch.cuda.is_available():\n",
    "            h_0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).cuda())\n",
    "        else:\n",
    "            h_0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "            \n",
    "        \n",
    "        out, h_n = self.rnn(x, h_0)\n",
    "        \n",
    "        # Index hidden state of last time step\n",
    "        # out.size()->100,28,100\n",
    "        # out[:,-1,:]-> 100,100 -> just want last time step hidden states\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        #out.size()-> 100,10 (batch_size=100. 10 prediction for every image in 100 images)\n",
    "        return out\n",
    "\n",
    "'''\n",
    "Step 4: Instantiate model class\n",
    "'''\n",
    "\n",
    "input_dim = 28 # not 28x28 as FNN\n",
    "hidden_dim = 100\n",
    "layer_dim = 2 # the only change is here\n",
    "output_dim = 10\n",
    "\n",
    "\n",
    "model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "'''\n",
    "Step 5: Instantiate loss class\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "Step 6: Instantiate optimizer class\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "Step 7: Train the model\n",
    "'''\n",
    "\n",
    "# Number of steps to unroll\n",
    "seq_dim = 28  \n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        #######################\n",
    "        #  USE GPU FOR MODEL  #\n",
    "        #######################\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            images = Variable(images.view(-1, seq_dim, input_dim))\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        # outputs.size() --> 100, 10\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                if torch.cuda.is_available():\n",
    "                    images = Variable(images.view(-1, seq_dim, input_dim).cuda())\n",
    "                else:\n",
    "                    images = Variable(images.view(-1, seq_dim, input_dim))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct // total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data, accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
