{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Network with PyTorch (on MNIST dataset)\n",
    "By [Zahra Taheri](https://github.com/zata213), August 30, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression problems\n",
    "- It can represent linear functions well, e.g., $y=2x+1$ and $y=x_1+x_2$.\n",
    "- It cannot represent non-linear functions well, e.g., $y=2x^2+1$ and $y=x_1x_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](logistic-vs-nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear functions in-depth\n",
    "\n",
    "**Common types of non-linearity:**\n",
    "- ReLUs (Rectified Linear Units)\n",
    "- Sigmoid\n",
    "- tanh\n",
    "\n",
    "![alt text](non-linear-functions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Feedforward Neural Network Models with PyTorch (on CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model A: 1 Hidden Layer Feedforward Neural Network (Sigmoid Activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='.\\data',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='.\\data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.targets.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make dataset iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following values are considered such that the number of epochs equals to 5. If we want the number of epochs to be 10,\n",
    "# then we must consider n_iters=6000\n",
    "\n",
    "batch_size = 100 # means that in every iteration we fit hundred images to our model at one time\n",
    "\n",
    "n_iters = 3000\n",
    "\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create iterable object :training dataset\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)# it means after each epochs, shuffle the training dataset\n",
    "\n",
    "# create iterable object :testing dataset\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetworkModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FeedforwardNeuralNetworkModel, self).__init__() #to inherit every things from nn.Module\n",
    "        # linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        # non-linearity\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # linear function\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # linear function\n",
    "        out = self.fc1(x)\n",
    "        # non-linearity\n",
    "        out = self.sigmoid(out)\n",
    "        #linear function\n",
    "        out = self.fc2(out)\n",
    "        return out       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Instantiate model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28 # size of image\n",
    "output_dim = 10 # 0,1,2,3,...,9\n",
    "hidden_dim = 100 # number of neurons/ number of non-linear activation functions\n",
    "\n",
    "model = FeedforwardNeuralNetworkModel(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Instantiate loss class\n",
    "**Cross Entropy Loss:** `nn.CrossEntropyLoss()`, computes softmax(logistic/softmax function) and then computes Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Instantiate optimizer class\n",
    "\n",
    "- Simplified equation for updating parameters is $\\theta=\\theta-\\eta\\cdot\\nabla_\\theta$, where $\\theta$ is parameters (our variables), $\\eta$ is learning rate (how fast we want the model to learn), and $\\nabla_\\theta$ is parameters' gradients.\n",
    "\n",
    "- At every iteration, we update our model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000001FA066B1510>\n",
      "4\n",
      "torch.Size([100, 784])\n",
      "torch.Size([100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())\n",
    "print(len(list(model.parameters())))\n",
    "\n",
    "# FC 1 parameters\n",
    "print(list(model.parameters())[0].size())\n",
    "\n",
    "# FC 1 Bias parameters\n",
    "print(list(model.parameters())[1].size())\n",
    "\n",
    "# FC 2 parameters\n",
    "print(list(model.parameters())[2].size())\n",
    "\n",
    "# FC 2 Bias parameters\n",
    "print(list(model.parameters())[3].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](nn-parameters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.6608788967132568. Accuracy: 85\n",
      "Iteration: 1000. Loss: 0.5462762713432312. Accuracy: 89\n",
      "Iteration: 1500. Loss: 0.4568452537059784. Accuracy: 90\n",
      "Iteration: 2000. Loss: 0.2747637927532196. Accuracy: 91\n",
      "Iteration: 2500. Loss: 0.2744704484939575. Accuracy: 91\n",
      "Iteration: 3000. Loss: 0.2502192258834839. Accuracy: 92\n"
     ]
    }
   ],
   "source": [
    "iter = 0 # n_iter is from 0 to 3000\n",
    "for epoch in range(num_epochs): #num_epochs is 5\n",
    "    # one iteration = \n",
    "    #{\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # load images as variables\n",
    "        images = Variable(images.view(-1, 28*28)) # number of images in each iteration is equal to batch_size=100\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        # 100x10 \n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "    #}\n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                #100x1\n",
    "                _, predicted = torch.max(outputs.data, 1) # torch.size(100, 1) containing largest predictions for each image\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "           \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct // total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = True\n",
    "if save_model is True:\n",
    "    torch.save(model.state_dict(), 'feedforward_neural_network_pytorch_sigmoid_mnist.pkl')# only save parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model B: 1 Hidden Layer Feedforward Neural Network (Tanh Activation)\n",
    "Just create model class is different from Model A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# read MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='.\\data',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='.\\data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# make dataset iterable\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "# create iterable objects\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetworkModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FeedforwardNeuralNetworkModel, self).__init__()\n",
    "        # linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        # non-linearity\n",
    "        self.tanh = nn.Tanh()\n",
    "        # linear function\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # linear function\n",
    "        out = self.fc1(x)\n",
    "        # non-linearity\n",
    "        out = self.tanh(out)\n",
    "        #linear function\n",
    "        out = self.fc2(out)\n",
    "        return out       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Instantiate model class\n",
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "hidden_dim = 100 \n",
    "\n",
    "model = FeedforwardNeuralNetworkModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "#  Instantiate loss class\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "#  Instantiate optimizer class\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.3384842574596405. Accuracy: 91\n",
      "Iteration: 1000. Loss: 0.214825838804245. Accuracy: 92\n",
      "Iteration: 1500. Loss: 0.32922565937042236. Accuracy: 93\n",
      "Iteration: 2000. Loss: 0.2514549195766449. Accuracy: 94\n",
      "Iteration: 2500. Loss: 0.18627643585205078. Accuracy: 94\n",
      "Iteration: 3000. Loss: 0.2388889193534851. Accuracy: 94\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # one iteration = \n",
    "    #{\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # load images as variables\n",
    "        images = Variable(images.view(-1, 28*28)) # number of images in each iteration is equal to batch_size=100\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        # 100x10 \n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "    #}\n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                #100x1\n",
    "                _, predicted = torch.max(outputs.data, 1) # torch.size(100, 1) containing largest predictions for each image\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "           \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct // total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = True\n",
    "if save_model is True:\n",
    "    torch.save(model.state_dict(), 'feedforward_neural_network_pytorch_tanh_mnist.pkl')# only save parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model C: 1 Hidden Layer Feedforward Neural Network (ReLU Activation)\n",
    "Just create model class is different from Models A and B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# read MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='.\\data',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='.\\data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# make dataset iterable\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "# create iterable objects\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetworkModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FeedforwardNeuralNetworkModel, self).__init__()\n",
    "        # linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        # non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        # linear function\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # linear function\n",
    "        out = self.fc1(x)\n",
    "        # non-linearity\n",
    "        out = self.relu(out)\n",
    "        #linear function\n",
    "        out = self.fc2(out)\n",
    "        return out       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Instantiate model class\n",
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "hidden_dim = 100 \n",
    "\n",
    "model = FeedforwardNeuralNetworkModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "#  Instantiate loss class\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "#  Instantiate optimizer class\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.2624063789844513. Accuracy: 91\n",
      "Iteration: 1000. Loss: 0.3202899992465973. Accuracy: 93\n",
      "Iteration: 1500. Loss: 0.18649150431156158. Accuracy: 93\n",
      "Iteration: 2000. Loss: 0.3143783211708069. Accuracy: 94\n",
      "Iteration: 2500. Loss: 0.11973336338996887. Accuracy: 95\n",
      "Iteration: 3000. Loss: 0.19717523455619812. Accuracy: 95\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # one iteration = \n",
    "    #{\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # load images as variables\n",
    "        images = Variable(images.view(-1, 28*28)) # number of images in each iteration is equal to batch_size=100\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        # 100x10 \n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "    #}\n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                #100x1\n",
    "                _, predicted = torch.max(outputs.data, 1) # torch.size(100, 1) containing largest predictions for each image\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "           \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct // total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = True\n",
    "if save_model is True:\n",
    "    torch.save(model.state_dict(), 'feedforward_neural_network_pytorch_relu_mnist.pkl')# only save parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model D: 2 Hidden Layers Feedforward Neural Network (ReLU Activation)\n",
    "\n",
    "![alt text](2-hidden-relu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# read MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='.\\data',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='.\\data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# make dataset iterable\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "# create iterable objects\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetworkModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FeedforwardNeuralNetworkModel, self).__init__()\n",
    "        # linear function 1: 784->100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        # non-linearity 1\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # linear function 2: 100->100\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # non-linearity 2\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # linear function (readout) 100->10\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # linear function 1\n",
    "        out = self.fc1(x)\n",
    "        # non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # linear function 2\n",
    "        out = self.fc2(out)\n",
    "        # non-linearity 2\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        #linear function 3 (readout)\n",
    "        out = self.fc3(out)\n",
    "        return out       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Instantiate model class\n",
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "hidden_dim = 100 \n",
    "\n",
    "model = FeedforwardNeuralNetworkModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "#  Instantiate loss class\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "#  Instantiate optimizer class\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.24841484427452087. Accuracy: 90\n",
      "Iteration: 1000. Loss: 0.3699096739292145. Accuracy: 93\n",
      "Iteration: 1500. Loss: 0.08417633920907974. Accuracy: 94\n",
      "Iteration: 2000. Loss: 0.2003040760755539. Accuracy: 95\n",
      "Iteration: 2500. Loss: 0.10492083430290222. Accuracy: 95\n",
      "Iteration: 3000. Loss: 0.13262341916561127. Accuracy: 96\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # one iteration = \n",
    "    #{\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # load images as variables\n",
    "        images = Variable(images.view(-1, 28*28)) # number of images in each iteration is equal to batch_size=100\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        # 100x10 \n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "    #}\n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                #100x1\n",
    "                _, predicted = torch.max(outputs.data, 1) # torch.size(100, 1) containing largest predictions for each image\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "           \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct // total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = True\n",
    "if save_model is True:\n",
    "    torch.save(model.state_dict(), 'feedforward_neural_network_pytorch_relu2_mnist.pkl')# only save parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model E: 3 Hidden Layers Feedforward Neural Network (ReLU Activation)\n",
    "\n",
    "![alt text](3-hidden-relu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# read MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='.\\data',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='.\\data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# make dataset iterable\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "# create iterable objects\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetworkModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FeedforwardNeuralNetworkModel, self).__init__()\n",
    "        # linear function 1: 784->100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        # non-linearity 1\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # linear function 2: 100->100\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # non-linearity 2\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # linear function 3: 100->100\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # non-linearity 3\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        # linear function 4 (readout) 100->10\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # linear function 1\n",
    "        out = self.fc1(x)\n",
    "        # non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # linear function 2\n",
    "        out = self.fc2(out)\n",
    "        # non-linearity 2\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        # linear function 3\n",
    "        out = self.fc3(out)\n",
    "        # non-linearity 3\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        #linear function 4 (readout)\n",
    "        out = self.fc4(out)\n",
    "        return out       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Instantiate model class\n",
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "hidden_dim = 100 \n",
    "\n",
    "model = FeedforwardNeuralNetworkModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "#  Instantiate loss class\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "#  Instantiate optimizer class\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.20820769667625427. Accuracy: 91\n",
      "Iteration: 1000. Loss: 0.27557432651519775. Accuracy: 93\n",
      "Iteration: 1500. Loss: 0.15087616443634033. Accuracy: 95\n",
      "Iteration: 2000. Loss: 0.16060371696949005. Accuracy: 95\n",
      "Iteration: 2500. Loss: 0.03184164687991142. Accuracy: 96\n",
      "Iteration: 3000. Loss: 0.04542054980993271. Accuracy: 96\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # one iteration = \n",
    "    #{\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # load images as variables\n",
    "        images = Variable(images.view(-1, 28*28)) # number of images in each iteration is equal to batch_size=100\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        # 100x10 \n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "    #}\n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                #100x1\n",
    "                _, predicted = torch.max(outputs.data, 1) # torch.size(100, 1) containing largest predictions for each image\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "           \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct // total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = True\n",
    "if save_model is True:\n",
    "    torch.save(model.state_dict(), 'feedforward_neural_network_pytorch_relu3_mnist.pkl')# only save parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning\n",
    "**2 ways to expand a neural network**\n",
    "   - More non-linear activation units (hidden dimension) (neurons)\n",
    "   - More hidden layers\n",
    "    \n",
    "**Cons**\n",
    "   - Need a larger dataset (curse of dimensionality)\n",
    "   - Does not necessarily mean higher accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Feedforward Neural Network Model with PyTorch (On GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Hidden Layers Feedforward Neural Network (ReLU Activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# read MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='.\\data',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='.\\data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# make dataset iterable\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "# create iterable objects\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetworkModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FeedforwardNeuralNetworkModel, self).__init__()\n",
    "        # linear function 1: 784->100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        # non-linearity 1\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # linear function 2: 100->100\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # non-linearity 2\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # linear function 3: 100->100\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # non-linearity 3\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        # linear function 4 (readout) 100->10\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # linear function 1\n",
    "        out = self.fc1(x)\n",
    "        # non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # linear function 2\n",
    "        out = self.fc2(out)\n",
    "        # non-linearity 2\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        # linear function 3\n",
    "        out = self.fc3(out)\n",
    "        # non-linearity 3\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        #linear function 4 (readout)\n",
    "        out = self.fc4(out)\n",
    "        return out       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Instantiate model class\n",
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "hidden_dim = 100 \n",
    "\n",
    "model = FeedforwardNeuralNetworkModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Instantiate loss class\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "#  Instantiate optimizer class\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.44783785939216614. Accuracy: 87\n",
      "Iteration: 1000. Loss: 0.2271917313337326. Accuracy: 94\n",
      "Iteration: 1500. Loss: 0.0881962701678276. Accuracy: 95\n",
      "Iteration: 2000. Loss: 0.130946084856987. Accuracy: 96\n",
      "Iteration: 2500. Loss: 0.057556651532649994. Accuracy: 96\n",
      "Iteration: 3000. Loss: 0.12545844912528992. Accuracy: 96\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # one iteration = \n",
    "    #{\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        #######################\n",
    "        #  USE GPU FOR MODEL  #\n",
    "        #######################\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.view(-1, 28*28).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            images = Variable(images.view(-1, 28*28))\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        # 100x10 \n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "    #}\n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                if torch.cuda.is_available():\n",
    "                    images = Variable(images.view(-1, 28*28).cuda())\n",
    "                else:\n",
    "                    images = Variable(images.view(-1, 28*28))    \n",
    "                    \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                #100x1\n",
    "                _, predicted = torch.max(outputs.data, 1) # torch.size(100, 1) containing largest predictions for each image\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "           \n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                # Total correct predictions\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum() # because .sum() function does not work on GPU\n",
    "                else:                \n",
    "                    correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct // total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data, accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
